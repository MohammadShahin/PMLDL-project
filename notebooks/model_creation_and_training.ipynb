{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ea516e3",
   "metadata": {
    "cellId": "s97scoe5rqkm3rhdulin"
   },
   "source": [
    "First we import all the libraries we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4f65b52",
   "metadata": {
    "cellId": "rjgrwel0mrnvphiv8dfvj"
   },
   "outputs": [],
   "source": [
    "#!c1.32\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "from IPython.display import Video\n",
    "import cv2\n",
    "import math\n",
    "import torch\n",
    "import glob\n",
    "from tqdm.auto import tqdm\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import time\n",
    "import argparse\n",
    "import logging\n",
    "from sys import exit\n",
    "import shutil\n",
    "\n",
    "from timm.models import create_model, apply_test_time_pool\n",
    "from timm.data import ImageDataset, create_loader, resolve_data_config\n",
    "from timm.utils import AverageMeter, setup_default_logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b201b59",
   "metadata": {
    "cellId": "dqx3pzthubb1p130hdprp5"
   },
   "source": [
    "We do some setting up here. First we define `debug`. If `debug` is set then that means that we do not actually want to train the model we're just checking if everything is working. That means small number of epochs and small amount of videos to train on.\n",
    "\n",
    "We will try different error tolrances for each type of event and tune it as a hyper parameter. We basically instead of giving the model a second where an event happens we give it a range of [event_timestamp - err_tol, event_timestamp + err_tol] and train it on that. \n",
    "\n",
    "Since the dataset is very small, we define the train/validation split manually. Once everything is ready we will try different combinations manually and keep the one that gives the best result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "47036136",
   "metadata": {
    "cellId": "6phkfwl0qy3pzat2qyhv4"
   },
   "outputs": [],
   "source": [
    "#!c1.32\n",
    "debug = False\n",
    "if debug:\n",
    "    epochs = 3\n",
    "else:\n",
    "    epochs = 20\n",
    "\n",
    "err_tol = {\n",
    "    'challenge': [ 0.30, 0.40, 0.50, 0.60, 0.70 ],\n",
    "    'play': [ 0.15, 0.20, 0.25, 0.30, 0.35 ],\n",
    "    'throwin': [ 0.15, 0.20, 0.25, 0.30, 0.35 ]\n",
    "}\n",
    "video_id_split = {\n",
    "    'val':[\n",
    "         '3c993bd2_0',\n",
    "         '3c993bd2_1',\n",
    "    ],\n",
    "    'train':[\n",
    "         '1606b0e6_0',\n",
    "         '1606b0e6_1',\n",
    "         '35bd9041_0',\n",
    "         '35bd9041_1',\n",
    "         '407c5a9e_1',\n",
    "         '4ffd5986_0',\n",
    "         '9a97dae4_1',\n",
    "         'cfbe2e94_0',\n",
    "         'cfbe2e94_1',\n",
    "         'ecf251d4_0',\n",
    "    ]\n",
    "}\n",
    "event_names = ['challenge', 'throwin', 'play']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0371be",
   "metadata": {
    "cellId": "wcnqi5k8mq36bajvj146x"
   },
   "source": [
    "We then load the data and remove every value of the form [id, timestamp, event_type, attr] (except for types `start` and `end`) and add two values in its place [id, timestamp - err_tol, start_event_type, attr] and [id, timestamp + err_tol, end_event_type, attr]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9397e27c",
   "metadata": {
    "cellId": "huyeje7e9fgnh247i9dvoj"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>time</th>\n",
       "      <th>event</th>\n",
       "      <th>event_attributes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1606b0e6_0</td>\n",
       "      <td>200.265822</td>\n",
       "      <td>start</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1606b0e6_0</td>\n",
       "      <td>201.000000</td>\n",
       "      <td>start_challenge</td>\n",
       "      <td>['ball_action_forced']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1606b0e6_0</td>\n",
       "      <td>201.300000</td>\n",
       "      <td>end_challenge</td>\n",
       "      <td>['ball_action_forced']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1606b0e6_0</td>\n",
       "      <td>202.765822</td>\n",
       "      <td>end</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1606b0e6_0</td>\n",
       "      <td>210.124111</td>\n",
       "      <td>start</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11214</th>\n",
       "      <td>ecf251d4_0</td>\n",
       "      <td>3058.072895</td>\n",
       "      <td>end</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11215</th>\n",
       "      <td>ecf251d4_0</td>\n",
       "      <td>3068.280519</td>\n",
       "      <td>start</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8762</th>\n",
       "      <td>ecf251d4_0</td>\n",
       "      <td>3069.472000</td>\n",
       "      <td>start_throwin</td>\n",
       "      <td>['pass']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8763</th>\n",
       "      <td>ecf251d4_0</td>\n",
       "      <td>3069.622000</td>\n",
       "      <td>end_throwin</td>\n",
       "      <td>['pass']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11217</th>\n",
       "      <td>ecf251d4_0</td>\n",
       "      <td>3070.780519</td>\n",
       "      <td>end</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15600 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         video_id         time            event        event_attributes\n",
       "0      1606b0e6_0   200.265822            start                     NaN\n",
       "0      1606b0e6_0   201.000000  start_challenge  ['ball_action_forced']\n",
       "1      1606b0e6_0   201.300000    end_challenge  ['ball_action_forced']\n",
       "2      1606b0e6_0   202.765822              end                     NaN\n",
       "3      1606b0e6_0   210.124111            start                     NaN\n",
       "...           ...          ...              ...                     ...\n",
       "11214  ecf251d4_0  3058.072895              end                     NaN\n",
       "11215  ecf251d4_0  3068.280519            start                     NaN\n",
       "8762   ecf251d4_0  3069.472000    start_throwin                ['pass']\n",
       "8763   ecf251d4_0  3069.622000      end_throwin                ['pass']\n",
       "11217  ecf251d4_0  3070.780519              end                     NaN\n",
       "\n",
       "[15600 rows x 4 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!c1.32\n",
    "df = pd.read_csv(\"../dfl-bundesliga-data-shootout/train.csv\")\n",
    "additional_events = []\n",
    "for arr in df.sort_values(['video_id','time','event','event_attributes']).values:\n",
    "    # if we encounter an event that isn't start or end\n",
    "    if arr[2] in err_tol:\n",
    "        # take half of the tolarince\n",
    "        tol = err_tol[arr[2]][0]/2\n",
    "        # add starting timestamp of the event\n",
    "        additional_events.append([arr[0], arr[1]-tol, 'start_'+arr[2], arr[3]])\n",
    "        # add ending timestamp of the event\n",
    "        additional_events.append([arr[0], arr[1]+tol, 'end_'+arr[2], arr[3]])\n",
    "df = pd.concat([df, pd.DataFrame(additional_events, columns=df.columns)])\n",
    "# take all the events that aren't contained in event_names\n",
    "# this effectively deletes all challenge, pass, throwin events\n",
    "# and leaves only start, end, and start_event, and end_event\n",
    "df = df[~df['event'].isin(event_names)]\n",
    "df = df.sort_values(['video_id', 'time'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00abe7ab",
   "metadata": {
    "cellId": "c18d6ehvad6a7x2cdc1gwt"
   },
   "source": [
    "In the next cell we go over every video and split it into photos. We assign 4 different kinds of photos. If a certain frame falls between the start and end of a certain event then the photo of that frame is assigned to that type. If a certain frame doesn't fall in any event then we assign it to type `background` which means no event is happening in this frame. \n",
    "\n",
    "Also, for instead of extracting the i-th frame as is, we stack three grayscaled frames (i-1)-th, i-th, (i+1)-th and consider them as the i-th frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7ffc201b",
   "metadata": {
    "cellId": "fbj2obrhomamlwf7dzm3ia"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 1606b0e6_0 (1396, 4)\n",
      "train 1606b0e6_1 (1756, 4)\n",
      "train 35bd9041_0 (1486, 4)\n",
      "train 35bd9041_1 (1292, 4)\n",
      "train 407c5a9e_1 (1208, 4)\n",
      "train 4ffd5986_0 (1094, 4)\n",
      "train 9a97dae4_1 (1028, 4)\n",
      "train cfbe2e94_0 (1128, 4)\n",
      "train cfbe2e94_1 (1048, 4)\n",
      "train ecf251d4_0 (1366, 4)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#!c1.32\n",
    "def extract_training_images(args):\n",
    "        video_id, split = args\n",
    "        video_path = f\"../dfl-bundesliga-data-shootout/train/{video_id}.mp4\"\n",
    "        # create video object\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        # get number of frames per second\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "        # this function returns a gray scaled frame given the index of the frame\n",
    "        def get_frame(frame_num):\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
    "            ret, frame = cap.read()\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            return frame\n",
    "        \n",
    "        time_interval = 1/fps\n",
    "\n",
    "        # grab only relevent rows from train.csv\n",
    "        df_video = df[df.video_id == video_id]\n",
    "        if debug:\n",
    "            df_video = df_video.head(10)\n",
    "        print(split, video_id, df_video.shape)\n",
    "\n",
    "        arr = df_video[['time','event']].values\n",
    "        # iterate over all events\n",
    "        for idx in range(len(arr)-1):\n",
    "            # the frame where this event starts\n",
    "            crr_frame = int(math.ceil(arr[idx,0] * fps))\n",
    "            # the frame where this event ends\n",
    "            nxt_frame = int(math.ceil(arr[idx+1,0] * fps))\n",
    "            crr_event = arr[idx,1]\n",
    "\n",
    "            # get which type of event this frame is in\n",
    "            crr_event = crr_event\n",
    "            if crr_event == 'start':\n",
    "                crr_status = 'background'\n",
    "            elif crr_event == 'end':\n",
    "                # should use as background?\n",
    "                continue\n",
    "            else:\n",
    "                start_or_end, crr_status = crr_event.split('_', 1)\n",
    "                if start_or_end == 'end':\n",
    "                    crr_status = 'background'\n",
    "\n",
    "            # create result directory\n",
    "            result_dir = f\"../work/split_images/{split}/{crr_status}\"\n",
    "            if not os.path.exists(result_dir):\n",
    "                os.makedirs(result_dir, exist_ok=True)\n",
    "\n",
    "            # iterate over all the frames in this event\n",
    "            this_frame = crr_frame\n",
    "            while this_frame < nxt_frame:\n",
    "                frame_num = this_frame\n",
    "                \n",
    "                # get three consecutive frames and stack them channel-wise\n",
    "                frame_prev = get_frame(frame_num - 1)\n",
    "                frame_cur = get_frame(frame_num)\n",
    "                frame_next = get_frame(frame_num + 1)\n",
    "                frame = np.stack((frame_prev, frame_cur, frame_next), axis=-1)\n",
    "                \n",
    "                out_file = f'{result_dir}/{video_id}_{frame_num:06}.jpg'\n",
    "                cv2.imwrite(out_file, frame)\n",
    "\n",
    "                if crr_status == 'background':\n",
    "                    this_frame += 10\n",
    "                else:\n",
    "                    this_frame += 1\n",
    "\n",
    "for split in video_id_split:\n",
    "   video_ids = video_id_split[split]\n",
    "   for video_id in video_ids:            \n",
    "       extract_training_images([video_id, split])\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c65808cc",
   "metadata": {
    "cellId": "c3pwwok3ilmq5fivi9bnbp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3c993bd2_0', 'val']\n",
      "['3c993bd2_1', 'val']\n",
      "['1606b0e6_0', 'train']\n",
      "['1606b0e6_1', 'train']\n",
      "['35bd9041_0', 'train']\n",
      "['35bd9041_1', 'train']\n",
      "['407c5a9e_1', 'train']\n",
      "['4ffd5986_0', 'train']\n",
      "['9a97dae4_1', 'train']\n",
      "['cfbe2e94_0', 'train']\n",
      "['cfbe2e94_1', 'train']\n",
      "['ecf251d4_0', 'train']\n"
     ]
    }
   ],
   "source": [
    "#!c1.32\n",
    "for split in video_id_split:\n",
    "   video_ids = video_id_split[split]\n",
    "   for video_id in video_ids:            \n",
    "       print([video_id, split])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f6df13",
   "metadata": {
    "cellId": "lkmg2utljkmly6759yycpn"
   },
   "source": [
    "Training was causing memory problems, we used the following line to elevate the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "78b01cfc",
   "metadata": {
    "cellId": "lap9mgplj9x70b5ptk46"
   },
   "outputs": [],
   "source": [
    "#!c1.32\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b6711e",
   "metadata": {
    "cellId": "0y4kkz3se44h6snx7ch68fi"
   },
   "source": [
    "We use the pretrianed tf_efficientnet_b0_ap model from the timm library and trained it on our images using the train script from the timm library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cea3d1",
   "metadata": {
    "cellId": "q7l7fpkacw36nxjuzlra4",
    "execution_id": "86d330c8-eb14-47b6-8c0f-8a395a5cb3b8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training with a single process on 1 GPUs.\n",
      "Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b0_ap-f262efe1.pth)\n",
      "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b0_ap-f262efe1.pth\" to /tmp/xdg_cache/torch/hub/checkpoints/tf_efficientnet_b0_ap-f262efe1.pth\n",
      "Model tf_efficientnet_b0_ap created, param count:4012672\n",
      "Data processing configuration for current model + dataset:\n",
      "\tinput_size: (3, 720, 1280)\n",
      "\tinterpolation: bicubic\n",
      "\tmean: (0.5, 0.5, 0.5)\n",
      "\tstd: (0.5, 0.5, 0.5)\n",
      "\tcrop_pct: 0.875\n",
      "Using native Torch AMP. Training in mixed precision.\n",
      "Scheduled epochs: 20\n",
      "Train: 0 [   0/2388 (  0%)]  Loss: 0.8875 (0.887)  Time: 10.801s,    1.48/s  (10.801s,    1.48/s)  LR: 1.000e-04  Data: 1.768 (1.768)\n",
      "Train: 0 [  50/2388 (  2%)]  Loss: 0.4743 (0.756)  Time: 0.337s,   47.52/s  (0.558s,   28.68/s)  LR: 1.000e-04  Data: 0.017 (0.066)\n",
      "Train: 0 [ 100/2388 (  4%)]  Loss: 0.4604 (0.654)  Time: 0.629s,   25.42/s  (0.479s,   33.40/s)  LR: 1.000e-04  Data: 0.309 (0.072)\n",
      "Train: 0 [ 150/2388 (  6%)]  Loss: 0.4226 (0.611)  Time: 0.336s,   47.61/s  (0.456s,   35.12/s)  LR: 1.000e-04  Data: 0.016 (0.077)\n",
      "Train: 0 [ 200/2388 (  8%)]  Loss: 0.5445 (0.594)  Time: 0.339s,   47.16/s  (0.445s,   35.92/s)  LR: 1.000e-04  Data: 0.018 (0.082)\n",
      "Train: 0 [ 250/2388 ( 10%)]  Loss: 0.5070 (0.581)  Time: 0.334s,   47.89/s  (0.443s,   36.15/s)  LR: 1.000e-04  Data: 0.015 (0.088)\n",
      "Train: 0 [ 300/2388 ( 13%)]  Loss: 0.5046 (0.575)  Time: 0.332s,   48.14/s  (0.440s,   36.33/s)  LR: 1.000e-04  Data: 0.012 (0.091)\n",
      "Train: 0 [ 350/2388 ( 15%)]  Loss: 0.5841 (0.570)  Time: 0.336s,   47.67/s  (0.439s,   36.48/s)  LR: 1.000e-04  Data: 0.016 (0.093)\n",
      "Train: 0 [ 400/2388 ( 17%)]  Loss: 0.5450 (0.563)  Time: 0.333s,   48.08/s  (0.434s,   36.86/s)  LR: 1.000e-04  Data: 0.013 (0.092)\n",
      "Train: 0 [ 450/2388 ( 19%)]  Loss: 0.5609 (0.561)  Time: 0.336s,   47.61/s  (0.434s,   36.86/s)  LR: 1.000e-04  Data: 0.015 (0.094)\n",
      "Train: 0 [ 500/2388 ( 21%)]  Loss: 0.4728 (0.559)  Time: 0.333s,   47.98/s  (0.434s,   36.89/s)  LR: 1.000e-04  Data: 0.014 (0.096)\n",
      "Train: 0 [ 550/2388 ( 23%)]  Loss: 0.3496 (0.553)  Time: 0.339s,   47.15/s  (0.435s,   36.82/s)  LR: 1.000e-04  Data: 0.019 (0.098)\n",
      "Train: 0 [ 600/2388 ( 25%)]  Loss: 0.4746 (0.550)  Time: 0.336s,   47.69/s  (0.433s,   36.92/s)  LR: 1.000e-04  Data: 0.016 (0.099)\n",
      "Train: 0 [ 650/2388 ( 27%)]  Loss: 0.6179 (0.548)  Time: 0.336s,   47.57/s  (0.432s,   37.03/s)  LR: 1.000e-04  Data: 0.017 (0.098)\n",
      "Train: 0 [ 700/2388 ( 29%)]  Loss: 0.5174 (0.545)  Time: 0.336s,   47.67/s  (0.430s,   37.17/s)  LR: 1.000e-04  Data: 0.016 (0.098)\n",
      "Train: 0 [ 750/2388 ( 31%)]  Loss: 0.5382 (0.542)  Time: 0.337s,   47.53/s  (0.430s,   37.23/s)  LR: 1.000e-04  Data: 0.017 (0.098)\n",
      "Train: 0 [ 800/2388 ( 34%)]  Loss: 0.5111 (0.541)  Time: 0.345s,   46.39/s  (0.428s,   37.34/s)  LR: 1.000e-04  Data: 0.024 (0.097)\n",
      "Train: 0 [ 850/2388 ( 36%)]  Loss: 0.4102 (0.539)  Time: 0.334s,   47.84/s  (0.427s,   37.45/s)  LR: 1.000e-04  Data: 0.015 (0.097)\n",
      "Train: 0 [ 900/2388 ( 38%)]  Loss: 0.6224 (0.537)  Time: 0.358s,   44.66/s  (0.427s,   37.43/s)  LR: 1.000e-04  Data: 0.039 (0.098)\n",
      "Train: 0 [ 950/2388 ( 40%)]  Loss: 0.4933 (0.535)  Time: 0.337s,   47.44/s  (0.427s,   37.44/s)  LR: 1.000e-04  Data: 0.016 (0.098)\n",
      "Train: 0 [1000/2388 ( 42%)]  Loss: 0.6062 (0.534)  Time: 0.351s,   45.55/s  (0.427s,   37.43/s)  LR: 1.000e-04  Data: 0.031 (0.099)\n",
      "Train: 0 [1050/2388 ( 44%)]  Loss: 0.4697 (0.532)  Time: 0.338s,   47.30/s  (0.428s,   37.42/s)  LR: 1.000e-04  Data: 0.017 (0.099)\n",
      "Train: 0 [1100/2388 ( 46%)]  Loss: 0.5027 (0.531)  Time: 0.336s,   47.64/s  (0.427s,   37.46/s)  LR: 1.000e-04  Data: 0.016 (0.099)\n",
      "Train: 0 [1150/2388 ( 48%)]  Loss: 0.5311 (0.530)  Time: 0.337s,   47.49/s  (0.427s,   37.48/s)  LR: 1.000e-04  Data: 0.016 (0.099)\n",
      "Train: 0 [1200/2388 ( 50%)]  Loss: 0.4336 (0.530)  Time: 0.336s,   47.58/s  (0.426s,   37.54/s)  LR: 1.000e-04  Data: 0.016 (0.099)\n",
      "Train: 0 [1250/2388 ( 52%)]  Loss: 0.4986 (0.529)  Time: 0.336s,   47.61/s  (0.427s,   37.51/s)  LR: 1.000e-04  Data: 0.017 (0.099)\n",
      "Train: 0 [1300/2388 ( 54%)]  Loss: 0.5242 (0.528)  Time: 0.335s,   47.70/s  (0.426s,   37.55/s)  LR: 1.000e-04  Data: 0.015 (0.099)\n",
      "Train: 0 [1350/2388 ( 57%)]  Loss: 0.5611 (0.528)  Time: 0.337s,   47.53/s  (0.426s,   37.58/s)  LR: 1.000e-04  Data: 0.017 (0.099)\n",
      "Train: 0 [1400/2388 ( 59%)]  Loss: 0.5419 (0.527)  Time: 0.335s,   47.73/s  (0.425s,   37.62/s)  LR: 1.000e-04  Data: 0.016 (0.099)\n",
      "Train: 0 [1450/2388 ( 61%)]  Loss: 0.5425 (0.527)  Time: 0.340s,   47.02/s  (0.425s,   37.63/s)  LR: 1.000e-04  Data: 0.019 (0.099)\n",
      "Train: 0 [1500/2388 ( 63%)]  Loss: 0.4497 (0.525)  Time: 0.336s,   47.62/s  (0.425s,   37.65/s)  LR: 1.000e-04  Data: 0.015 (0.099)\n",
      "Train: 0 [1550/2388 ( 65%)]  Loss: 0.6349 (0.524)  Time: 0.668s,   23.93/s  (0.425s,   37.64/s)  LR: 1.000e-04  Data: 0.349 (0.099)\n",
      "Train: 0 [1600/2388 ( 67%)]  Loss: 0.4904 (0.524)  Time: 0.334s,   47.88/s  (0.425s,   37.68/s)  LR: 1.000e-04  Data: 0.015 (0.099)\n",
      "Train: 0 [1650/2388 ( 69%)]  Loss: 0.5303 (0.523)  Time: 0.339s,   47.16/s  (0.425s,   37.68/s)  LR: 1.000e-04  Data: 0.018 (0.099)\n",
      "Train: 0 [1700/2388 ( 71%)]  Loss: 0.4511 (0.523)  Time: 0.335s,   47.80/s  (0.425s,   37.67/s)  LR: 1.000e-04  Data: 0.015 (0.099)\n",
      "Train: 0 [1750/2388 ( 73%)]  Loss: 0.4799 (0.522)  Time: 0.337s,   47.43/s  (0.425s,   37.66/s)  LR: 1.000e-04  Data: 0.017 (0.100)\n",
      "Train: 0 [1800/2388 ( 75%)]  Loss: 0.6585 (0.522)  Time: 0.335s,   47.77/s  (0.424s,   37.70/s)  LR: 1.000e-04  Data: 0.015 (0.099)\n",
      "Train: 0 [1850/2388 ( 78%)]  Loss: 0.4592 (0.521)  Time: 0.778s,   20.56/s  (0.425s,   37.66/s)  LR: 1.000e-04  Data: 0.459 (0.100)\n",
      "Train: 0 [1900/2388 ( 80%)]  Loss: 0.5374 (0.521)  Time: 0.337s,   47.45/s  (0.425s,   37.68/s)  LR: 1.000e-04  Data: 0.017 (0.100)\n",
      "Train: 0 [1950/2388 ( 82%)]  Loss: 0.5154 (0.520)  Time: 0.651s,   24.57/s  (0.425s,   37.67/s)  LR: 1.000e-04  Data: 0.332 (0.100)\n",
      "Train: 0 [2000/2388 ( 84%)]  Loss: 0.4112 (0.520)  Time: 0.335s,   47.75/s  (0.425s,   37.67/s)  LR: 1.000e-04  Data: 0.016 (0.100)\n",
      "Train: 0 [2050/2388 ( 86%)]  Loss: 0.5008 (0.519)  Time: 0.717s,   22.33/s  (0.425s,   37.68/s)  LR: 1.000e-04  Data: 0.394 (0.100)\n",
      "Train: 0 [2100/2388 ( 88%)]  Loss: 0.6138 (0.519)  Time: 0.337s,   47.44/s  (0.425s,   37.67/s)  LR: 1.000e-04  Data: 0.017 (0.100)\n",
      "Train: 0 [2150/2388 ( 90%)]  Loss: 0.5147 (0.519)  Time: 0.827s,   19.36/s  (0.425s,   37.64/s)  LR: 1.000e-04  Data: 0.507 (0.101)\n",
      "Train: 0 [2200/2388 ( 92%)]  Loss: 0.4870 (0.518)  Time: 0.339s,   47.21/s  (0.425s,   37.66/s)  LR: 1.000e-04  Data: 0.019 (0.101)\n",
      "Train: 0 [2250/2388 ( 94%)]  Loss: 0.4592 (0.518)  Time: 0.637s,   25.12/s  (0.425s,   37.66/s)  LR: 1.000e-04  Data: 0.317 (0.101)\n",
      "Train: 0 [2300/2388 ( 96%)]  Loss: 0.5779 (0.517)  Time: 0.336s,   47.56/s  (0.425s,   37.68/s)  LR: 1.000e-04  Data: 0.016 (0.101)\n",
      "Train: 0 [2350/2388 ( 98%)]  Loss: 0.5185 (0.517)  Time: 0.662s,   24.16/s  (0.425s,   37.69/s)  LR: 1.000e-04  Data: 0.342 (0.101)\n",
      "Train: 0 [2387/2388 (100%)]  Loss: 0.5727 (0.516)  Time: 0.318s,   50.27/s  (0.424s,   37.71/s)  LR: 1.000e-04  Data: 0.000 (0.100)\n",
      "Test: [   0/2388]  Time: 1.496 (1.496)  Loss:  0.2366 (0.2366)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [  50/2388]  Time: 0.088 (0.303)  Loss:  0.1694 (0.1647)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 100/2388]  Time: 0.492 (0.294)  Loss:  0.1604 (0.1737)  Acc@1: 100.0000 (99.9381)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 150/2388]  Time: 0.088 (0.289)  Loss:  0.2983 (0.1921)  Acc@1: 100.0000 (99.9586)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 200/2388]  Time: 0.600 (0.288)  Loss:  0.2529 (0.2187)  Acc@1: 100.0000 (99.9067)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 250/2388]  Time: 0.087 (0.285)  Loss:  0.1827 (0.2298)  Acc@1: 100.0000 (99.9004)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 300/2388]  Time: 0.157 (0.285)  Loss:  0.2386 (0.2322)  Acc@1: 100.0000 (99.8339)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 350/2388]  Time: 0.094 (0.284)  Loss:  0.2042 (0.2316)  Acc@1: 100.0000 (99.8575)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 400/2388]  Time: 0.085 (0.284)  Loss:  0.2203 (0.2292)  Acc@1: 100.0000 (99.8441)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 450/2388]  Time: 0.525 (0.285)  Loss:  0.2283 (0.2359)  Acc@1: 100.0000 (99.8198)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 500/2388]  Time: 0.091 (0.286)  Loss:  0.2708 (0.2424)  Acc@1: 100.0000 (99.7505)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 550/2388]  Time: 0.881 (0.287)  Loss:  0.2244 (0.2471)  Acc@1: 100.0000 (99.7391)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 600/2388]  Time: 0.086 (0.286)  Loss:  0.1960 (0.2506)  Acc@1: 100.0000 (99.6568)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 650/2388]  Time: 0.953 (0.287)  Loss:  0.2625 (0.2518)  Acc@1: 100.0000 (99.6256)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 700/2388]  Time: 0.086 (0.286)  Loss:  0.3220 (0.2541)  Acc@1: 100.0000 (99.6077)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 750/2388]  Time: 0.863 (0.287)  Loss:  0.3281 (0.2603)  Acc@1: 93.7500 (99.5672)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 800/2388]  Time: 0.086 (0.286)  Loss:  0.2844 (0.2659)  Acc@1: 100.0000 (99.4616)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 850/2388]  Time: 0.935 (0.287)  Loss:  0.2367 (0.2703)  Acc@1: 100.0000 (99.4345)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 900/2388]  Time: 0.092 (0.286)  Loss:  0.3330 (0.2762)  Acc@1: 100.0000 (99.3757)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 950/2388]  Time: 0.857 (0.286)  Loss:  0.4568 (0.2821)  Acc@1: 93.7500 (99.3034)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1000/2388]  Time: 0.087 (0.286)  Loss:  0.2615 (0.2873)  Acc@1: 100.0000 (99.2820)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1050/2388]  Time: 0.660 (0.286)  Loss:  0.2366 (0.2888)  Acc@1: 100.0000 (99.2388)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1100/2388]  Time: 0.087 (0.286)  Loss:  0.2225 (0.2885)  Acc@1: 100.0000 (99.2450)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1150/2388]  Time: 0.346 (0.286)  Loss:  0.3684 (0.2888)  Acc@1: 100.0000 (99.2452)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1200/2388]  Time: 0.089 (0.286)  Loss:  0.3662 (0.2894)  Acc@1: 100.0000 (99.2506)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1250/2388]  Time: 0.087 (0.287)  Loss:  0.2446 (0.2890)  Acc@1: 100.0000 (99.2706)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1300/2388]  Time: 0.086 (0.287)  Loss:  0.1820 (0.2870)  Acc@1: 100.0000 (99.2890)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1350/2388]  Time: 0.088 (0.287)  Loss:  0.2195 (0.2849)  Acc@1: 100.0000 (99.3061)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1400/2388]  Time: 0.092 (0.287)  Loss:  0.2365 (0.2824)  Acc@1: 100.0000 (99.3264)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1450/2388]  Time: 0.088 (0.287)  Loss:  3.2109 (0.3579)  Acc@1:  0.0000 (96.8212)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1500/2388]  Time: 0.086 (0.287)  Loss:  3.0059 (0.4493)  Acc@1:  0.0000 (93.5959)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1550/2388]  Time: 0.087 (0.286)  Loss:  2.3867 (0.5312)  Acc@1:  0.0000 (90.5787)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1600/2388]  Time: 0.087 (0.286)  Loss:  3.1152 (0.5978)  Acc@1:  0.0000 (87.7537)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1650/2388]  Time: 0.090 (0.287)  Loss:  3.3203 (0.6756)  Acc@1:  0.0000 (85.0962)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1700/2388]  Time: 0.086 (0.286)  Loss:  2.7598 (0.7305)  Acc@1:  0.0000 (82.6021)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1750/2388]  Time: 0.088 (0.287)  Loss:  1.9248 (0.7650)  Acc@1:  0.0000 (80.2434)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1800/2388]  Time: 0.088 (0.286)  Loss:  1.9775 (0.8004)  Acc@1:  0.0000 (78.0226)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1850/2388]  Time: 0.089 (0.287)  Loss:  2.0098 (0.8380)  Acc@1:  0.0000 (75.9184)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1900/2388]  Time: 0.087 (0.286)  Loss:  2.0391 (0.8680)  Acc@1:  0.0000 (73.9348)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1950/2388]  Time: 0.088 (0.286)  Loss:  2.2715 (0.8934)  Acc@1:  0.0000 (72.0496)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2000/2388]  Time: 0.088 (0.286)  Loss:  1.8896 (0.9217)  Acc@1:  0.0000 (70.2711)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2050/2388]  Time: 0.089 (0.286)  Loss:  1.7568 (0.9431)  Acc@1:  0.0000 (68.5946)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2100/2388]  Time: 0.087 (0.286)  Loss:  1.5898 (0.9627)  Acc@1:  0.0000 (67.0068)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2150/2388]  Time: 0.091 (0.286)  Loss:  1.4229 (0.9774)  Acc@1:  0.0000 (65.5044)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2200/2388]  Time: 0.088 (0.286)  Loss:  2.0000 (0.9983)  Acc@1:  0.0000 (64.0164)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2250/2388]  Time: 0.091 (0.286)  Loss:  1.8574 (1.0170)  Acc@1:  0.0000 (62.5972)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2300/2388]  Time: 0.088 (0.286)  Loss:  2.0215 (1.0377)  Acc@1:  0.0000 (61.2451)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2350/2388]  Time: 0.091 (0.286)  Loss:  1.8926 (1.0611)  Acc@1:  0.0000 (59.9452)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2388/2388]  Time: 2.612 (0.287)  Loss:  2.8164 (1.1067)  Acc@1:  0.0000 (59.0133)  Acc@5: 100.0000 (100.0000)\n",
      "Current checkpoints:\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-0.pth.tar', 59.013347291285)\n",
      "\n",
      "Train: 1 [   0/2388 (  0%)]  Loss: 0.4969 (0.497)  Time: 1.879s,    8.52/s  (1.879s,    8.52/s)  LR: 1.673e-02  Data: 1.558 (1.558)\n",
      "Train: 1 [  50/2388 (  2%)]  Loss: 0.4532 (0.585)  Time: 0.342s,   46.83/s  (0.394s,   40.56/s)  LR: 1.673e-02  Data: 0.018 (0.074)\n",
      "Train: 1 [ 100/2388 (  4%)]  Loss: 0.4520 (0.559)  Time: 0.337s,   47.51/s  (0.383s,   41.76/s)  LR: 1.673e-02  Data: 0.017 (0.063)\n",
      "Train: 1 [ 150/2388 (  6%)]  Loss: 0.5088 (0.536)  Time: 0.464s,   34.48/s  (0.383s,   41.82/s)  LR: 1.673e-02  Data: 0.141 (0.062)\n",
      "Train: 1 [ 200/2388 (  8%)]  Loss: 0.4422 (0.527)  Time: 0.338s,   47.30/s  (0.381s,   42.01/s)  LR: 1.673e-02  Data: 0.018 (0.060)\n",
      "Train: 1 [ 250/2388 ( 10%)]  Loss: 0.4731 (0.521)  Time: 0.533s,   30.00/s  (0.383s,   41.80/s)  LR: 1.673e-02  Data: 0.213 (0.062)\n",
      "Train: 1 [ 300/2388 ( 13%)]  Loss: 0.4919 (0.514)  Time: 0.337s,   47.52/s  (0.381s,   41.98/s)  LR: 1.673e-02  Data: 0.015 (0.061)\n",
      "Train: 1 [ 350/2388 ( 15%)]  Loss: 0.5293 (0.508)  Time: 0.507s,   31.56/s  (0.381s,   41.98/s)  LR: 1.673e-02  Data: 0.187 (0.060)\n",
      "Train: 1 [ 400/2388 ( 17%)]  Loss: 0.5086 (0.505)  Time: 0.338s,   47.30/s  (0.379s,   42.21/s)  LR: 1.673e-02  Data: 0.018 (0.058)\n",
      "Train: 1 [ 450/2388 ( 19%)]  Loss: 0.5028 (0.502)  Time: 0.377s,   42.48/s  (0.378s,   42.37/s)  LR: 1.673e-02  Data: 0.054 (0.057)\n",
      "Train: 1 [ 500/2388 ( 21%)]  Loss: 0.4143 (0.497)  Time: 0.338s,   47.27/s  (0.377s,   42.45/s)  LR: 1.673e-02  Data: 0.018 (0.056)\n",
      "Train: 1 [ 550/2388 ( 23%)]  Loss: 0.3905 (0.496)  Time: 0.474s,   33.78/s  (0.376s,   42.56/s)  LR: 1.673e-02  Data: 0.152 (0.055)\n",
      "Train: 1 [ 600/2388 ( 25%)]  Loss: 0.4085 (0.493)  Time: 0.337s,   47.46/s  (0.375s,   42.68/s)  LR: 1.673e-02  Data: 0.016 (0.054)\n",
      "Train: 1 [ 650/2388 ( 27%)]  Loss: 0.5234 (0.492)  Time: 0.450s,   35.57/s  (0.374s,   42.75/s)  LR: 1.673e-02  Data: 0.130 (0.053)\n",
      "Train: 1 [ 700/2388 ( 29%)]  Loss: 0.3779 (0.489)  Time: 0.340s,   47.01/s  (0.373s,   42.87/s)  LR: 1.673e-02  Data: 0.019 (0.052)\n",
      "Train: 1 [ 750/2388 ( 31%)]  Loss: 0.4232 (0.487)  Time: 0.483s,   33.15/s  (0.372s,   43.00/s)  LR: 1.673e-02  Data: 0.161 (0.051)\n",
      "Train: 1 [ 800/2388 ( 34%)]  Loss: 0.4861 (0.486)  Time: 0.342s,   46.76/s  (0.372s,   43.01/s)  LR: 1.673e-02  Data: 0.020 (0.051)\n",
      "Train: 1 [ 850/2388 ( 36%)]  Loss: 0.3954 (0.485)  Time: 0.424s,   37.71/s  (0.372s,   43.05/s)  LR: 1.673e-02  Data: 0.099 (0.051)\n",
      "Train: 1 [ 900/2388 ( 38%)]  Loss: 0.4983 (0.484)  Time: 0.337s,   47.43/s  (0.372s,   43.03/s)  LR: 1.673e-02  Data: 0.017 (0.051)\n",
      "Train: 1 [ 950/2388 ( 40%)]  Loss: 0.5517 (0.482)  Time: 0.401s,   39.94/s  (0.371s,   43.08/s)  LR: 1.673e-02  Data: 0.079 (0.051)\n",
      "Train: 1 [1000/2388 ( 42%)]  Loss: 0.4368 (0.480)  Time: 0.339s,   47.21/s  (0.371s,   43.10/s)  LR: 1.673e-02  Data: 0.018 (0.050)\n",
      "Train: 1 [1050/2388 ( 44%)]  Loss: 0.5419 (0.479)  Time: 0.431s,   37.16/s  (0.371s,   43.08/s)  LR: 1.673e-02  Data: 0.109 (0.051)\n",
      "Train: 1 [1100/2388 ( 46%)]  Loss: 0.4610 (0.479)  Time: 0.339s,   47.19/s  (0.371s,   43.13/s)  LR: 1.673e-02  Data: 0.018 (0.050)\n",
      "Train: 1 [1150/2388 ( 48%)]  Loss: 0.4378 (0.478)  Time: 0.373s,   42.87/s  (0.371s,   43.13/s)  LR: 1.673e-02  Data: 0.052 (0.050)\n",
      "Train: 1 [1200/2388 ( 50%)]  Loss: 0.5212 (0.477)  Time: 0.338s,   47.29/s  (0.371s,   43.15/s)  LR: 1.673e-02  Data: 0.018 (0.050)\n",
      "Train: 1 [1250/2388 ( 52%)]  Loss: 0.4472 (0.476)  Time: 0.426s,   37.56/s  (0.371s,   43.15/s)  LR: 1.673e-02  Data: 0.105 (0.050)\n",
      "Train: 1 [1300/2388 ( 54%)]  Loss: 0.4583 (0.475)  Time: 0.339s,   47.15/s  (0.371s,   43.18/s)  LR: 1.673e-02  Data: 0.018 (0.050)\n",
      "Train: 1 [1350/2388 ( 57%)]  Loss: 0.5422 (0.474)  Time: 0.429s,   37.30/s  (0.370s,   43.22/s)  LR: 1.673e-02  Data: 0.107 (0.049)\n",
      "Train: 1 [1400/2388 ( 59%)]  Loss: 0.5530 (0.474)  Time: 0.339s,   47.17/s  (0.370s,   43.25/s)  LR: 1.673e-02  Data: 0.018 (0.049)\n",
      "Train: 1 [1450/2388 ( 61%)]  Loss: 0.3821 (0.473)  Time: 0.443s,   36.14/s  (0.370s,   43.26/s)  LR: 1.673e-02  Data: 0.122 (0.049)\n",
      "Train: 1 [1500/2388 ( 63%)]  Loss: 0.4835 (0.473)  Time: 0.342s,   46.80/s  (0.370s,   43.28/s)  LR: 1.673e-02  Data: 0.018 (0.049)\n",
      "Train: 1 [1550/2388 ( 65%)]  Loss: 0.4341 (0.472)  Time: 0.437s,   36.58/s  (0.370s,   43.30/s)  LR: 1.673e-02  Data: 0.109 (0.049)\n",
      "Train: 1 [1600/2388 ( 67%)]  Loss: 0.4201 (0.472)  Time: 0.340s,   47.07/s  (0.369s,   43.33/s)  LR: 1.673e-02  Data: 0.018 (0.048)\n",
      "Train: 1 [1650/2388 ( 69%)]  Loss: 0.3999 (0.471)  Time: 0.488s,   32.81/s  (0.369s,   43.32/s)  LR: 1.673e-02  Data: 0.165 (0.048)\n",
      "Train: 1 [1700/2388 ( 71%)]  Loss: 0.5415 (0.471)  Time: 0.340s,   47.12/s  (0.369s,   43.34/s)  LR: 1.673e-02  Data: 0.019 (0.048)\n",
      "Train: 1 [1750/2388 ( 73%)]  Loss: 0.5311 (0.470)  Time: 0.397s,   40.30/s  (0.369s,   43.34/s)  LR: 1.673e-02  Data: 0.077 (0.048)\n",
      "Train: 1 [1800/2388 ( 75%)]  Loss: 0.4429 (0.470)  Time: 0.337s,   47.52/s  (0.369s,   43.36/s)  LR: 1.673e-02  Data: 0.017 (0.048)\n",
      "Train: 1 [1850/2388 ( 78%)]  Loss: 0.4618 (0.469)  Time: 0.399s,   40.07/s  (0.369s,   43.35/s)  LR: 1.673e-02  Data: 0.078 (0.048)\n",
      "Train: 1 [1900/2388 ( 80%)]  Loss: 0.5659 (0.469)  Time: 0.339s,   47.23/s  (0.369s,   43.37/s)  LR: 1.673e-02  Data: 0.018 (0.048)\n",
      "Train: 1 [1950/2388 ( 82%)]  Loss: 0.3564 (0.468)  Time: 0.474s,   33.76/s  (0.369s,   43.36/s)  LR: 1.673e-02  Data: 0.153 (0.048)\n",
      "Train: 1 [2000/2388 ( 84%)]  Loss: 0.4154 (0.467)  Time: 0.339s,   47.14/s  (0.369s,   43.38/s)  LR: 1.673e-02  Data: 0.018 (0.048)\n",
      "Train: 1 [2050/2388 ( 86%)]  Loss: 0.4474 (0.467)  Time: 0.548s,   29.20/s  (0.369s,   43.38/s)  LR: 1.673e-02  Data: 0.226 (0.048)\n",
      "Train: 1 [2100/2388 ( 88%)]  Loss: 0.4412 (0.467)  Time: 0.338s,   47.36/s  (0.369s,   43.41/s)  LR: 1.673e-02  Data: 0.017 (0.048)\n",
      "Train: 1 [2150/2388 ( 90%)]  Loss: 0.4015 (0.466)  Time: 0.419s,   38.18/s  (0.368s,   43.42/s)  LR: 1.673e-02  Data: 0.099 (0.048)\n",
      "Train: 1 [2200/2388 ( 92%)]  Loss: 0.4647 (0.466)  Time: 0.337s,   47.47/s  (0.368s,   43.44/s)  LR: 1.673e-02  Data: 0.017 (0.047)\n",
      "Train: 1 [2250/2388 ( 94%)]  Loss: 0.4379 (0.466)  Time: 0.385s,   41.58/s  (0.368s,   43.45/s)  LR: 1.673e-02  Data: 0.063 (0.047)\n",
      "Train: 1 [2300/2388 ( 96%)]  Loss: 0.3793 (0.465)  Time: 0.338s,   47.36/s  (0.368s,   43.48/s)  LR: 1.673e-02  Data: 0.017 (0.047)\n",
      "Train: 1 [2350/2388 ( 98%)]  Loss: 0.5482 (0.465)  Time: 0.405s,   39.47/s  (0.368s,   43.48/s)  LR: 1.673e-02  Data: 0.084 (0.047)\n",
      "Train: 1 [2387/2388 (100%)]  Loss: 0.4925 (0.465)  Time: 0.319s,   50.14/s  (0.368s,   43.49/s)  LR: 1.673e-02  Data: 0.000 (0.047)\n",
      "Test: [   0/2388]  Time: 1.242 (1.242)  Loss:  0.3069 (0.3069)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [  50/2388]  Time: 0.089 (0.289)  Loss:  0.2297 (0.3020)  Acc@1: 100.0000 (99.7549)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 100/2388]  Time: 0.663 (0.290)  Loss:  0.3167 (0.3064)  Acc@1: 100.0000 (99.7525)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 150/2388]  Time: 0.087 (0.285)  Loss:  0.4250 (0.3101)  Acc@1: 100.0000 (99.0894)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 200/2388]  Time: 0.861 (0.287)  Loss:  0.4089 (0.3282)  Acc@1: 93.7500 (98.0721)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 250/2388]  Time: 0.087 (0.285)  Loss:  0.2549 (0.3313)  Acc@1: 100.0000 (98.0578)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 300/2388]  Time: 0.885 (0.286)  Loss:  0.1871 (0.3250)  Acc@1: 100.0000 (98.2558)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 350/2388]  Time: 0.086 (0.284)  Loss:  0.3110 (0.3215)  Acc@1: 100.0000 (98.4152)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 400/2388]  Time: 0.860 (0.284)  Loss:  0.2671 (0.3168)  Acc@1: 100.0000 (98.5193)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 450/2388]  Time: 0.086 (0.284)  Loss:  0.2184 (0.3105)  Acc@1: 100.0000 (98.5865)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 500/2388]  Time: 0.846 (0.285)  Loss:  0.4197 (0.3036)  Acc@1: 100.0000 (98.7026)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 550/2388]  Time: 0.086 (0.285)  Loss:  0.2002 (0.2995)  Acc@1: 100.0000 (98.7636)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 600/2388]  Time: 0.893 (0.285)  Loss:  0.3047 (0.2983)  Acc@1: 100.0000 (98.7417)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 650/2388]  Time: 0.085 (0.284)  Loss:  0.2856 (0.2973)  Acc@1: 100.0000 (98.7615)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 700/2388]  Time: 0.810 (0.285)  Loss:  0.3364 (0.2968)  Acc@1: 93.7500 (98.7874)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 750/2388]  Time: 0.087 (0.284)  Loss:  0.2507 (0.2965)  Acc@1: 100.0000 (98.7766)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 800/2388]  Time: 0.918 (0.285)  Loss:  0.3196 (0.2964)  Acc@1: 100.0000 (98.7594)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 850/2388]  Time: 0.086 (0.284)  Loss:  0.3457 (0.2962)  Acc@1: 100.0000 (98.7588)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 900/2388]  Time: 0.824 (0.285)  Loss:  0.1700 (0.2952)  Acc@1: 100.0000 (98.7306)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 950/2388]  Time: 0.086 (0.285)  Loss:  0.3850 (0.2952)  Acc@1: 100.0000 (98.7513)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1000/2388]  Time: 0.853 (0.285)  Loss:  0.3667 (0.2951)  Acc@1: 100.0000 (98.7887)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1050/2388]  Time: 0.086 (0.284)  Loss:  0.3240 (0.2947)  Acc@1: 100.0000 (98.8166)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1100/2388]  Time: 0.907 (0.285)  Loss:  0.3162 (0.2929)  Acc@1: 100.0000 (98.8533)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1150/2388]  Time: 0.086 (0.285)  Loss:  0.4128 (0.2918)  Acc@1: 100.0000 (98.8814)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1200/2388]  Time: 0.941 (0.285)  Loss:  0.2484 (0.2906)  Acc@1: 100.0000 (98.9176)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1250/2388]  Time: 0.086 (0.285)  Loss:  0.2253 (0.2888)  Acc@1: 100.0000 (98.9608)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1300/2388]  Time: 0.861 (0.285)  Loss:  0.2568 (0.2885)  Acc@1: 100.0000 (98.9623)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1350/2388]  Time: 0.085 (0.285)  Loss:  0.3286 (0.2874)  Acc@1: 100.0000 (98.9961)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1400/2388]  Time: 0.895 (0.286)  Loss:  0.3213 (0.2880)  Acc@1: 100.0000 (99.0141)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1450/2388]  Time: 0.086 (0.285)  Loss:  3.0137 (0.3478)  Acc@1:  0.0000 (96.5024)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1500/2388]  Time: 0.905 (0.286)  Loss:  2.3105 (0.4230)  Acc@1:  0.0000 (93.2878)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1550/2388]  Time: 0.086 (0.285)  Loss:  3.1367 (0.4943)  Acc@1:  0.0000 (90.2805)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1600/2388]  Time: 0.894 (0.286)  Loss:  2.6406 (0.5605)  Acc@1:  0.0000 (87.4610)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1650/2388]  Time: 0.086 (0.285)  Loss:  3.1484 (0.6281)  Acc@1:  0.0000 (84.8122)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1700/2388]  Time: 0.832 (0.285)  Loss:  1.2656 (0.6562)  Acc@1: 12.5000 (82.4111)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1750/2388]  Time: 0.086 (0.285)  Loss:  0.6162 (0.6760)  Acc@1: 100.0000 (80.3612)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1800/2388]  Time: 0.851 (0.285)  Loss:  1.6104 (0.6954)  Acc@1:  0.0000 (78.3454)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1850/2388]  Time: 0.085 (0.285)  Loss:  1.5059 (0.7171)  Acc@1: 25.0000 (76.3743)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1900/2388]  Time: 0.855 (0.285)  Loss:  1.6689 (0.7438)  Acc@1:  6.2500 (74.5463)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1950/2388]  Time: 0.086 (0.285)  Loss:  1.5352 (0.7651)  Acc@1:  0.0000 (72.8056)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2000/2388]  Time: 0.825 (0.285)  Loss:  1.1572 (0.7809)  Acc@1:  0.0000 (71.1644)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2050/2388]  Time: 0.086 (0.285)  Loss:  1.2998 (0.7968)  Acc@1:  0.0000 (69.5697)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2100/2388]  Time: 0.858 (0.285)  Loss:  1.3086 (0.8105)  Acc@1: 18.7500 (68.1431)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2150/2388]  Time: 0.085 (0.285)  Loss:  0.8867 (0.8234)  Acc@1: 50.0000 (66.7596)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2200/2388]  Time: 0.884 (0.285)  Loss:  1.4648 (0.8388)  Acc@1:  0.0000 (65.3964)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2250/2388]  Time: 0.086 (0.285)  Loss:  1.5342 (0.8574)  Acc@1: 12.5000 (64.0215)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2300/2388]  Time: 0.846 (0.286)  Loss:  2.0195 (0.8768)  Acc@1:  0.0000 (62.7309)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2350/2388]  Time: 0.086 (0.286)  Loss:  1.1807 (0.8953)  Acc@1: 12.5000 (61.4632)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2388/2388]  Time: 0.015 (0.285)  Loss:  3.2422 (0.9338)  Acc@1:  0.0000 (60.5287)  Acc@5: 100.0000 (100.0000)\n",
      "Current checkpoints:\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-1.pth.tar', 60.52865741952368)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-0.pth.tar', 59.013347291285)\n",
      "\n",
      "Train: 2 [   0/2388 (  0%)]  Loss: 0.3833 (0.383)  Time: 1.850s,    8.65/s  (1.850s,    8.65/s)  LR: 3.337e-02  Data: 1.529 (1.529)\n",
      "Train: 2 [  50/2388 (  2%)]  Loss: 0.4765 (0.460)  Time: 0.340s,   47.08/s  (0.401s,   39.86/s)  LR: 3.337e-02  Data: 0.019 (0.080)\n",
      "Train: 2 [ 100/2388 (  4%)]  Loss: 0.4215 (0.453)  Time: 0.343s,   46.66/s  (0.383s,   41.81/s)  LR: 3.337e-02  Data: 0.018 (0.062)\n",
      "Train: 2 [ 150/2388 (  6%)]  Loss: 0.3997 (0.453)  Time: 0.338s,   47.32/s  (0.377s,   42.45/s)  LR: 3.337e-02  Data: 0.018 (0.056)\n",
      "Train: 2 [ 200/2388 (  8%)]  Loss: 0.4967 (0.450)  Time: 0.337s,   47.52/s  (0.375s,   42.63/s)  LR: 3.337e-02  Data: 0.016 (0.054)\n",
      "Train: 2 [ 250/2388 ( 10%)]  Loss: 0.4861 (0.449)  Time: 0.341s,   46.90/s  (0.373s,   42.88/s)  LR: 3.337e-02  Data: 0.021 (0.052)\n",
      "Train: 2 [ 300/2388 ( 13%)]  Loss: 0.3944 (0.448)  Time: 0.340s,   47.06/s  (0.371s,   43.08/s)  LR: 3.337e-02  Data: 0.019 (0.050)\n",
      "Train: 2 [ 350/2388 ( 15%)]  Loss: 0.5104 (0.446)  Time: 0.349s,   45.85/s  (0.370s,   43.27/s)  LR: 3.337e-02  Data: 0.028 (0.049)\n",
      "Train: 2 [ 400/2388 ( 17%)]  Loss: 0.3650 (0.446)  Time: 0.339s,   47.20/s  (0.370s,   43.27/s)  LR: 3.337e-02  Data: 0.019 (0.049)\n",
      "Train: 2 [ 450/2388 ( 19%)]  Loss: 0.3745 (0.444)  Time: 0.361s,   44.36/s  (0.369s,   43.38/s)  LR: 3.337e-02  Data: 0.039 (0.048)\n",
      "Train: 2 [ 500/2388 ( 21%)]  Loss: 0.5956 (0.444)  Time: 0.346s,   46.18/s  (0.369s,   43.36/s)  LR: 3.337e-02  Data: 0.024 (0.048)\n",
      "Train: 2 [ 550/2388 ( 23%)]  Loss: 0.4650 (0.445)  Time: 0.445s,   35.92/s  (0.369s,   43.34/s)  LR: 3.337e-02  Data: 0.125 (0.048)\n",
      "Train: 2 [ 600/2388 ( 25%)]  Loss: 0.3964 (0.445)  Time: 0.347s,   46.07/s  (0.368s,   43.49/s)  LR: 3.337e-02  Data: 0.026 (0.047)\n",
      "Train: 2 [ 650/2388 ( 27%)]  Loss: 0.4208 (0.446)  Time: 0.409s,   39.12/s  (0.368s,   43.51/s)  LR: 3.337e-02  Data: 0.089 (0.047)\n",
      "Train: 2 [ 700/2388 ( 29%)]  Loss: 0.3968 (0.445)  Time: 0.342s,   46.72/s  (0.368s,   43.52/s)  LR: 3.337e-02  Data: 0.021 (0.047)\n",
      "Train: 2 [ 750/2388 ( 31%)]  Loss: 0.4263 (0.446)  Time: 0.495s,   32.33/s  (0.368s,   43.51/s)  LR: 3.337e-02  Data: 0.174 (0.047)\n",
      "Train: 2 [ 800/2388 ( 34%)]  Loss: 0.4345 (0.446)  Time: 0.341s,   46.97/s  (0.368s,   43.46/s)  LR: 3.337e-02  Data: 0.021 (0.047)\n",
      "Train: 2 [ 850/2388 ( 36%)]  Loss: 0.4842 (0.446)  Time: 0.372s,   43.06/s  (0.368s,   43.44/s)  LR: 3.337e-02  Data: 0.050 (0.047)\n",
      "Train: 2 [ 900/2388 ( 38%)]  Loss: 0.4303 (0.446)  Time: 0.337s,   47.44/s  (0.368s,   43.45/s)  LR: 3.337e-02  Data: 0.017 (0.047)\n",
      "Train: 2 [ 950/2388 ( 40%)]  Loss: 0.4531 (0.446)  Time: 0.400s,   39.95/s  (0.368s,   43.44/s)  LR: 3.337e-02  Data: 0.081 (0.047)\n",
      "Train: 2 [1000/2388 ( 42%)]  Loss: 0.3970 (0.445)  Time: 0.341s,   46.89/s  (0.368s,   43.49/s)  LR: 3.337e-02  Data: 0.018 (0.047)\n",
      "Train: 2 [1050/2388 ( 44%)]  Loss: 0.4950 (0.445)  Time: 0.424s,   37.76/s  (0.368s,   43.50/s)  LR: 3.337e-02  Data: 0.102 (0.047)\n",
      "Train: 2 [1100/2388 ( 46%)]  Loss: 0.3904 (0.445)  Time: 0.341s,   46.87/s  (0.367s,   43.54/s)  LR: 3.337e-02  Data: 0.021 (0.047)\n",
      "Train: 2 [1150/2388 ( 48%)]  Loss: 0.3737 (0.445)  Time: 0.429s,   37.30/s  (0.368s,   43.53/s)  LR: 3.337e-02  Data: 0.109 (0.047)\n",
      "Train: 2 [1200/2388 ( 50%)]  Loss: 0.4951 (0.445)  Time: 0.352s,   45.44/s  (0.368s,   43.51/s)  LR: 3.337e-02  Data: 0.031 (0.047)\n",
      "Train: 2 [1250/2388 ( 52%)]  Loss: 0.4131 (0.445)  Time: 0.500s,   32.01/s  (0.368s,   43.51/s)  LR: 3.337e-02  Data: 0.178 (0.047)\n",
      "Train: 2 [1300/2388 ( 54%)]  Loss: 0.4249 (0.445)  Time: 0.356s,   44.90/s  (0.368s,   43.49/s)  LR: 3.337e-02  Data: 0.036 (0.047)\n",
      "Train: 2 [1350/2388 ( 57%)]  Loss: 0.4569 (0.444)  Time: 0.466s,   34.35/s  (0.368s,   43.51/s)  LR: 3.337e-02  Data: 0.146 (0.047)\n",
      "Train: 2 [1400/2388 ( 59%)]  Loss: 0.4283 (0.444)  Time: 0.345s,   46.34/s  (0.368s,   43.51/s)  LR: 3.337e-02  Data: 0.025 (0.047)\n",
      "Train: 2 [1450/2388 ( 61%)]  Loss: 0.4657 (0.444)  Time: 0.383s,   41.80/s  (0.368s,   43.50/s)  LR: 3.337e-02  Data: 0.063 (0.047)\n",
      "Train: 2 [1500/2388 ( 63%)]  Loss: 0.5281 (0.444)  Time: 0.350s,   45.72/s  (0.368s,   43.50/s)  LR: 3.337e-02  Data: 0.028 (0.047)\n",
      "Train: 2 [1550/2388 ( 65%)]  Loss: 0.4296 (0.444)  Time: 0.461s,   34.69/s  (0.368s,   43.48/s)  LR: 3.337e-02  Data: 0.140 (0.047)\n",
      "Train: 2 [1600/2388 ( 67%)]  Loss: 0.3969 (0.444)  Time: 0.346s,   46.23/s  (0.368s,   43.47/s)  LR: 3.337e-02  Data: 0.024 (0.047)\n",
      "Train: 2 [1650/2388 ( 69%)]  Loss: 0.4011 (0.444)  Time: 0.423s,   37.79/s  (0.368s,   43.48/s)  LR: 3.337e-02  Data: 0.103 (0.047)\n",
      "Train: 2 [1700/2388 ( 71%)]  Loss: 0.3927 (0.444)  Time: 0.341s,   46.97/s  (0.368s,   43.47/s)  LR: 3.337e-02  Data: 0.021 (0.047)\n",
      "Train: 2 [1750/2388 ( 73%)]  Loss: 0.3805 (0.444)  Time: 0.426s,   37.57/s  (0.368s,   43.46/s)  LR: 3.337e-02  Data: 0.105 (0.047)\n",
      "Train: 2 [1800/2388 ( 75%)]  Loss: 0.3928 (0.443)  Time: 0.367s,   43.59/s  (0.368s,   43.46/s)  LR: 3.337e-02  Data: 0.046 (0.047)\n",
      "Train: 2 [1850/2388 ( 78%)]  Loss: 0.3775 (0.443)  Time: 0.435s,   36.76/s  (0.368s,   43.43/s)  LR: 3.337e-02  Data: 0.114 (0.048)\n",
      "Train: 2 [1900/2388 ( 80%)]  Loss: 0.5585 (0.443)  Time: 0.339s,   47.27/s  (0.369s,   43.41/s)  LR: 3.337e-02  Data: 0.018 (0.048)\n",
      "Train: 2 [1950/2388 ( 82%)]  Loss: 0.5207 (0.443)  Time: 0.474s,   33.73/s  (0.369s,   43.39/s)  LR: 3.337e-02  Data: 0.153 (0.048)\n",
      "Train: 2 [2000/2388 ( 84%)]  Loss: 0.3724 (0.442)  Time: 0.343s,   46.71/s  (0.369s,   43.41/s)  LR: 3.337e-02  Data: 0.022 (0.048)\n",
      "Train: 2 [2050/2388 ( 86%)]  Loss: 0.4856 (0.442)  Time: 0.647s,   24.73/s  (0.369s,   43.39/s)  LR: 3.337e-02  Data: 0.327 (0.048)\n",
      "Train: 2 [2100/2388 ( 88%)]  Loss: 0.4465 (0.442)  Time: 0.359s,   44.53/s  (0.369s,   43.37/s)  LR: 3.337e-02  Data: 0.039 (0.048)\n",
      "Train: 2 [2150/2388 ( 90%)]  Loss: 0.3991 (0.442)  Time: 0.529s,   30.26/s  (0.369s,   43.37/s)  LR: 3.337e-02  Data: 0.208 (0.048)\n",
      "Train: 2 [2200/2388 ( 92%)]  Loss: 0.4432 (0.442)  Time: 0.351s,   45.60/s  (0.369s,   43.39/s)  LR: 3.337e-02  Data: 0.030 (0.048)\n",
      "Train: 2 [2250/2388 ( 94%)]  Loss: 0.3916 (0.442)  Time: 0.578s,   27.70/s  (0.369s,   43.38/s)  LR: 3.337e-02  Data: 0.257 (0.048)\n",
      "Train: 2 [2300/2388 ( 96%)]  Loss: 0.4708 (0.442)  Time: 0.340s,   47.10/s  (0.369s,   43.37/s)  LR: 3.337e-02  Data: 0.019 (0.048)\n",
      "Train: 2 [2350/2388 ( 98%)]  Loss: 0.4623 (0.442)  Time: 0.425s,   37.61/s  (0.369s,   43.38/s)  LR: 3.337e-02  Data: 0.104 (0.048)\n",
      "Train: 2 [2387/2388 (100%)]  Loss: 0.4099 (0.441)  Time: 0.318s,   50.35/s  (0.369s,   43.37/s)  LR: 3.337e-02  Data: 0.000 (0.048)\n",
      "Test: [   0/2388]  Time: 1.351 (1.351)  Loss:  0.3098 (0.3098)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [  50/2388]  Time: 0.086 (0.295)  Loss:  0.2817 (0.3915)  Acc@1: 100.0000 (95.5882)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 100/2388]  Time: 0.095 (0.292)  Loss:  0.4395 (0.4073)  Acc@1: 93.7500 (94.8639)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 150/2388]  Time: 0.091 (0.287)  Loss:  0.4050 (0.4068)  Acc@1: 100.0000 (94.2467)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 200/2388]  Time: 0.089 (0.288)  Loss:  0.7642 (0.4115)  Acc@1: 81.2500 (94.0920)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 250/2388]  Time: 0.087 (0.286)  Loss:  0.3037 (0.4082)  Acc@1: 100.0000 (94.1484)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 300/2388]  Time: 0.091 (0.288)  Loss:  0.2922 (0.4050)  Acc@1: 93.7500 (94.0615)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 350/2388]  Time: 0.088 (0.286)  Loss:  0.4653 (0.4088)  Acc@1: 93.7500 (93.9103)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 400/2388]  Time: 0.091 (0.287)  Loss:  0.3416 (0.4118)  Acc@1: 93.7500 (93.6097)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 450/2388]  Time: 0.091 (0.287)  Loss:  0.2411 (0.4051)  Acc@1: 93.7500 (93.7084)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 500/2388]  Time: 0.093 (0.289)  Loss:  0.6436 (0.3963)  Acc@1: 81.2500 (93.8623)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 550/2388]  Time: 0.088 (0.289)  Loss:  0.2607 (0.3922)  Acc@1: 100.0000 (93.9882)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 600/2388]  Time: 0.091 (0.288)  Loss:  0.6050 (0.3994)  Acc@1: 87.5000 (93.7292)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 650/2388]  Time: 0.088 (0.287)  Loss:  0.3665 (0.4045)  Acc@1: 93.7500 (93.5580)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 700/2388]  Time: 0.092 (0.287)  Loss:  0.5649 (0.4114)  Acc@1: 87.5000 (93.2596)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 750/2388]  Time: 0.088 (0.286)  Loss:  0.3757 (0.4183)  Acc@1: 93.7500 (93.1508)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 800/2388]  Time: 0.091 (0.287)  Loss:  0.3777 (0.4239)  Acc@1: 100.0000 (93.0009)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 850/2388]  Time: 0.087 (0.286)  Loss:  0.2534 (0.4303)  Acc@1: 100.0000 (92.8981)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 900/2388]  Time: 0.091 (0.286)  Loss:  0.3601 (0.4309)  Acc@1: 93.7500 (92.9037)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 950/2388]  Time: 0.088 (0.286)  Loss:  0.5415 (0.4315)  Acc@1: 75.0000 (92.8891)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1000/2388]  Time: 0.093 (0.286)  Loss:  0.4548 (0.4314)  Acc@1: 93.7500 (92.9008)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1050/2388]  Time: 0.087 (0.286)  Loss:  0.2959 (0.4312)  Acc@1: 100.0000 (92.9234)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1100/2388]  Time: 0.091 (0.286)  Loss:  0.4238 (0.4305)  Acc@1: 93.7500 (93.0064)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1150/2388]  Time: 0.087 (0.285)  Loss:  0.7622 (0.4302)  Acc@1: 68.7500 (93.0495)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1200/2388]  Time: 0.093 (0.286)  Loss:  0.4846 (0.4292)  Acc@1: 93.7500 (93.1255)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1250/2388]  Time: 0.087 (0.286)  Loss:  0.3533 (0.4258)  Acc@1: 100.0000 (93.2604)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1300/2388]  Time: 0.115 (0.286)  Loss:  0.2859 (0.4227)  Acc@1: 100.0000 (93.3369)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1350/2388]  Time: 0.088 (0.286)  Loss:  0.3625 (0.4190)  Acc@1: 93.7500 (93.4585)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1400/2388]  Time: 0.093 (0.287)  Loss:  0.3145 (0.4157)  Acc@1: 93.7500 (93.5894)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1450/2388]  Time: 0.087 (0.287)  Loss:  2.2168 (0.4625)  Acc@1: 25.0000 (91.4369)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1500/2388]  Time: 0.089 (0.287)  Loss:  2.6328 (0.5267)  Acc@1:  0.0000 (88.6159)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1550/2388]  Time: 0.087 (0.287)  Loss:  3.4238 (0.5825)  Acc@1:  0.0000 (86.0896)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1600/2388]  Time: 0.091 (0.287)  Loss:  3.0000 (0.6389)  Acc@1:  0.0000 (83.5572)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1650/2388]  Time: 0.088 (0.287)  Loss:  3.2812 (0.7028)  Acc@1:  0.0000 (81.0759)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1700/2388]  Time: 0.092 (0.287)  Loss:  0.5703 (0.7129)  Acc@1: 75.0000 (79.8354)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1750/2388]  Time: 0.087 (0.287)  Loss:  1.0508 (0.7248)  Acc@1: 50.0000 (78.5872)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1800/2388]  Time: 0.092 (0.287)  Loss:  1.2148 (0.7363)  Acc@1: 31.2500 (77.3320)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1850/2388]  Time: 0.087 (0.287)  Loss:  1.9365 (0.7476)  Acc@1:  0.0000 (76.1008)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1900/2388]  Time: 0.093 (0.287)  Loss:  1.6396 (0.7561)  Acc@1:  6.2500 (75.2038)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1950/2388]  Time: 0.087 (0.287)  Loss:  1.4150 (0.7652)  Acc@1: 12.5000 (74.1094)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2000/2388]  Time: 0.093 (0.287)  Loss:  0.7485 (0.7690)  Acc@1: 50.0000 (73.2415)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2050/2388]  Time: 0.088 (0.287)  Loss:  1.4414 (0.7751)  Acc@1: 25.0000 (72.2117)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2100/2388]  Time: 0.091 (0.287)  Loss:  0.4712 (0.7773)  Acc@1: 68.7500 (71.5106)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2150/2388]  Time: 0.088 (0.287)  Loss:  0.3113 (0.7805)  Acc@1: 81.2500 (70.7839)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2200/2388]  Time: 0.093 (0.287)  Loss:  1.4043 (0.7864)  Acc@1: 18.7500 (69.9852)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2250/2388]  Time: 0.089 (0.287)  Loss:  0.8633 (0.7951)  Acc@1: 43.7500 (69.0471)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2300/2388]  Time: 0.091 (0.287)  Loss:  1.3027 (0.8043)  Acc@1: 31.2500 (68.2095)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2350/2388]  Time: 0.088 (0.287)  Loss:  0.9248 (0.8127)  Acc@1: 50.0000 (67.4048)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2388/2388]  Time: 0.014 (0.287)  Loss:  2.6406 (0.8414)  Acc@1:  0.0000 (66.4224)  Acc@5: 100.0000 (100.0000)\n",
      "Current checkpoints:\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-2.pth.tar', 66.4224025124313)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-1.pth.tar', 60.52865741952368)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-0.pth.tar', 59.013347291285)\n",
      "\n",
      "Train: 3 [   0/2388 (  0%)]  Loss: 0.4825 (0.483)  Time: 1.870s,    8.55/s  (1.870s,    8.55/s)  LR: 4.728e-02  Data: 1.550 (1.550)\n",
      "Train: 3 [  50/2388 (  2%)]  Loss: 0.4421 (0.434)  Time: 0.339s,   47.24/s  (0.399s,   40.12/s)  LR: 4.728e-02  Data: 0.017 (0.078)\n",
      "Train: 3 [ 100/2388 (  4%)]  Loss: 0.4323 (0.432)  Time: 0.340s,   47.06/s  (0.383s,   41.73/s)  LR: 4.728e-02  Data: 0.018 (0.062)\n",
      "Train: 3 [ 150/2388 (  6%)]  Loss: 0.3944 (0.432)  Time: 0.343s,   46.62/s  (0.375s,   42.69/s)  LR: 4.728e-02  Data: 0.021 (0.054)\n",
      "Train: 3 [ 200/2388 (  8%)]  Loss: 0.3839 (0.431)  Time: 0.359s,   44.56/s  (0.372s,   42.96/s)  LR: 4.728e-02  Data: 0.038 (0.051)\n",
      "Train: 3 [ 250/2388 ( 10%)]  Loss: 0.4293 (0.431)  Time: 0.338s,   47.28/s  (0.371s,   43.13/s)  LR: 4.728e-02  Data: 0.018 (0.050)\n",
      "Train: 3 [ 300/2388 ( 13%)]  Loss: 0.4476 (0.432)  Time: 0.471s,   33.97/s  (0.370s,   43.25/s)  LR: 4.728e-02  Data: 0.150 (0.049)\n",
      "Train: 3 [ 350/2388 ( 15%)]  Loss: 0.5371 (0.432)  Time: 0.342s,   46.84/s  (0.370s,   43.24/s)  LR: 4.728e-02  Data: 0.019 (0.049)\n",
      "Train: 3 [ 400/2388 ( 17%)]  Loss: 0.3710 (0.432)  Time: 0.461s,   34.67/s  (0.370s,   43.26/s)  LR: 4.728e-02  Data: 0.141 (0.049)\n",
      "Train: 3 [ 450/2388 ( 19%)]  Loss: 0.4285 (0.433)  Time: 0.338s,   47.32/s  (0.369s,   43.32/s)  LR: 4.728e-02  Data: 0.018 (0.048)\n",
      "Train: 3 [ 500/2388 ( 21%)]  Loss: 0.4560 (0.434)  Time: 0.553s,   28.93/s  (0.370s,   43.25/s)  LR: 4.728e-02  Data: 0.232 (0.049)\n",
      "Train: 3 [ 550/2388 ( 23%)]  Loss: 0.4072 (0.433)  Time: 0.340s,   47.04/s  (0.370s,   43.27/s)  LR: 4.728e-02  Data: 0.019 (0.049)\n",
      "Train: 3 [ 600/2388 ( 25%)]  Loss: 0.4458 (0.433)  Time: 0.503s,   31.79/s  (0.370s,   43.23/s)  LR: 4.728e-02  Data: 0.179 (0.049)\n",
      "Train: 3 [ 650/2388 ( 27%)]  Loss: 0.3881 (0.433)  Time: 0.340s,   47.09/s  (0.370s,   43.26/s)  LR: 4.728e-02  Data: 0.019 (0.049)\n",
      "Train: 3 [ 700/2388 ( 29%)]  Loss: 0.4138 (0.433)  Time: 0.463s,   34.57/s  (0.370s,   43.25/s)  LR: 4.728e-02  Data: 0.143 (0.049)\n",
      "Train: 3 [ 750/2388 ( 31%)]  Loss: 0.3786 (0.433)  Time: 0.349s,   45.83/s  (0.370s,   43.29/s)  LR: 4.728e-02  Data: 0.028 (0.049)\n",
      "Train: 3 [ 800/2388 ( 34%)]  Loss: 0.4285 (0.433)  Time: 0.456s,   35.12/s  (0.370s,   43.20/s)  LR: 4.728e-02  Data: 0.135 (0.049)\n",
      "Train: 3 [ 850/2388 ( 36%)]  Loss: 0.4041 (0.434)  Time: 0.342s,   46.73/s  (0.370s,   43.24/s)  LR: 4.728e-02  Data: 0.021 (0.049)\n",
      "Train: 3 [ 900/2388 ( 38%)]  Loss: 0.4260 (0.434)  Time: 0.511s,   31.32/s  (0.370s,   43.20/s)  LR: 4.728e-02  Data: 0.191 (0.049)\n",
      "Train: 3 [ 950/2388 ( 40%)]  Loss: 0.4471 (0.434)  Time: 0.343s,   46.69/s  (0.370s,   43.21/s)  LR: 4.728e-02  Data: 0.021 (0.049)\n",
      "Train: 3 [1000/2388 ( 42%)]  Loss: 0.4982 (0.433)  Time: 0.425s,   37.62/s  (0.370s,   43.21/s)  LR: 4.728e-02  Data: 0.105 (0.049)\n",
      "Train: 3 [1050/2388 ( 44%)]  Loss: 0.4365 (0.434)  Time: 0.357s,   44.76/s  (0.370s,   43.22/s)  LR: 4.728e-02  Data: 0.037 (0.049)\n",
      "Train: 3 [1100/2388 ( 46%)]  Loss: 0.3617 (0.434)  Time: 0.345s,   46.36/s  (0.371s,   43.16/s)  LR: 4.728e-02  Data: 0.022 (0.050)\n",
      "Train: 3 [1150/2388 ( 48%)]  Loss: 0.4555 (0.434)  Time: 0.342s,   46.81/s  (0.371s,   43.14/s)  LR: 4.728e-02  Data: 0.021 (0.050)\n",
      "Train: 3 [1200/2388 ( 50%)]  Loss: 0.4125 (0.434)  Time: 0.415s,   38.52/s  (0.371s,   43.15/s)  LR: 4.728e-02  Data: 0.095 (0.050)\n",
      "Train: 3 [1250/2388 ( 52%)]  Loss: 0.4109 (0.434)  Time: 0.345s,   46.33/s  (0.371s,   43.16/s)  LR: 4.728e-02  Data: 0.026 (0.050)\n",
      "Train: 3 [1300/2388 ( 54%)]  Loss: 0.4534 (0.434)  Time: 0.418s,   38.32/s  (0.371s,   43.15/s)  LR: 4.728e-02  Data: 0.096 (0.050)\n",
      "Train: 3 [1350/2388 ( 57%)]  Loss: 0.3918 (0.434)  Time: 0.343s,   46.62/s  (0.371s,   43.18/s)  LR: 4.728e-02  Data: 0.023 (0.050)\n",
      "Train: 3 [1400/2388 ( 59%)]  Loss: 0.4005 (0.434)  Time: 0.354s,   45.14/s  (0.370s,   43.20/s)  LR: 4.728e-02  Data: 0.034 (0.049)\n",
      "Train: 3 [1450/2388 ( 61%)]  Loss: 0.3864 (0.434)  Time: 0.429s,   37.26/s  (0.370s,   43.19/s)  LR: 4.728e-02  Data: 0.108 (0.049)\n",
      "Train: 3 [1500/2388 ( 63%)]  Loss: 0.4730 (0.434)  Time: 0.360s,   44.43/s  (0.370s,   43.20/s)  LR: 4.728e-02  Data: 0.040 (0.049)\n",
      "Train: 3 [1550/2388 ( 65%)]  Loss: 0.4472 (0.434)  Time: 0.473s,   33.83/s  (0.371s,   43.15/s)  LR: 4.728e-02  Data: 0.152 (0.050)\n",
      "Train: 3 [1600/2388 ( 67%)]  Loss: 0.3871 (0.435)  Time: 0.341s,   46.87/s  (0.371s,   43.18/s)  LR: 4.728e-02  Data: 0.021 (0.050)\n",
      "Train: 3 [1650/2388 ( 69%)]  Loss: 0.4173 (0.435)  Time: 0.620s,   25.79/s  (0.371s,   43.18/s)  LR: 4.728e-02  Data: 0.300 (0.050)\n",
      "Train: 3 [1700/2388 ( 71%)]  Loss: 0.4304 (0.435)  Time: 0.343s,   46.61/s  (0.370s,   43.19/s)  LR: 4.728e-02  Data: 0.023 (0.050)\n",
      "Train: 3 [1750/2388 ( 73%)]  Loss: 0.3762 (0.434)  Time: 0.389s,   41.13/s  (0.370s,   43.20/s)  LR: 4.728e-02  Data: 0.069 (0.049)\n",
      "Train: 3 [1800/2388 ( 75%)]  Loss: 0.4079 (0.434)  Time: 0.429s,   37.27/s  (0.370s,   43.20/s)  LR: 4.728e-02  Data: 0.109 (0.049)\n",
      "Train: 3 [1850/2388 ( 78%)]  Loss: 0.4519 (0.434)  Time: 0.414s,   38.63/s  (0.371s,   43.18/s)  LR: 4.728e-02  Data: 0.093 (0.050)\n",
      "Train: 3 [1900/2388 ( 80%)]  Loss: 0.4024 (0.434)  Time: 0.375s,   42.63/s  (0.371s,   43.17/s)  LR: 4.728e-02  Data: 0.054 (0.050)\n",
      "Train: 3 [1950/2388 ( 82%)]  Loss: 0.3956 (0.434)  Time: 0.442s,   36.23/s  (0.371s,   43.17/s)  LR: 4.728e-02  Data: 0.120 (0.050)\n",
      "Train: 3 [2000/2388 ( 84%)]  Loss: 0.4367 (0.434)  Time: 0.340s,   47.07/s  (0.371s,   43.14/s)  LR: 4.728e-02  Data: 0.020 (0.050)\n",
      "Train: 3 [2050/2388 ( 86%)]  Loss: 0.4626 (0.434)  Time: 0.406s,   39.36/s  (0.371s,   43.14/s)  LR: 4.728e-02  Data: 0.086 (0.050)\n",
      "Train: 3 [2100/2388 ( 88%)]  Loss: 0.3951 (0.434)  Time: 0.338s,   47.28/s  (0.371s,   43.12/s)  LR: 4.728e-02  Data: 0.018 (0.050)\n",
      "Train: 3 [2150/2388 ( 90%)]  Loss: 0.4685 (0.433)  Time: 0.535s,   29.91/s  (0.371s,   43.12/s)  LR: 4.728e-02  Data: 0.215 (0.050)\n",
      "Train: 3 [2200/2388 ( 92%)]  Loss: 0.4222 (0.433)  Time: 0.339s,   47.23/s  (0.371s,   43.12/s)  LR: 4.728e-02  Data: 0.018 (0.050)\n",
      "Train: 3 [2250/2388 ( 94%)]  Loss: 0.4333 (0.433)  Time: 0.390s,   41.02/s  (0.371s,   43.13/s)  LR: 4.728e-02  Data: 0.069 (0.050)\n",
      "Train: 3 [2300/2388 ( 96%)]  Loss: 0.3988 (0.433)  Time: 0.360s,   44.42/s  (0.371s,   43.13/s)  LR: 4.728e-02  Data: 0.040 (0.050)\n",
      "Train: 3 [2350/2388 ( 98%)]  Loss: 0.4385 (0.433)  Time: 0.465s,   34.38/s  (0.371s,   43.12/s)  LR: 4.728e-02  Data: 0.144 (0.050)\n",
      "Train: 3 [2387/2388 (100%)]  Loss: 0.4041 (0.433)  Time: 0.318s,   50.25/s  (0.371s,   43.11/s)  LR: 4.728e-02  Data: 0.000 (0.050)\n",
      "Test: [   0/2388]  Time: 1.243 (1.243)  Loss:  0.1578 (0.1578)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [  50/2388]  Time: 0.096 (0.295)  Loss:  0.0904 (0.1738)  Acc@1: 100.0000 (97.7941)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 100/2388]  Time: 0.095 (0.293)  Loss:  0.1404 (0.1834)  Acc@1: 100.0000 (97.7723)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 150/2388]  Time: 0.390 (0.288)  Loss:  0.1181 (0.1735)  Acc@1: 100.0000 (97.6821)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 200/2388]  Time: 0.085 (0.289)  Loss:  0.2910 (0.1703)  Acc@1: 87.5000 (97.5124)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 250/2388]  Time: 0.407 (0.289)  Loss:  0.1080 (0.1711)  Acc@1: 100.0000 (97.1863)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 300/2388]  Time: 0.093 (0.288)  Loss:  0.1212 (0.1654)  Acc@1: 100.0000 (97.3007)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 350/2388]  Time: 0.501 (0.288)  Loss:  0.1003 (0.1622)  Acc@1: 100.0000 (97.4003)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 400/2388]  Time: 0.109 (0.287)  Loss:  0.1562 (0.1612)  Acc@1: 100.0000 (97.3192)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 450/2388]  Time: 0.844 (0.288)  Loss:  0.1087 (0.1592)  Acc@1: 100.0000 (97.3670)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 500/2388]  Time: 0.087 (0.287)  Loss:  0.2554 (0.1557)  Acc@1: 93.7500 (97.4925)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 550/2388]  Time: 0.839 (0.288)  Loss:  0.1505 (0.1546)  Acc@1: 100.0000 (97.5159)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 600/2388]  Time: 0.086 (0.287)  Loss:  0.2830 (0.1585)  Acc@1: 87.5000 (97.3794)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 650/2388]  Time: 0.840 (0.288)  Loss:  0.0939 (0.1597)  Acc@1: 100.0000 (97.2830)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 700/2388]  Time: 0.091 (0.287)  Loss:  0.2800 (0.1624)  Acc@1: 93.7500 (97.1380)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 750/2388]  Time: 0.877 (0.286)  Loss:  0.2170 (0.1655)  Acc@1: 93.7500 (96.9957)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 800/2388]  Time: 0.087 (0.285)  Loss:  0.2257 (0.1696)  Acc@1: 87.5000 (96.8711)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 850/2388]  Time: 0.864 (0.286)  Loss:  0.1344 (0.1732)  Acc@1: 100.0000 (96.7098)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 900/2388]  Time: 0.086 (0.286)  Loss:  0.1085 (0.1749)  Acc@1: 100.0000 (96.6287)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 950/2388]  Time: 0.827 (0.286)  Loss:  0.2087 (0.1760)  Acc@1: 100.0000 (96.6285)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1000/2388]  Time: 0.087 (0.286)  Loss:  0.2213 (0.1772)  Acc@1: 93.7500 (96.5722)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1050/2388]  Time: 0.864 (0.287)  Loss:  0.1464 (0.1779)  Acc@1: 100.0000 (96.5450)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1100/2388]  Time: 0.092 (0.286)  Loss:  0.1534 (0.1776)  Acc@1: 100.0000 (96.5713)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1150/2388]  Time: 0.878 (0.287)  Loss:  0.3516 (0.1779)  Acc@1: 87.5000 (96.5682)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1200/2388]  Time: 0.085 (0.287)  Loss:  0.2050 (0.1772)  Acc@1: 93.7500 (96.5966)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1250/2388]  Time: 0.861 (0.287)  Loss:  0.1407 (0.1760)  Acc@1: 100.0000 (96.6427)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1300/2388]  Time: 0.086 (0.286)  Loss:  0.1141 (0.1756)  Acc@1: 100.0000 (96.6852)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1350/2388]  Time: 0.930 (0.287)  Loss:  0.1936 (0.1750)  Acc@1: 93.7500 (96.7432)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1400/2388]  Time: 0.086 (0.287)  Loss:  0.1796 (0.1747)  Acc@1: 100.0000 (96.7925)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1450/2388]  Time: 0.921 (0.287)  Loss:  3.5273 (0.2486)  Acc@1:  0.0000 (94.3401)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1500/2388]  Time: 0.085 (0.287)  Loss:  3.7109 (0.3562)  Acc@1:  0.0000 (91.2142)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1550/2388]  Time: 0.840 (0.287)  Loss:  3.2637 (0.4478)  Acc@1:  0.0000 (88.3019)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1600/2388]  Time: 0.086 (0.286)  Loss:  3.3164 (0.5273)  Acc@1:  0.0000 (85.5871)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1650/2388]  Time: 0.868 (0.287)  Loss:  3.8418 (0.6092)  Acc@1:  0.0000 (82.9952)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1700/2388]  Time: 0.086 (0.287)  Loss:  1.1270 (0.6350)  Acc@1: 43.7500 (81.4999)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1750/2388]  Time: 0.926 (0.287)  Loss:  1.4092 (0.6711)  Acc@1: 43.7500 (79.9472)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1800/2388]  Time: 0.085 (0.287)  Loss:  1.9668 (0.7018)  Acc@1: 12.5000 (78.4842)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1850/2388]  Time: 0.884 (0.288)  Loss:  1.9395 (0.7336)  Acc@1: 18.7500 (77.0563)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1900/2388]  Time: 0.086 (0.288)  Loss:  2.5625 (0.7577)  Acc@1:  6.2500 (75.8285)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1950/2388]  Time: 0.848 (0.288)  Loss:  1.6387 (0.7832)  Acc@1: 18.7500 (74.5227)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2000/2388]  Time: 0.085 (0.287)  Loss:  1.4033 (0.8031)  Acc@1: 25.0000 (73.3696)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2050/2388]  Time: 0.851 (0.287)  Loss:  1.6172 (0.8232)  Acc@1: 18.7500 (72.2239)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2100/2388]  Time: 0.086 (0.287)  Loss:  0.5498 (0.8353)  Acc@1: 68.7500 (71.3856)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2150/2388]  Time: 0.909 (0.287)  Loss:  0.4421 (0.8473)  Acc@1: 75.0000 (70.5602)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2200/2388]  Time: 0.086 (0.287)  Loss:  2.3613 (0.8657)  Acc@1:  0.0000 (69.6587)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2250/2388]  Time: 0.921 (0.287)  Loss:  1.4746 (0.8875)  Acc@1: 31.2500 (68.5973)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2300/2388]  Time: 0.086 (0.287)  Loss:  2.1465 (0.9079)  Acc@1: 12.5000 (67.6255)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2350/2388]  Time: 0.862 (0.288)  Loss:  1.2559 (0.9234)  Acc@1: 43.7500 (66.7402)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2388/2388]  Time: 0.014 (0.287)  Loss:  3.7266 (0.9555)  Acc@1:  0.0000 (65.8336)  Acc@5: 100.0000 (100.0000)\n",
      "Current checkpoints:\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-2.pth.tar', 66.4224025124313)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-3.pth.tar', 65.83355142632819)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-1.pth.tar', 60.52865741952368)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-0.pth.tar', 59.013347291285)\n",
      "\n",
      "Train: 4 [   0/2388 (  0%)]  Loss: 0.6312 (0.631)  Time: 1.888s,    8.47/s  (1.888s,    8.47/s)  LR: 4.523e-02  Data: 1.566 (1.566)\n",
      "Train: 4 [  50/2388 (  2%)]  Loss: 0.3840 (0.433)  Time: 0.339s,   47.21/s  (0.404s,   39.59/s)  LR: 4.523e-02  Data: 0.018 (0.083)\n",
      "Train: 4 [ 100/2388 (  4%)]  Loss: 0.4460 (0.427)  Time: 0.544s,   29.39/s  (0.385s,   41.61/s)  LR: 4.523e-02  Data: 0.224 (0.063)\n",
      "Train: 4 [ 150/2388 (  6%)]  Loss: 0.4367 (0.427)  Time: 0.337s,   47.42/s  (0.378s,   42.28/s)  LR: 4.523e-02  Data: 0.017 (0.058)\n",
      "Train: 4 [ 200/2388 (  8%)]  Loss: 0.4232 (0.427)  Time: 0.441s,   36.32/s  (0.376s,   42.58/s)  LR: 4.523e-02  Data: 0.120 (0.055)\n",
      "Train: 4 [ 250/2388 ( 10%)]  Loss: 0.3641 (0.427)  Time: 0.345s,   46.39/s  (0.374s,   42.79/s)  LR: 4.523e-02  Data: 0.025 (0.053)\n",
      "Train: 4 [ 300/2388 ( 13%)]  Loss: 0.4362 (0.426)  Time: 0.452s,   35.39/s  (0.375s,   42.69/s)  LR: 4.523e-02  Data: 0.130 (0.054)\n",
      "Train: 4 [ 350/2388 ( 15%)]  Loss: 0.4895 (0.425)  Time: 0.340s,   47.11/s  (0.373s,   42.86/s)  LR: 4.523e-02  Data: 0.019 (0.052)\n",
      "Train: 4 [ 400/2388 ( 17%)]  Loss: 0.4018 (0.426)  Time: 0.410s,   39.01/s  (0.374s,   42.82/s)  LR: 4.523e-02  Data: 0.089 (0.053)\n",
      "Train: 4 [ 450/2388 ( 19%)]  Loss: 0.4477 (0.428)  Time: 0.343s,   46.68/s  (0.372s,   43.02/s)  LR: 4.523e-02  Data: 0.021 (0.051)\n",
      "Train: 4 [ 500/2388 ( 21%)]  Loss: 0.4526 (0.428)  Time: 0.439s,   36.46/s  (0.371s,   43.09/s)  LR: 4.523e-02  Data: 0.117 (0.050)\n",
      "Train: 4 [ 550/2388 ( 23%)]  Loss: 0.4711 (0.427)  Time: 0.339s,   47.21/s  (0.370s,   43.19/s)  LR: 4.523e-02  Data: 0.018 (0.050)\n",
      "Train: 4 [ 600/2388 ( 25%)]  Loss: 0.4147 (0.427)  Time: 0.453s,   35.35/s  (0.370s,   43.19/s)  LR: 4.523e-02  Data: 0.132 (0.050)\n",
      "Train: 4 [ 650/2388 ( 27%)]  Loss: 0.5414 (0.428)  Time: 0.341s,   46.94/s  (0.370s,   43.21/s)  LR: 4.523e-02  Data: 0.020 (0.049)\n",
      "Train: 4 [ 700/2388 ( 29%)]  Loss: 0.4432 (0.427)  Time: 0.411s,   38.93/s  (0.369s,   43.31/s)  LR: 4.523e-02  Data: 0.089 (0.048)\n",
      "Train: 4 [ 750/2388 ( 31%)]  Loss: 0.3536 (0.427)  Time: 0.343s,   46.69/s  (0.369s,   43.31/s)  LR: 4.523e-02  Data: 0.022 (0.048)\n",
      "Train: 4 [ 800/2388 ( 34%)]  Loss: 0.3854 (0.427)  Time: 0.441s,   36.32/s  (0.370s,   43.25/s)  LR: 4.523e-02  Data: 0.121 (0.049)\n",
      "Train: 4 [ 850/2388 ( 36%)]  Loss: 0.4424 (0.427)  Time: 0.341s,   46.89/s  (0.370s,   43.26/s)  LR: 4.523e-02  Data: 0.021 (0.049)\n",
      "Train: 4 [ 900/2388 ( 38%)]  Loss: 0.4454 (0.427)  Time: 0.429s,   37.32/s  (0.370s,   43.27/s)  LR: 4.523e-02  Data: 0.109 (0.049)\n",
      "Train: 4 [ 950/2388 ( 40%)]  Loss: 0.4383 (0.427)  Time: 0.381s,   42.03/s  (0.370s,   43.27/s)  LR: 4.523e-02  Data: 0.061 (0.049)\n",
      "Train: 4 [1000/2388 ( 42%)]  Loss: 0.4130 (0.427)  Time: 0.388s,   41.20/s  (0.370s,   43.27/s)  LR: 4.523e-02  Data: 0.069 (0.049)\n",
      "Train: 4 [1050/2388 ( 44%)]  Loss: 0.4146 (0.427)  Time: 0.348s,   45.98/s  (0.370s,   43.28/s)  LR: 4.523e-02  Data: 0.027 (0.049)\n",
      "Train: 4 [1100/2388 ( 46%)]  Loss: 0.3522 (0.427)  Time: 0.414s,   38.69/s  (0.370s,   43.24/s)  LR: 4.523e-02  Data: 0.092 (0.049)\n",
      "Train: 4 [1150/2388 ( 48%)]  Loss: 0.4632 (0.428)  Time: 0.418s,   38.27/s  (0.370s,   43.23/s)  LR: 4.523e-02  Data: 0.097 (0.049)\n",
      "Train: 4 [1200/2388 ( 50%)]  Loss: 0.3718 (0.428)  Time: 0.345s,   46.44/s  (0.370s,   43.23/s)  LR: 4.523e-02  Data: 0.024 (0.049)\n",
      "Train: 4 [1250/2388 ( 52%)]  Loss: 0.3741 (0.427)  Time: 0.437s,   36.58/s  (0.370s,   43.24/s)  LR: 4.523e-02  Data: 0.117 (0.049)\n",
      "Train: 4 [1300/2388 ( 54%)]  Loss: 0.4117 (0.427)  Time: 0.352s,   45.47/s  (0.370s,   43.23/s)  LR: 4.523e-02  Data: 0.031 (0.049)\n",
      "Train: 4 [1350/2388 ( 57%)]  Loss: 0.4785 (0.427)  Time: 0.501s,   31.92/s  (0.370s,   43.22/s)  LR: 4.523e-02  Data: 0.177 (0.049)\n",
      "Train: 4 [1400/2388 ( 59%)]  Loss: 0.3616 (0.428)  Time: 0.343s,   46.65/s  (0.370s,   43.22/s)  LR: 4.523e-02  Data: 0.023 (0.049)\n",
      "Train: 4 [1450/2388 ( 61%)]  Loss: 0.5133 (0.428)  Time: 0.541s,   29.58/s  (0.370s,   43.20/s)  LR: 4.523e-02  Data: 0.219 (0.050)\n",
      "Train: 4 [1500/2388 ( 63%)]  Loss: 0.3913 (0.427)  Time: 0.360s,   44.50/s  (0.370s,   43.22/s)  LR: 4.523e-02  Data: 0.037 (0.049)\n",
      "Train: 4 [1550/2388 ( 65%)]  Loss: 0.4432 (0.428)  Time: 0.551s,   29.04/s  (0.370s,   43.20/s)  LR: 4.523e-02  Data: 0.224 (0.050)\n",
      "Train: 4 [1600/2388 ( 67%)]  Loss: 0.4769 (0.427)  Time: 0.343s,   46.61/s  (0.371s,   43.15/s)  LR: 4.523e-02  Data: 0.019 (0.050)\n",
      "Train: 4 [1650/2388 ( 69%)]  Loss: 0.3849 (0.427)  Time: 0.501s,   31.95/s  (0.371s,   43.16/s)  LR: 4.523e-02  Data: 0.178 (0.050)\n",
      "Train: 4 [1700/2388 ( 71%)]  Loss: 0.5385 (0.427)  Time: 0.340s,   47.11/s  (0.371s,   43.13/s)  LR: 4.523e-02  Data: 0.018 (0.050)\n",
      "Train: 4 [1750/2388 ( 73%)]  Loss: 0.3300 (0.426)  Time: 0.815s,   19.64/s  (0.371s,   43.11/s)  LR: 4.523e-02  Data: 0.493 (0.050)\n",
      "Train: 4 [1800/2388 ( 75%)]  Loss: 0.4427 (0.427)  Time: 0.338s,   47.35/s  (0.371s,   43.13/s)  LR: 4.523e-02  Data: 0.018 (0.050)\n",
      "Train: 4 [1850/2388 ( 78%)]  Loss: 0.3537 (0.427)  Time: 0.440s,   36.39/s  (0.371s,   43.13/s)  LR: 4.523e-02  Data: 0.119 (0.050)\n",
      "Train: 4 [1900/2388 ( 80%)]  Loss: 0.4999 (0.427)  Time: 0.344s,   46.52/s  (0.371s,   43.13/s)  LR: 4.523e-02  Data: 0.019 (0.050)\n",
      "Train: 4 [1950/2388 ( 82%)]  Loss: 0.4803 (0.427)  Time: 0.630s,   25.41/s  (0.371s,   43.11/s)  LR: 4.523e-02  Data: 0.309 (0.050)\n",
      "Train: 4 [2000/2388 ( 84%)]  Loss: 0.4589 (0.426)  Time: 0.339s,   47.23/s  (0.371s,   43.11/s)  LR: 4.523e-02  Data: 0.019 (0.050)\n",
      "Train: 4 [2050/2388 ( 86%)]  Loss: 0.3669 (0.426)  Time: 0.454s,   35.25/s  (0.371s,   43.11/s)  LR: 4.523e-02  Data: 0.133 (0.050)\n",
      "Train: 4 [2100/2388 ( 88%)]  Loss: 0.5648 (0.426)  Time: 0.340s,   47.01/s  (0.371s,   43.14/s)  LR: 4.523e-02  Data: 0.020 (0.050)\n",
      "Train: 4 [2150/2388 ( 90%)]  Loss: 0.3941 (0.426)  Time: 0.499s,   32.07/s  (0.371s,   43.14/s)  LR: 4.523e-02  Data: 0.179 (0.050)\n",
      "Train: 4 [2200/2388 ( 92%)]  Loss: 0.3933 (0.426)  Time: 0.339s,   47.13/s  (0.371s,   43.14/s)  LR: 4.523e-02  Data: 0.019 (0.050)\n",
      "Train: 4 [2250/2388 ( 94%)]  Loss: 0.4543 (0.426)  Time: 0.407s,   39.35/s  (0.371s,   43.13/s)  LR: 4.523e-02  Data: 0.086 (0.050)\n",
      "Train: 4 [2300/2388 ( 96%)]  Loss: 0.3472 (0.426)  Time: 0.341s,   46.99/s  (0.371s,   43.13/s)  LR: 4.523e-02  Data: 0.021 (0.050)\n",
      "Train: 4 [2350/2388 ( 98%)]  Loss: 0.3780 (0.426)  Time: 0.381s,   41.98/s  (0.371s,   43.14/s)  LR: 4.523e-02  Data: 0.056 (0.050)\n",
      "Train: 4 [2387/2388 (100%)]  Loss: 0.3967 (0.426)  Time: 0.319s,   50.19/s  (0.371s,   43.15/s)  LR: 4.523e-02  Data: 0.000 (0.050)\n",
      "Test: [   0/2388]  Time: 1.237 (1.237)  Loss:  0.4170 (0.4170)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [  50/2388]  Time: 0.092 (0.293)  Loss:  0.1497 (0.3031)  Acc@1: 100.0000 (96.2010)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 100/2388]  Time: 0.087 (0.288)  Loss:  0.2810 (0.3046)  Acc@1: 100.0000 (96.8441)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 150/2388]  Time: 0.090 (0.288)  Loss:  0.2568 (0.2855)  Acc@1: 100.0000 (97.0199)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 200/2388]  Time: 0.087 (0.285)  Loss:  0.2542 (0.2748)  Acc@1: 93.7500 (97.0149)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 250/2388]  Time: 0.094 (0.288)  Loss:  0.1962 (0.2675)  Acc@1: 100.0000 (97.1614)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 300/2388]  Time: 0.087 (0.286)  Loss:  0.1656 (0.2681)  Acc@1: 100.0000 (96.8439)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 350/2388]  Time: 0.091 (0.287)  Loss:  0.2053 (0.2687)  Acc@1: 100.0000 (96.7415)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 400/2388]  Time: 0.087 (0.286)  Loss:  0.3418 (0.2655)  Acc@1: 93.7500 (96.7893)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 450/2388]  Time: 0.092 (0.287)  Loss:  0.1653 (0.2637)  Acc@1: 93.7500 (96.7018)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 500/2388]  Time: 0.085 (0.286)  Loss:  0.4326 (0.2592)  Acc@1: 93.7500 (96.8313)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 550/2388]  Time: 0.094 (0.287)  Loss:  0.3572 (0.2594)  Acc@1: 87.5000 (96.7786)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 600/2388]  Time: 0.086 (0.287)  Loss:  0.1577 (0.2613)  Acc@1: 93.7500 (96.6514)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 650/2388]  Time: 0.093 (0.288)  Loss:  0.2379 (0.2596)  Acc@1: 100.0000 (96.6590)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 700/2388]  Time: 0.087 (0.288)  Loss:  0.2981 (0.2618)  Acc@1: 100.0000 (96.5317)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 750/2388]  Time: 0.093 (0.288)  Loss:  0.3318 (0.2640)  Acc@1: 87.5000 (96.3965)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 800/2388]  Time: 0.087 (0.287)  Loss:  0.2438 (0.2670)  Acc@1: 100.0000 (96.2313)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 850/2388]  Time: 0.089 (0.288)  Loss:  0.1787 (0.2682)  Acc@1: 100.0000 (96.2250)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 900/2388]  Time: 0.086 (0.287)  Loss:  0.0746 (0.2676)  Acc@1: 100.0000 (96.2195)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 950/2388]  Time: 0.093 (0.288)  Loss:  0.2517 (0.2683)  Acc@1: 100.0000 (96.2145)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1000/2388]  Time: 0.087 (0.288)  Loss:  0.3103 (0.2678)  Acc@1: 100.0000 (96.2600)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1050/2388]  Time: 0.092 (0.288)  Loss:  0.5073 (0.2674)  Acc@1: 93.7500 (96.3130)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1100/2388]  Time: 0.087 (0.288)  Loss:  0.1884 (0.2662)  Acc@1: 100.0000 (96.3669)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1150/2388]  Time: 0.093 (0.288)  Loss:  0.5913 (0.2662)  Acc@1: 93.7500 (96.3727)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1200/2388]  Time: 0.087 (0.288)  Loss:  0.1309 (0.2657)  Acc@1: 100.0000 (96.3884)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1250/2388]  Time: 0.092 (0.289)  Loss:  0.6387 (0.2645)  Acc@1: 81.2500 (96.4428)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1300/2388]  Time: 0.087 (0.289)  Loss:  0.2031 (0.2633)  Acc@1: 100.0000 (96.5315)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1350/2388]  Time: 0.093 (0.289)  Loss:  0.2094 (0.2620)  Acc@1: 100.0000 (96.6229)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1400/2388]  Time: 0.087 (0.290)  Loss:  0.2585 (0.2615)  Acc@1: 100.0000 (96.6809)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1450/2388]  Time: 0.093 (0.290)  Loss:  2.2188 (0.3062)  Acc@1:  6.2500 (94.6158)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1500/2388]  Time: 0.087 (0.289)  Loss:  2.9199 (0.3670)  Acc@1:  0.0000 (91.9096)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1550/2388]  Time: 0.092 (0.289)  Loss:  3.1953 (0.4213)  Acc@1:  0.0000 (89.4907)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1600/2388]  Time: 0.087 (0.289)  Loss:  2.6035 (0.4751)  Acc@1:  0.0000 (87.0550)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1650/2388]  Time: 0.092 (0.289)  Loss:  3.0254 (0.5282)  Acc@1:  0.0000 (84.7857)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1700/2388]  Time: 0.089 (0.289)  Loss:  1.2246 (0.5603)  Acc@1: 43.7500 (82.9733)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1750/2388]  Time: 0.092 (0.289)  Loss:  0.9438 (0.5909)  Acc@1: 43.7500 (81.2357)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1800/2388]  Time: 0.087 (0.289)  Loss:  1.5674 (0.6136)  Acc@1: 18.7500 (79.7161)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1850/2388]  Time: 0.096 (0.289)  Loss:  1.4346 (0.6317)  Acc@1: 25.0000 (78.3428)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1900/2388]  Time: 0.087 (0.289)  Loss:  1.6006 (0.6523)  Acc@1: 25.0000 (77.1074)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1950/2388]  Time: 0.093 (0.289)  Loss:  1.9531 (0.6744)  Acc@1: 12.5000 (75.7784)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2000/2388]  Time: 0.087 (0.289)  Loss:  1.2383 (0.6892)  Acc@1: 37.5000 (74.6845)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2050/2388]  Time: 0.092 (0.289)  Loss:  1.5391 (0.7040)  Acc@1: 18.7500 (73.5129)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2100/2388]  Time: 0.088 (0.289)  Loss:  0.7070 (0.7157)  Acc@1: 56.2500 (72.5607)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2150/2388]  Time: 0.092 (0.289)  Loss:  0.7168 (0.7269)  Acc@1: 68.7500 (71.6702)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2200/2388]  Time: 0.087 (0.289)  Loss:  1.7480 (0.7423)  Acc@1:  0.0000 (70.6298)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2250/2388]  Time: 0.092 (0.289)  Loss:  1.8896 (0.7623)  Acc@1: 12.5000 (69.4636)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2300/2388]  Time: 0.088 (0.289)  Loss:  1.8604 (0.7845)  Acc@1: 12.5000 (68.3670)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2350/2388]  Time: 0.093 (0.289)  Loss:  1.5078 (0.8021)  Acc@1: 18.7500 (67.3437)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2388/2388]  Time: 0.014 (0.289)  Loss:  1.5020 (0.8125)  Acc@1:  0.0000 (66.8804)  Acc@5: 100.0000 (100.0000)\n",
      "Current checkpoints:\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-4.pth.tar', 66.8803978016226)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-2.pth.tar', 66.4224025124313)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-3.pth.tar', 65.83355142632819)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-1.pth.tar', 60.52865741952368)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-0.pth.tar', 59.013347291285)\n",
      "\n",
      "Train: 5 [   0/2388 (  0%)]  Loss: 0.5047 (0.505)  Time: 1.908s,    8.39/s  (1.908s,    8.39/s)  LR: 4.268e-02  Data: 1.585 (1.585)\n",
      "Train: 5 [  50/2388 (  2%)]  Loss: 0.4597 (0.420)  Time: 0.339s,   47.20/s  (0.406s,   39.41/s)  LR: 4.268e-02  Data: 0.019 (0.085)\n",
      "Train: 5 [ 100/2388 (  4%)]  Loss: 0.4008 (0.422)  Time: 0.434s,   36.89/s  (0.392s,   40.85/s)  LR: 4.268e-02  Data: 0.112 (0.071)\n",
      "Train: 5 [ 150/2388 (  6%)]  Loss: 0.3392 (0.420)  Time: 0.340s,   47.06/s  (0.383s,   41.75/s)  LR: 4.268e-02  Data: 0.019 (0.062)\n",
      "Train: 5 [ 200/2388 (  8%)]  Loss: 0.5145 (0.420)  Time: 0.380s,   42.05/s  (0.380s,   42.08/s)  LR: 4.268e-02  Data: 0.060 (0.059)\n",
      "Train: 5 [ 250/2388 ( 10%)]  Loss: 0.4471 (0.419)  Time: 0.342s,   46.84/s  (0.379s,   42.26/s)  LR: 4.268e-02  Data: 0.021 (0.058)\n",
      "Train: 5 [ 300/2388 ( 13%)]  Loss: 0.3470 (0.420)  Time: 0.446s,   35.87/s  (0.377s,   42.49/s)  LR: 4.268e-02  Data: 0.126 (0.056)\n",
      "Train: 5 [ 350/2388 ( 15%)]  Loss: 0.4904 (0.420)  Time: 0.470s,   34.05/s  (0.376s,   42.52/s)  LR: 4.268e-02  Data: 0.149 (0.055)\n",
      "Train: 5 [ 400/2388 ( 17%)]  Loss: 0.3628 (0.421)  Time: 0.346s,   46.20/s  (0.376s,   42.52/s)  LR: 4.268e-02  Data: 0.026 (0.055)\n",
      "Train: 5 [ 450/2388 ( 19%)]  Loss: 0.3024 (0.422)  Time: 0.440s,   36.40/s  (0.376s,   42.59/s)  LR: 4.268e-02  Data: 0.116 (0.055)\n",
      "Train: 5 [ 500/2388 ( 21%)]  Loss: 0.3804 (0.423)  Time: 0.345s,   46.38/s  (0.374s,   42.73/s)  LR: 4.268e-02  Data: 0.023 (0.054)\n",
      "Train: 5 [ 550/2388 ( 23%)]  Loss: 0.4318 (0.423)  Time: 0.455s,   35.18/s  (0.375s,   42.63/s)  LR: 4.268e-02  Data: 0.133 (0.054)\n",
      "Train: 5 [ 600/2388 ( 25%)]  Loss: 0.3651 (0.423)  Time: 0.339s,   47.22/s  (0.375s,   42.70/s)  LR: 4.268e-02  Data: 0.019 (0.054)\n",
      "Train: 5 [ 650/2388 ( 27%)]  Loss: 0.3650 (0.423)  Time: 0.461s,   34.68/s  (0.375s,   42.65/s)  LR: 4.268e-02  Data: 0.140 (0.054)\n",
      "Train: 5 [ 700/2388 ( 29%)]  Loss: 0.3940 (0.423)  Time: 0.340s,   47.05/s  (0.374s,   42.79/s)  LR: 4.268e-02  Data: 0.020 (0.053)\n",
      "Train: 5 [ 750/2388 ( 31%)]  Loss: 0.3663 (0.423)  Time: 0.428s,   37.34/s  (0.373s,   42.84/s)  LR: 4.268e-02  Data: 0.108 (0.053)\n",
      "Train: 5 [ 800/2388 ( 34%)]  Loss: 0.3833 (0.423)  Time: 0.350s,   45.71/s  (0.373s,   42.88/s)  LR: 4.268e-02  Data: 0.029 (0.052)\n",
      "Train: 5 [ 850/2388 ( 36%)]  Loss: 0.4885 (0.422)  Time: 0.411s,   38.92/s  (0.373s,   42.87/s)  LR: 4.268e-02  Data: 0.086 (0.052)\n",
      "Train: 5 [ 900/2388 ( 38%)]  Loss: 0.3683 (0.423)  Time: 0.339s,   47.26/s  (0.373s,   42.85/s)  LR: 4.268e-02  Data: 0.018 (0.053)\n",
      "Train: 5 [ 950/2388 ( 40%)]  Loss: 0.3836 (0.423)  Time: 0.444s,   36.03/s  (0.373s,   42.89/s)  LR: 4.268e-02  Data: 0.125 (0.052)\n",
      "Train: 5 [1000/2388 ( 42%)]  Loss: 0.4225 (0.423)  Time: 0.343s,   46.58/s  (0.373s,   42.94/s)  LR: 4.268e-02  Data: 0.023 (0.052)\n",
      "Train: 5 [1050/2388 ( 44%)]  Loss: 0.4395 (0.422)  Time: 0.541s,   29.57/s  (0.373s,   42.92/s)  LR: 4.268e-02  Data: 0.220 (0.052)\n",
      "Train: 5 [1100/2388 ( 46%)]  Loss: 0.4193 (0.422)  Time: 0.340s,   47.07/s  (0.373s,   42.93/s)  LR: 4.268e-02  Data: 0.018 (0.052)\n",
      "Train: 5 [1150/2388 ( 48%)]  Loss: 0.4524 (0.421)  Time: 0.452s,   35.41/s  (0.372s,   42.97/s)  LR: 4.268e-02  Data: 0.132 (0.051)\n",
      "Train: 5 [1200/2388 ( 50%)]  Loss: 0.4264 (0.421)  Time: 0.344s,   46.49/s  (0.372s,   42.99/s)  LR: 4.268e-02  Data: 0.022 (0.051)\n",
      "Train: 5 [1250/2388 ( 52%)]  Loss: 0.4280 (0.421)  Time: 0.561s,   28.52/s  (0.372s,   42.96/s)  LR: 4.268e-02  Data: 0.241 (0.052)\n",
      "Train: 5 [1300/2388 ( 54%)]  Loss: 0.4492 (0.421)  Time: 0.339s,   47.22/s  (0.373s,   42.94/s)  LR: 4.268e-02  Data: 0.018 (0.052)\n",
      "Train: 5 [1350/2388 ( 57%)]  Loss: 0.3830 (0.421)  Time: 0.459s,   34.89/s  (0.373s,   42.92/s)  LR: 4.268e-02  Data: 0.137 (0.052)\n",
      "Train: 5 [1400/2388 ( 59%)]  Loss: 0.4293 (0.422)  Time: 0.344s,   46.47/s  (0.373s,   42.94/s)  LR: 4.268e-02  Data: 0.020 (0.052)\n",
      "Train: 5 [1450/2388 ( 61%)]  Loss: 0.4412 (0.422)  Time: 0.404s,   39.56/s  (0.373s,   42.94/s)  LR: 4.268e-02  Data: 0.083 (0.052)\n",
      "Train: 5 [1500/2388 ( 63%)]  Loss: 0.3339 (0.422)  Time: 0.339s,   47.15/s  (0.372s,   42.95/s)  LR: 4.268e-02  Data: 0.019 (0.052)\n",
      "Train: 5 [1550/2388 ( 65%)]  Loss: 0.4657 (0.422)  Time: 0.439s,   36.46/s  (0.372s,   42.97/s)  LR: 4.268e-02  Data: 0.117 (0.051)\n",
      "Train: 5 [1600/2388 ( 67%)]  Loss: 0.4058 (0.422)  Time: 0.342s,   46.81/s  (0.372s,   42.99/s)  LR: 4.268e-02  Data: 0.019 (0.051)\n",
      "Train: 5 [1650/2388 ( 69%)]  Loss: 0.4075 (0.423)  Time: 0.411s,   38.90/s  (0.372s,   42.98/s)  LR: 4.268e-02  Data: 0.088 (0.051)\n",
      "Train: 5 [1700/2388 ( 71%)]  Loss: 0.4530 (0.422)  Time: 0.339s,   47.20/s  (0.372s,   42.97/s)  LR: 4.268e-02  Data: 0.018 (0.051)\n",
      "Train: 5 [1750/2388 ( 73%)]  Loss: 0.5358 (0.422)  Time: 0.387s,   41.34/s  (0.372s,   42.99/s)  LR: 4.268e-02  Data: 0.067 (0.051)\n",
      "Train: 5 [1800/2388 ( 75%)]  Loss: 0.4967 (0.422)  Time: 0.340s,   47.03/s  (0.372s,   42.99/s)  LR: 4.268e-02  Data: 0.018 (0.051)\n",
      "Train: 5 [1850/2388 ( 78%)]  Loss: 0.4287 (0.422)  Time: 0.429s,   37.27/s  (0.372s,   43.03/s)  LR: 4.268e-02  Data: 0.108 (0.051)\n",
      "Train: 5 [1900/2388 ( 80%)]  Loss: 0.3741 (0.422)  Time: 0.339s,   47.24/s  (0.372s,   43.05/s)  LR: 4.268e-02  Data: 0.019 (0.051)\n",
      "Train: 5 [1950/2388 ( 82%)]  Loss: 0.3617 (0.422)  Time: 0.432s,   37.04/s  (0.372s,   43.06/s)  LR: 4.268e-02  Data: 0.111 (0.051)\n",
      "Train: 5 [2000/2388 ( 84%)]  Loss: 0.4360 (0.422)  Time: 0.339s,   47.21/s  (0.371s,   43.09/s)  LR: 4.268e-02  Data: 0.019 (0.050)\n",
      "Train: 5 [2050/2388 ( 86%)]  Loss: 0.3953 (0.422)  Time: 0.499s,   32.08/s  (0.371s,   43.09/s)  LR: 4.268e-02  Data: 0.176 (0.050)\n",
      "Train: 5 [2100/2388 ( 88%)]  Loss: 0.4807 (0.421)  Time: 0.339s,   47.18/s  (0.371s,   43.10/s)  LR: 4.268e-02  Data: 0.019 (0.050)\n",
      "Train: 5 [2150/2388 ( 90%)]  Loss: 0.3515 (0.421)  Time: 0.492s,   32.52/s  (0.371s,   43.11/s)  LR: 4.268e-02  Data: 0.170 (0.050)\n",
      "Train: 5 [2200/2388 ( 92%)]  Loss: 0.3828 (0.421)  Time: 0.342s,   46.83/s  (0.371s,   43.13/s)  LR: 4.268e-02  Data: 0.020 (0.050)\n",
      "Train: 5 [2250/2388 ( 94%)]  Loss: 0.4350 (0.421)  Time: 0.447s,   35.82/s  (0.371s,   43.14/s)  LR: 4.268e-02  Data: 0.127 (0.050)\n",
      "Train: 5 [2300/2388 ( 96%)]  Loss: 0.3824 (0.421)  Time: 0.340s,   47.11/s  (0.371s,   43.14/s)  LR: 4.268e-02  Data: 0.019 (0.050)\n",
      "Train: 5 [2350/2388 ( 98%)]  Loss: 0.4508 (0.421)  Time: 0.483s,   33.12/s  (0.371s,   43.13/s)  LR: 4.268e-02  Data: 0.161 (0.050)\n",
      "Train: 5 [2387/2388 (100%)]  Loss: 0.4084 (0.421)  Time: 0.318s,   50.24/s  (0.371s,   43.14/s)  LR: 4.268e-02  Data: 0.000 (0.050)\n",
      "Test: [   0/2388]  Time: 1.368 (1.368)  Loss:  0.4407 (0.4407)  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [  50/2388]  Time: 0.087 (0.294)  Loss:  0.2340 (0.4014)  Acc@1: 100.0000 (93.7500)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 100/2388]  Time: 0.864 (0.293)  Loss:  0.3184 (0.4454)  Acc@1: 100.0000 (92.6361)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 150/2388]  Time: 0.086 (0.287)  Loss:  0.3896 (0.4243)  Acc@1: 100.0000 (93.2533)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 200/2388]  Time: 0.870 (0.288)  Loss:  0.4009 (0.4134)  Acc@1: 93.7500 (93.5945)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 250/2388]  Time: 0.086 (0.285)  Loss:  0.2803 (0.4044)  Acc@1: 100.0000 (93.8247)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 300/2388]  Time: 0.952 (0.287)  Loss:  0.2786 (0.3923)  Acc@1: 93.7500 (94.1030)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 350/2388]  Time: 0.086 (0.286)  Loss:  0.2417 (0.3852)  Acc@1: 100.0000 (94.3020)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 400/2388]  Time: 0.849 (0.287)  Loss:  0.3511 (0.3790)  Acc@1: 100.0000 (94.3890)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 450/2388]  Time: 0.087 (0.286)  Loss:  0.1937 (0.3722)  Acc@1: 100.0000 (94.5815)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 500/2388]  Time: 0.864 (0.286)  Loss:  0.6465 (0.3643)  Acc@1: 81.2500 (94.7106)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 550/2388]  Time: 0.086 (0.286)  Loss:  0.5020 (0.3631)  Acc@1: 81.2500 (94.7482)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 600/2388]  Time: 0.835 (0.287)  Loss:  0.4629 (0.3704)  Acc@1: 81.2500 (94.3532)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 650/2388]  Time: 0.086 (0.285)  Loss:  0.3228 (0.3727)  Acc@1: 100.0000 (94.2108)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 700/2388]  Time: 0.826 (0.286)  Loss:  0.4275 (0.3764)  Acc@1: 93.7500 (93.9907)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 750/2388]  Time: 0.086 (0.285)  Loss:  0.6636 (0.3799)  Acc@1: 75.0000 (93.7833)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 800/2388]  Time: 0.875 (0.286)  Loss:  0.3079 (0.3822)  Acc@1: 100.0000 (93.6252)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 850/2388]  Time: 0.089 (0.285)  Loss:  0.3860 (0.3828)  Acc@1: 100.0000 (93.5737)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 900/2388]  Time: 0.825 (0.285)  Loss:  0.2455 (0.3818)  Acc@1: 93.7500 (93.5835)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 950/2388]  Time: 0.086 (0.285)  Loss:  0.4045 (0.3806)  Acc@1: 93.7500 (93.6711)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1000/2388]  Time: 0.867 (0.285)  Loss:  0.5151 (0.3798)  Acc@1: 93.7500 (93.7188)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1050/2388]  Time: 0.085 (0.285)  Loss:  0.3640 (0.3795)  Acc@1: 100.0000 (93.7441)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1100/2388]  Time: 1.060 (0.285)  Loss:  0.3293 (0.3789)  Acc@1: 100.0000 (93.7841)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1150/2388]  Time: 0.086 (0.285)  Loss:  0.7559 (0.3783)  Acc@1: 87.5000 (93.8423)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1200/2388]  Time: 0.939 (0.286)  Loss:  0.2971 (0.3769)  Acc@1: 87.5000 (93.9061)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1250/2388]  Time: 0.086 (0.285)  Loss:  0.4792 (0.3754)  Acc@1: 93.7500 (94.0098)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1300/2388]  Time: 0.911 (0.286)  Loss:  0.2898 (0.3746)  Acc@1: 100.0000 (94.1055)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1350/2388]  Time: 0.085 (0.286)  Loss:  0.3181 (0.3737)  Acc@1: 100.0000 (94.1802)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1400/2388]  Time: 0.886 (0.287)  Loss:  0.5229 (0.3734)  Acc@1: 100.0000 (94.2675)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1450/2388]  Time: 0.092 (0.287)  Loss:  1.2998 (0.3944)  Acc@1: 12.5000 (92.6129)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1500/2388]  Time: 0.847 (0.287)  Loss:  1.7246 (0.4286)  Acc@1:  0.0000 (90.2482)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1550/2388]  Time: 0.086 (0.287)  Loss:  1.8398 (0.4560)  Acc@1: 18.7500 (88.3503)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1600/2388]  Time: 0.898 (0.287)  Loss:  1.6660 (0.4827)  Acc@1:  0.0000 (86.4499)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1650/2388]  Time: 0.086 (0.287)  Loss:  1.8340 (0.5128)  Acc@1:  0.0000 (84.4337)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1700/2388]  Time: 0.842 (0.288)  Loss:  0.9180 (0.5401)  Acc@1: 56.2500 (82.9034)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1750/2388]  Time: 0.085 (0.287)  Loss:  1.0322 (0.5640)  Acc@1: 37.5000 (81.4356)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1800/2388]  Time: 0.923 (0.288)  Loss:  1.5107 (0.5869)  Acc@1: 25.0000 (80.0076)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1850/2388]  Time: 0.086 (0.287)  Loss:  1.5195 (0.6071)  Acc@1: 25.0000 (78.7311)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1900/2388]  Time: 0.871 (0.288)  Loss:  1.9277 (0.6255)  Acc@1:  6.2500 (77.6664)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1950/2388]  Time: 0.086 (0.288)  Loss:  1.5049 (0.6427)  Acc@1: 18.7500 (76.5409)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2000/2388]  Time: 0.912 (0.288)  Loss:  1.1533 (0.6556)  Acc@1: 37.5000 (75.5935)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2050/2388]  Time: 0.086 (0.287)  Loss:  1.5820 (0.6710)  Acc@1: 18.7500 (74.5338)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2100/2388]  Time: 0.860 (0.287)  Loss:  0.6201 (0.6822)  Acc@1: 62.5000 (73.7327)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2150/2388]  Time: 0.086 (0.287)  Loss:  0.5215 (0.6938)  Acc@1: 81.2500 (72.9021)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2200/2388]  Time: 0.852 (0.287)  Loss:  1.8613 (0.7087)  Acc@1:  6.2500 (72.0326)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2250/2388]  Time: 0.086 (0.287)  Loss:  1.2832 (0.7276)  Acc@1: 31.2500 (70.9546)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2300/2388]  Time: 0.865 (0.287)  Loss:  1.3975 (0.7443)  Acc@1: 31.2500 (70.0130)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2350/2388]  Time: 0.087 (0.287)  Loss:  1.0996 (0.7563)  Acc@1: 37.5000 (69.1807)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2388/2388]  Time: 0.015 (0.287)  Loss:  2.0234 (0.7683)  Acc@1:  0.0000 (68.6836)  Acc@5: 100.0000 (100.0000)\n",
      "Current checkpoints:\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-5.pth.tar', 68.68359068306727)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-4.pth.tar', 66.8803978016226)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-2.pth.tar', 66.4224025124313)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-3.pth.tar', 65.83355142632819)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-1.pth.tar', 60.52865741952368)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-0.pth.tar', 59.013347291285)\n",
      "\n",
      "Train: 6 [   0/2388 (  0%)]  Loss: 0.3545 (0.354)  Time: 1.912s,    8.37/s  (1.912s,    8.37/s)  LR: 3.969e-02  Data: 1.591 (1.591)\n",
      "Train: 6 [  50/2388 (  2%)]  Loss: 0.4184 (0.424)  Time: 0.338s,   47.41/s  (0.404s,   39.65/s)  LR: 3.969e-02  Data: 0.017 (0.083)\n",
      "Train: 6 [ 100/2388 (  4%)]  Loss: 0.4370 (0.424)  Time: 0.432s,   37.02/s  (0.386s,   41.43/s)  LR: 3.969e-02  Data: 0.111 (0.065)\n",
      "Train: 6 [ 150/2388 (  6%)]  Loss: 0.3775 (0.421)  Time: 0.338s,   47.28/s  (0.380s,   42.11/s)  LR: 3.969e-02  Data: 0.018 (0.059)\n",
      "Train: 6 [ 200/2388 (  8%)]  Loss: 0.4722 (0.421)  Time: 0.436s,   36.71/s  (0.379s,   42.26/s)  LR: 3.969e-02  Data: 0.115 (0.058)\n",
      "Train: 6 [ 250/2388 ( 10%)]  Loss: 0.3955 (0.422)  Time: 0.363s,   44.04/s  (0.376s,   42.53/s)  LR: 3.969e-02  Data: 0.043 (0.055)\n",
      "Train: 6 [ 300/2388 ( 13%)]  Loss: 0.3947 (0.422)  Time: 0.474s,   33.73/s  (0.377s,   42.49/s)  LR: 3.969e-02  Data: 0.154 (0.056)\n",
      "Train: 6 [ 350/2388 ( 15%)]  Loss: 0.4487 (0.420)  Time: 0.346s,   46.24/s  (0.376s,   42.56/s)  LR: 3.969e-02  Data: 0.024 (0.055)\n",
      "Train: 6 [ 400/2388 ( 17%)]  Loss: 0.4604 (0.419)  Time: 0.415s,   38.53/s  (0.375s,   42.69/s)  LR: 3.969e-02  Data: 0.095 (0.054)\n",
      "Train: 6 [ 450/2388 ( 19%)]  Loss: 0.4462 (0.420)  Time: 0.432s,   37.08/s  (0.375s,   42.64/s)  LR: 3.969e-02  Data: 0.110 (0.054)\n",
      "Train: 6 [ 500/2388 ( 21%)]  Loss: 0.3945 (0.420)  Time: 0.360s,   44.48/s  (0.375s,   42.69/s)  LR: 3.969e-02  Data: 0.040 (0.054)\n",
      "Train: 6 [ 550/2388 ( 23%)]  Loss: 0.4648 (0.420)  Time: 0.442s,   36.16/s  (0.375s,   42.70/s)  LR: 3.969e-02  Data: 0.122 (0.054)\n",
      "Train: 6 [ 600/2388 ( 25%)]  Loss: 0.4424 (0.421)  Time: 0.342s,   46.75/s  (0.374s,   42.79/s)  LR: 3.969e-02  Data: 0.022 (0.053)\n",
      "Train: 6 [ 650/2388 ( 27%)]  Loss: 0.4375 (0.421)  Time: 0.416s,   38.44/s  (0.374s,   42.77/s)  LR: 3.969e-02  Data: 0.095 (0.053)\n",
      "Train: 6 [ 700/2388 ( 29%)]  Loss: 0.4657 (0.420)  Time: 0.340s,   47.07/s  (0.374s,   42.79/s)  LR: 3.969e-02  Data: 0.020 (0.053)\n",
      "Train: 6 [ 750/2388 ( 31%)]  Loss: 0.4081 (0.420)  Time: 0.406s,   39.39/s  (0.373s,   42.84/s)  LR: 3.969e-02  Data: 0.086 (0.053)\n",
      "Train: 6 [ 800/2388 ( 34%)]  Loss: 0.3958 (0.420)  Time: 0.341s,   46.99/s  (0.373s,   42.88/s)  LR: 3.969e-02  Data: 0.020 (0.052)\n",
      "Train: 6 [ 850/2388 ( 36%)]  Loss: 0.4466 (0.419)  Time: 0.401s,   39.86/s  (0.373s,   42.86/s)  LR: 3.969e-02  Data: 0.081 (0.052)\n",
      "Train: 6 [ 900/2388 ( 38%)]  Loss: 0.3304 (0.419)  Time: 0.340s,   47.09/s  (0.373s,   42.88/s)  LR: 3.969e-02  Data: 0.019 (0.052)\n",
      "Train: 6 [ 950/2388 ( 40%)]  Loss: 0.4792 (0.419)  Time: 0.436s,   36.71/s  (0.373s,   42.91/s)  LR: 3.969e-02  Data: 0.116 (0.052)\n",
      "Train: 6 [1000/2388 ( 42%)]  Loss: 0.3953 (0.420)  Time: 0.341s,   46.94/s  (0.373s,   42.94/s)  LR: 3.969e-02  Data: 0.020 (0.052)\n",
      "Train: 6 [1050/2388 ( 44%)]  Loss: 0.4139 (0.419)  Time: 0.373s,   42.89/s  (0.372s,   42.97/s)  LR: 3.969e-02  Data: 0.051 (0.052)\n",
      "Train: 6 [1100/2388 ( 46%)]  Loss: 0.4653 (0.419)  Time: 0.340s,   47.07/s  (0.372s,   42.97/s)  LR: 3.969e-02  Data: 0.020 (0.051)\n",
      "Train: 6 [1150/2388 ( 48%)]  Loss: 0.4469 (0.419)  Time: 0.498s,   32.14/s  (0.372s,   42.96/s)  LR: 3.969e-02  Data: 0.176 (0.052)\n",
      "Train: 6 [1200/2388 ( 50%)]  Loss: 0.4713 (0.419)  Time: 0.340s,   47.00/s  (0.372s,   42.99/s)  LR: 3.969e-02  Data: 0.019 (0.051)\n",
      "Train: 6 [1250/2388 ( 52%)]  Loss: 0.3960 (0.419)  Time: 0.469s,   34.15/s  (0.372s,   43.02/s)  LR: 3.969e-02  Data: 0.147 (0.051)\n",
      "Train: 6 [1300/2388 ( 54%)]  Loss: 0.4240 (0.419)  Time: 0.341s,   46.88/s  (0.371s,   43.07/s)  LR: 3.969e-02  Data: 0.020 (0.051)\n",
      "Train: 6 [1350/2388 ( 57%)]  Loss: 0.4786 (0.419)  Time: 0.446s,   35.84/s  (0.371s,   43.09/s)  LR: 3.969e-02  Data: 0.125 (0.050)\n",
      "Train: 6 [1400/2388 ( 59%)]  Loss: 0.4347 (0.420)  Time: 0.361s,   44.30/s  (0.371s,   43.11/s)  LR: 3.969e-02  Data: 0.040 (0.050)\n",
      "Train: 6 [1450/2388 ( 61%)]  Loss: 0.3724 (0.419)  Time: 0.397s,   40.27/s  (0.371s,   43.09/s)  LR: 3.969e-02  Data: 0.078 (0.050)\n",
      "Train: 6 [1500/2388 ( 63%)]  Loss: 0.5056 (0.419)  Time: 0.349s,   45.78/s  (0.371s,   43.12/s)  LR: 3.969e-02  Data: 0.027 (0.050)\n",
      "Train: 6 [1550/2388 ( 65%)]  Loss: 0.4213 (0.419)  Time: 0.411s,   38.91/s  (0.371s,   43.11/s)  LR: 3.969e-02  Data: 0.090 (0.050)\n",
      "Train: 6 [1600/2388 ( 67%)]  Loss: 0.3865 (0.419)  Time: 0.353s,   45.28/s  (0.371s,   43.12/s)  LR: 3.969e-02  Data: 0.031 (0.050)\n",
      "Train: 6 [1650/2388 ( 69%)]  Loss: 0.4411 (0.419)  Time: 0.437s,   36.59/s  (0.371s,   43.12/s)  LR: 3.969e-02  Data: 0.116 (0.050)\n",
      "Train: 6 [1700/2388 ( 71%)]  Loss: 0.4286 (0.419)  Time: 0.340s,   47.09/s  (0.371s,   43.13/s)  LR: 3.969e-02  Data: 0.019 (0.050)\n",
      "Train: 6 [1750/2388 ( 73%)]  Loss: 0.4279 (0.419)  Time: 0.459s,   34.83/s  (0.371s,   43.13/s)  LR: 3.969e-02  Data: 0.139 (0.050)\n",
      "Train: 6 [1800/2388 ( 75%)]  Loss: 0.3284 (0.418)  Time: 0.353s,   45.29/s  (0.371s,   43.13/s)  LR: 3.969e-02  Data: 0.033 (0.050)\n",
      "Train: 6 [1850/2388 ( 78%)]  Loss: 0.3792 (0.419)  Time: 0.446s,   35.87/s  (0.371s,   43.11/s)  LR: 3.969e-02  Data: 0.125 (0.050)\n",
      "Train: 6 [1900/2388 ( 80%)]  Loss: 0.4103 (0.419)  Time: 0.361s,   44.31/s  (0.371s,   43.09/s)  LR: 3.969e-02  Data: 0.041 (0.050)\n",
      "Train: 6 [1950/2388 ( 82%)]  Loss: 0.3916 (0.419)  Time: 0.428s,   37.42/s  (0.371s,   43.09/s)  LR: 3.969e-02  Data: 0.107 (0.050)\n",
      "Train: 6 [2000/2388 ( 84%)]  Loss: 0.3571 (0.419)  Time: 0.340s,   47.10/s  (0.371s,   43.09/s)  LR: 3.969e-02  Data: 0.020 (0.050)\n",
      "Train: 6 [2050/2388 ( 86%)]  Loss: 0.3820 (0.419)  Time: 0.460s,   34.75/s  (0.371s,   43.10/s)  LR: 3.969e-02  Data: 0.139 (0.050)\n",
      "Train: 6 [2100/2388 ( 88%)]  Loss: 0.3706 (0.418)  Time: 0.345s,   46.43/s  (0.371s,   43.13/s)  LR: 3.969e-02  Data: 0.020 (0.050)\n",
      "Train: 6 [2150/2388 ( 90%)]  Loss: 0.3735 (0.419)  Time: 0.392s,   40.80/s  (0.371s,   43.13/s)  LR: 3.969e-02  Data: 0.072 (0.050)\n",
      "Train: 6 [2200/2388 ( 92%)]  Loss: 0.4749 (0.419)  Time: 0.338s,   47.32/s  (0.371s,   43.12/s)  LR: 3.969e-02  Data: 0.018 (0.050)\n",
      "Train: 6 [2250/2388 ( 94%)]  Loss: 0.4597 (0.419)  Time: 0.606s,   26.41/s  (0.371s,   43.10/s)  LR: 3.969e-02  Data: 0.285 (0.050)\n",
      "Train: 6 [2300/2388 ( 96%)]  Loss: 0.4523 (0.419)  Time: 0.339s,   47.23/s  (0.371s,   43.10/s)  LR: 3.969e-02  Data: 0.019 (0.050)\n",
      "Train: 6 [2350/2388 ( 98%)]  Loss: 0.3448 (0.418)  Time: 0.433s,   36.94/s  (0.371s,   43.11/s)  LR: 3.969e-02  Data: 0.113 (0.050)\n",
      "Train: 6 [2387/2388 (100%)]  Loss: 0.3973 (0.419)  Time: 0.318s,   50.25/s  (0.371s,   43.12/s)  LR: 3.969e-02  Data: 0.000 (0.050)\n",
      "Test: [   0/2388]  Time: 1.419 (1.419)  Loss:  0.4629 (0.4629)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [  50/2388]  Time: 0.093 (0.299)  Loss:  0.1708 (0.3619)  Acc@1: 100.0000 (91.4216)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 100/2388]  Time: 0.093 (0.294)  Loss:  0.2878 (0.3658)  Acc@1: 93.7500 (91.4604)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 150/2388]  Time: 0.087 (0.288)  Loss:  0.2896 (0.3915)  Acc@1: 93.7500 (89.5695)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 200/2388]  Time: 0.091 (0.289)  Loss:  0.6118 (0.4097)  Acc@1: 68.7500 (88.2152)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 250/2388]  Time: 0.085 (0.286)  Loss:  0.4299 (0.4275)  Acc@1: 87.5000 (87.1016)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 300/2388]  Time: 0.092 (0.287)  Loss:  0.4709 (0.4385)  Acc@1: 81.2500 (86.6071)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 350/2388]  Time: 0.086 (0.287)  Loss:  0.4023 (0.4514)  Acc@1: 87.5000 (86.0933)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 400/2388]  Time: 0.094 (0.287)  Loss:  0.3948 (0.4534)  Acc@1: 93.7500 (85.9102)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 450/2388]  Time: 0.087 (0.287)  Loss:  0.5825 (0.4570)  Acc@1: 68.7500 (85.6569)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 500/2388]  Time: 0.092 (0.290)  Loss:  0.7339 (0.4553)  Acc@1: 62.5000 (85.5040)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 550/2388]  Time: 0.087 (0.289)  Loss:  0.8379 (0.4568)  Acc@1: 68.7500 (85.4696)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 600/2388]  Time: 0.092 (0.289)  Loss:  0.6572 (0.4700)  Acc@1: 75.0000 (84.7442)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 650/2388]  Time: 0.087 (0.288)  Loss:  0.5576 (0.4758)  Acc@1: 75.0000 (84.2838)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 700/2388]  Time: 0.112 (0.289)  Loss:  0.4797 (0.4810)  Acc@1: 87.5000 (84.0228)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 750/2388]  Time: 0.087 (0.288)  Loss:  0.7812 (0.4844)  Acc@1: 62.5000 (83.9048)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 800/2388]  Time: 0.092 (0.288)  Loss:  0.4036 (0.4859)  Acc@1: 81.2500 (83.7391)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 850/2388]  Time: 0.090 (0.287)  Loss:  0.5337 (0.4859)  Acc@1: 81.2500 (83.6442)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 900/2388]  Time: 0.092 (0.288)  Loss:  0.3203 (0.4833)  Acc@1: 87.5000 (83.7542)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 950/2388]  Time: 0.087 (0.287)  Loss:  0.9434 (0.4811)  Acc@1: 68.7500 (83.8920)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1000/2388]  Time: 0.092 (0.288)  Loss:  0.4841 (0.4787)  Acc@1: 87.5000 (83.9660)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1050/2388]  Time: 0.087 (0.288)  Loss:  0.5981 (0.4771)  Acc@1: 81.2500 (84.0450)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1100/2388]  Time: 0.093 (0.288)  Loss:  0.4912 (0.4761)  Acc@1: 87.5000 (84.2019)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1150/2388]  Time: 0.088 (0.288)  Loss:  0.7261 (0.4745)  Acc@1: 93.7500 (84.3560)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1200/2388]  Time: 0.092 (0.288)  Loss:  0.3911 (0.4743)  Acc@1: 75.0000 (84.3828)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1250/2388]  Time: 0.086 (0.288)  Loss:  0.7212 (0.4727)  Acc@1: 75.0000 (84.5224)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1300/2388]  Time: 0.093 (0.288)  Loss:  0.4536 (0.4719)  Acc@1: 87.5000 (84.5503)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1350/2388]  Time: 0.087 (0.288)  Loss:  0.3076 (0.4715)  Acc@1: 100.0000 (84.6318)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1400/2388]  Time: 0.091 (0.288)  Loss:  0.6650 (0.4706)  Acc@1: 75.0000 (84.7029)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1450/2388]  Time: 0.086 (0.288)  Loss:  1.5947 (0.4985)  Acc@1: 43.7500 (83.4381)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1500/2388]  Time: 0.094 (0.288)  Loss:  2.0762 (0.5307)  Acc@1: 12.5000 (81.9370)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1550/2388]  Time: 0.087 (0.288)  Loss:  1.6680 (0.5547)  Acc@1: 25.0000 (80.7664)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1600/2388]  Time: 0.091 (0.288)  Loss:  2.4844 (0.5773)  Acc@1:  0.0000 (79.6494)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1650/2388]  Time: 0.087 (0.288)  Loss:  3.0684 (0.6144)  Acc@1:  0.0000 (78.0663)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1700/2388]  Time: 0.092 (0.288)  Loss:  0.5435 (0.6243)  Acc@1: 75.0000 (77.2413)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1750/2388]  Time: 0.092 (0.287)  Loss:  0.5010 (0.6293)  Acc@1: 81.2500 (76.6598)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1800/2388]  Time: 0.091 (0.287)  Loss:  0.4961 (0.6326)  Acc@1: 68.7500 (76.1452)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1850/2388]  Time: 0.086 (0.287)  Loss:  1.1045 (0.6339)  Acc@1: 50.0000 (75.7428)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1900/2388]  Time: 0.093 (0.287)  Loss:  1.0986 (0.6335)  Acc@1: 43.7500 (75.5293)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1950/2388]  Time: 0.086 (0.287)  Loss:  1.0723 (0.6360)  Acc@1: 18.7500 (75.1313)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2000/2388]  Time: 0.091 (0.287)  Loss:  0.5361 (0.6365)  Acc@1: 68.7500 (74.8751)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2050/2388]  Time: 0.087 (0.287)  Loss:  0.9028 (0.6393)  Acc@1: 50.0000 (74.4728)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2100/2388]  Time: 0.093 (0.287)  Loss:  0.3125 (0.6386)  Acc@1: 81.2500 (74.2712)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2150/2388]  Time: 0.087 (0.286)  Loss:  0.2898 (0.6388)  Acc@1: 87.5000 (74.0673)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2200/2388]  Time: 0.092 (0.287)  Loss:  0.6851 (0.6405)  Acc@1: 62.5000 (73.7790)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2250/2388]  Time: 0.087 (0.287)  Loss:  0.6309 (0.6445)  Acc@1: 68.7500 (73.3035)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2300/2388]  Time: 0.093 (0.287)  Loss:  0.6919 (0.6481)  Acc@1: 50.0000 (72.9547)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2350/2388]  Time: 0.087 (0.287)  Loss:  0.6553 (0.6487)  Acc@1: 62.5000 (72.7616)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2388/2388]  Time: 0.015 (0.286)  Loss:  2.2441 (0.6647)  Acc@1:  0.0000 (72.2350)  Acc@5: 100.0000 (100.0000)\n",
      "Current checkpoints:\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-6.pth.tar', 72.2350170112536)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-5.pth.tar', 68.68359068306727)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-4.pth.tar', 66.8803978016226)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-2.pth.tar', 66.4224025124313)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-3.pth.tar', 65.83355142632819)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-1.pth.tar', 60.52865741952368)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-0.pth.tar', 59.013347291285)\n",
      "\n",
      "Train: 7 [   0/2388 (  0%)]  Loss: 0.4257 (0.426)  Time: 1.816s,    8.81/s  (1.816s,    8.81/s)  LR: 3.635e-02  Data: 1.496 (1.496)\n",
      "Train: 7 [  50/2388 (  2%)]  Loss: 0.4066 (0.420)  Time: 0.341s,   46.94/s  (0.402s,   39.77/s)  LR: 3.635e-02  Data: 0.021 (0.082)\n",
      "Train: 7 [ 100/2388 (  4%)]  Loss: 0.4329 (0.420)  Time: 0.435s,   36.78/s  (0.385s,   41.54/s)  LR: 3.635e-02  Data: 0.114 (0.064)\n",
      "Train: 7 [ 150/2388 (  6%)]  Loss: 0.5082 (0.418)  Time: 0.344s,   46.50/s  (0.382s,   41.93/s)  LR: 3.635e-02  Data: 0.023 (0.061)\n",
      "Train: 7 [ 200/2388 (  8%)]  Loss: 0.4476 (0.418)  Time: 0.447s,   35.79/s  (0.380s,   42.09/s)  LR: 3.635e-02  Data: 0.125 (0.059)\n",
      "Train: 7 [ 250/2388 ( 10%)]  Loss: 0.3998 (0.420)  Time: 0.352s,   45.45/s  (0.379s,   42.23/s)  LR: 3.635e-02  Data: 0.032 (0.058)\n",
      "Train: 7 [ 300/2388 ( 13%)]  Loss: 0.4650 (0.421)  Time: 0.545s,   29.37/s  (0.378s,   42.27/s)  LR: 3.635e-02  Data: 0.223 (0.058)\n",
      "Train: 7 [ 350/2388 ( 15%)]  Loss: 0.4582 (0.421)  Time: 0.341s,   46.88/s  (0.377s,   42.40/s)  LR: 3.635e-02  Data: 0.019 (0.056)\n",
      "Train: 7 [ 400/2388 ( 17%)]  Loss: 0.3956 (0.421)  Time: 0.407s,   39.28/s  (0.377s,   42.44/s)  LR: 3.635e-02  Data: 0.086 (0.056)\n",
      "Train: 7 [ 450/2388 ( 19%)]  Loss: 0.4367 (0.421)  Time: 0.340s,   47.03/s  (0.376s,   42.59/s)  LR: 3.635e-02  Data: 0.020 (0.055)\n",
      "Train: 7 [ 500/2388 ( 21%)]  Loss: 0.3897 (0.420)  Time: 0.458s,   34.97/s  (0.376s,   42.59/s)  LR: 3.635e-02  Data: 0.137 (0.055)\n",
      "Train: 7 [ 550/2388 ( 23%)]  Loss: 0.3444 (0.419)  Time: 0.354s,   45.19/s  (0.374s,   42.73/s)  LR: 3.635e-02  Data: 0.032 (0.053)\n",
      "Train: 7 [ 600/2388 ( 25%)]  Loss: 0.4290 (0.419)  Time: 0.354s,   45.15/s  (0.374s,   42.79/s)  LR: 3.635e-02  Data: 0.033 (0.053)\n",
      "Train: 7 [ 650/2388 ( 27%)]  Loss: 0.4366 (0.419)  Time: 0.604s,   26.47/s  (0.374s,   42.75/s)  LR: 3.635e-02  Data: 0.284 (0.053)\n",
      "Train: 7 [ 700/2388 ( 29%)]  Loss: 0.4002 (0.419)  Time: 0.340s,   47.08/s  (0.374s,   42.75/s)  LR: 3.635e-02  Data: 0.019 (0.053)\n",
      "Train: 7 [ 750/2388 ( 31%)]  Loss: 0.3516 (0.418)  Time: 0.366s,   43.76/s  (0.374s,   42.78/s)  LR: 3.635e-02  Data: 0.045 (0.053)\n",
      "Train: 7 [ 800/2388 ( 34%)]  Loss: 0.4464 (0.417)  Time: 0.340s,   47.09/s  (0.374s,   42.81/s)  LR: 3.635e-02  Data: 0.020 (0.053)\n",
      "Train: 7 [ 850/2388 ( 36%)]  Loss: 0.4478 (0.417)  Time: 0.484s,   33.08/s  (0.373s,   42.84/s)  LR: 3.635e-02  Data: 0.162 (0.052)\n",
      "Train: 7 [ 900/2388 ( 38%)]  Loss: 0.5008 (0.416)  Time: 0.339s,   47.20/s  (0.373s,   42.84/s)  LR: 3.635e-02  Data: 0.019 (0.052)\n",
      "Train: 7 [ 950/2388 ( 40%)]  Loss: 0.3859 (0.416)  Time: 0.493s,   32.43/s  (0.374s,   42.83/s)  LR: 3.635e-02  Data: 0.172 (0.053)\n",
      "Train: 7 [1000/2388 ( 42%)]  Loss: 0.3799 (0.416)  Time: 0.340s,   47.01/s  (0.373s,   42.88/s)  LR: 3.635e-02  Data: 0.019 (0.052)\n",
      "Train: 7 [1050/2388 ( 44%)]  Loss: 0.4637 (0.415)  Time: 0.488s,   32.79/s  (0.373s,   42.88/s)  LR: 3.635e-02  Data: 0.167 (0.052)\n",
      "Train: 7 [1100/2388 ( 46%)]  Loss: 0.5258 (0.415)  Time: 0.340s,   47.01/s  (0.373s,   42.89/s)  LR: 3.635e-02  Data: 0.020 (0.052)\n",
      "Train: 7 [1150/2388 ( 48%)]  Loss: 0.3838 (0.415)  Time: 0.452s,   35.39/s  (0.373s,   42.94/s)  LR: 3.635e-02  Data: 0.131 (0.052)\n",
      "Train: 7 [1200/2388 ( 50%)]  Loss: 0.5016 (0.416)  Time: 0.339s,   47.22/s  (0.372s,   42.98/s)  LR: 3.635e-02  Data: 0.019 (0.051)\n",
      "Train: 7 [1250/2388 ( 52%)]  Loss: 0.4342 (0.416)  Time: 0.452s,   35.41/s  (0.372s,   43.00/s)  LR: 3.635e-02  Data: 0.130 (0.051)\n",
      "Train: 7 [1300/2388 ( 54%)]  Loss: 0.4949 (0.416)  Time: 0.338s,   47.30/s  (0.372s,   43.03/s)  LR: 3.635e-02  Data: 0.018 (0.051)\n",
      "Train: 7 [1350/2388 ( 57%)]  Loss: 0.4092 (0.416)  Time: 0.402s,   39.78/s  (0.372s,   43.06/s)  LR: 3.635e-02  Data: 0.080 (0.051)\n",
      "Train: 7 [1400/2388 ( 59%)]  Loss: 0.3777 (0.416)  Time: 0.341s,   46.94/s  (0.372s,   43.06/s)  LR: 3.635e-02  Data: 0.019 (0.051)\n",
      "Train: 7 [1450/2388 ( 61%)]  Loss: 0.4981 (0.416)  Time: 0.466s,   34.37/s  (0.371s,   43.07/s)  LR: 3.635e-02  Data: 0.144 (0.050)\n",
      "Train: 7 [1500/2388 ( 63%)]  Loss: 0.3885 (0.415)  Time: 0.339s,   47.18/s  (0.371s,   43.08/s)  LR: 3.635e-02  Data: 0.019 (0.050)\n",
      "Train: 7 [1550/2388 ( 65%)]  Loss: 0.4099 (0.415)  Time: 0.402s,   39.81/s  (0.371s,   43.10/s)  LR: 3.635e-02  Data: 0.082 (0.050)\n",
      "Train: 7 [1600/2388 ( 67%)]  Loss: 0.4261 (0.415)  Time: 0.348s,   45.98/s  (0.371s,   43.10/s)  LR: 3.635e-02  Data: 0.026 (0.050)\n",
      "Train: 7 [1650/2388 ( 69%)]  Loss: 0.4071 (0.416)  Time: 0.425s,   37.67/s  (0.371s,   43.10/s)  LR: 3.635e-02  Data: 0.105 (0.050)\n",
      "Train: 7 [1700/2388 ( 71%)]  Loss: 0.4016 (0.416)  Time: 0.339s,   47.24/s  (0.371s,   43.11/s)  LR: 3.635e-02  Data: 0.019 (0.050)\n",
      "Train: 7 [1750/2388 ( 73%)]  Loss: 0.4220 (0.416)  Time: 0.468s,   34.18/s  (0.371s,   43.09/s)  LR: 3.635e-02  Data: 0.147 (0.050)\n",
      "Train: 7 [1800/2388 ( 75%)]  Loss: 0.3620 (0.416)  Time: 0.340s,   47.10/s  (0.371s,   43.10/s)  LR: 3.635e-02  Data: 0.019 (0.050)\n",
      "Train: 7 [1850/2388 ( 78%)]  Loss: 0.4160 (0.416)  Time: 0.463s,   34.58/s  (0.371s,   43.12/s)  LR: 3.635e-02  Data: 0.141 (0.050)\n",
      "Train: 7 [1900/2388 ( 80%)]  Loss: 0.3909 (0.416)  Time: 0.344s,   46.56/s  (0.371s,   43.13/s)  LR: 3.635e-02  Data: 0.020 (0.050)\n",
      "Train: 7 [1950/2388 ( 82%)]  Loss: 0.5042 (0.415)  Time: 0.359s,   44.52/s  (0.371s,   43.16/s)  LR: 3.635e-02  Data: 0.038 (0.050)\n",
      "Train: 7 [2000/2388 ( 84%)]  Loss: 0.3952 (0.415)  Time: 0.339s,   47.19/s  (0.371s,   43.15/s)  LR: 3.635e-02  Data: 0.019 (0.050)\n",
      "Train: 7 [2050/2388 ( 86%)]  Loss: 0.3727 (0.415)  Time: 0.432s,   37.05/s  (0.371s,   43.14/s)  LR: 3.635e-02  Data: 0.110 (0.050)\n",
      "Train: 7 [2100/2388 ( 88%)]  Loss: 0.3505 (0.415)  Time: 0.339s,   47.16/s  (0.371s,   43.15/s)  LR: 3.635e-02  Data: 0.019 (0.050)\n",
      "Train: 7 [2150/2388 ( 90%)]  Loss: 0.4846 (0.416)  Time: 0.457s,   34.99/s  (0.371s,   43.14/s)  LR: 3.635e-02  Data: 0.136 (0.050)\n",
      "Train: 7 [2200/2388 ( 92%)]  Loss: 0.4327 (0.416)  Time: 0.339s,   47.15/s  (0.371s,   43.14/s)  LR: 3.635e-02  Data: 0.018 (0.050)\n",
      "Train: 7 [2250/2388 ( 94%)]  Loss: 0.4072 (0.415)  Time: 0.445s,   35.95/s  (0.371s,   43.14/s)  LR: 3.635e-02  Data: 0.124 (0.050)\n",
      "Train: 7 [2300/2388 ( 96%)]  Loss: 0.4140 (0.415)  Time: 0.342s,   46.81/s  (0.371s,   43.15/s)  LR: 3.635e-02  Data: 0.020 (0.050)\n",
      "Train: 7 [2350/2388 ( 98%)]  Loss: 0.4245 (0.415)  Time: 0.482s,   33.22/s  (0.371s,   43.13/s)  LR: 3.635e-02  Data: 0.157 (0.050)\n",
      "Train: 7 [2387/2388 (100%)]  Loss: 0.4653 (0.414)  Time: 0.318s,   50.33/s  (0.371s,   43.13/s)  LR: 3.635e-02  Data: 0.000 (0.050)\n",
      "Test: [   0/2388]  Time: 1.276 (1.276)  Loss:  0.5474 (0.5474)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [  50/2388]  Time: 0.094 (0.294)  Loss:  0.2433 (0.3621)  Acc@1: 87.5000 (94.1176)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 100/2388]  Time: 0.086 (0.294)  Loss:  0.3047 (0.3619)  Acc@1: 100.0000 (94.3069)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 150/2388]  Time: 0.090 (0.288)  Loss:  0.3262 (0.3512)  Acc@1: 100.0000 (94.2881)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 200/2388]  Time: 0.086 (0.290)  Loss:  0.2532 (0.3462)  Acc@1: 93.7500 (94.1542)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 250/2388]  Time: 0.096 (0.287)  Loss:  0.2964 (0.3474)  Acc@1: 100.0000 (93.8745)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 300/2388]  Time: 0.094 (0.288)  Loss:  0.1962 (0.3459)  Acc@1: 100.0000 (93.6877)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 350/2388]  Time: 0.279 (0.286)  Loss:  0.2891 (0.3463)  Acc@1: 93.7500 (93.5541)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 400/2388]  Time: 0.086 (0.286)  Loss:  0.4661 (0.3453)  Acc@1: 81.2500 (93.3759)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 450/2388]  Time: 0.211 (0.286)  Loss:  0.1165 (0.3423)  Acc@1: 100.0000 (93.3897)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 500/2388]  Time: 0.086 (0.287)  Loss:  0.5073 (0.3362)  Acc@1: 87.5000 (93.5878)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 550/2388]  Time: 0.658 (0.287)  Loss:  0.4963 (0.3363)  Acc@1: 81.2500 (93.6025)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 600/2388]  Time: 0.086 (0.287)  Loss:  0.2478 (0.3396)  Acc@1: 93.7500 (93.3132)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 650/2388]  Time: 0.849 (0.287)  Loss:  0.2529 (0.3393)  Acc@1: 100.0000 (93.3180)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 700/2388]  Time: 0.093 (0.286)  Loss:  0.4624 (0.3444)  Acc@1: 93.7500 (93.1259)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 750/2388]  Time: 0.826 (0.286)  Loss:  0.4443 (0.3478)  Acc@1: 87.5000 (92.9011)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 800/2388]  Time: 0.086 (0.286)  Loss:  0.3774 (0.3538)  Acc@1: 87.5000 (92.6810)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 850/2388]  Time: 0.863 (0.286)  Loss:  0.3882 (0.3553)  Acc@1: 93.7500 (92.6484)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 900/2388]  Time: 0.086 (0.286)  Loss:  0.1377 (0.3539)  Acc@1: 100.0000 (92.6679)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 950/2388]  Time: 0.920 (0.286)  Loss:  0.3291 (0.3533)  Acc@1: 87.5000 (92.7379)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1000/2388]  Time: 0.085 (0.286)  Loss:  0.3005 (0.3519)  Acc@1: 100.0000 (92.8197)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1050/2388]  Time: 0.870 (0.287)  Loss:  1.0439 (0.3523)  Acc@1: 87.5000 (92.7807)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1100/2388]  Time: 0.085 (0.287)  Loss:  0.3213 (0.3542)  Acc@1: 93.7500 (92.7679)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1150/2388]  Time: 0.882 (0.287)  Loss:  1.0625 (0.3548)  Acc@1: 87.5000 (92.7780)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1200/2388]  Time: 0.085 (0.287)  Loss:  0.1620 (0.3557)  Acc@1: 100.0000 (92.7716)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1250/2388]  Time: 0.855 (0.287)  Loss:  0.5093 (0.3547)  Acc@1: 81.2500 (92.8757)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1300/2388]  Time: 0.086 (0.287)  Loss:  0.2106 (0.3532)  Acc@1: 100.0000 (92.9718)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1350/2388]  Time: 0.981 (0.287)  Loss:  0.2076 (0.3505)  Acc@1: 100.0000 (93.1162)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1400/2388]  Time: 0.085 (0.287)  Loss:  0.3296 (0.3505)  Acc@1: 93.7500 (93.1879)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1450/2388]  Time: 0.874 (0.288)  Loss:  1.7393 (0.3849)  Acc@1: 43.7500 (91.5102)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1500/2388]  Time: 0.086 (0.288)  Loss:  2.3164 (0.4265)  Acc@1:  0.0000 (89.4404)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1550/2388]  Time: 0.842 (0.288)  Loss:  2.6582 (0.4643)  Acc@1: 18.7500 (87.6572)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1600/2388]  Time: 0.085 (0.288)  Loss:  2.3613 (0.5006)  Acc@1:  0.0000 (85.8994)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1650/2388]  Time: 0.883 (0.288)  Loss:  2.9844 (0.5445)  Acc@1:  0.0000 (84.0286)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1700/2388]  Time: 0.086 (0.287)  Loss:  0.8799 (0.5604)  Acc@1: 62.5000 (82.8336)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1750/2388]  Time: 0.847 (0.288)  Loss:  0.4880 (0.5737)  Acc@1: 62.5000 (81.6462)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1800/2388]  Time: 0.085 (0.288)  Loss:  1.2500 (0.5878)  Acc@1: 25.0000 (80.5386)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1850/2388]  Time: 0.847 (0.288)  Loss:  1.3350 (0.6000)  Acc@1: 37.5000 (79.5516)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1900/2388]  Time: 0.091 (0.287)  Loss:  1.5205 (0.6116)  Acc@1: 37.5000 (78.7546)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1950/2388]  Time: 0.848 (0.287)  Loss:  1.4170 (0.6230)  Acc@1: 18.7500 (77.7870)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2000/2388]  Time: 0.086 (0.287)  Loss:  1.0410 (0.6307)  Acc@1: 43.7500 (76.9615)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2050/2388]  Time: 0.846 (0.288)  Loss:  1.1396 (0.6382)  Acc@1: 25.0000 (76.1702)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2100/2388]  Time: 0.085 (0.287)  Loss:  0.3850 (0.6422)  Acc@1: 75.0000 (75.6069)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2150/2388]  Time: 0.843 (0.287)  Loss:  0.3386 (0.6476)  Acc@1: 87.5000 (75.0116)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2200/2388]  Time: 0.091 (0.287)  Loss:  1.2471 (0.6560)  Acc@1: 37.5000 (74.2958)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2250/2388]  Time: 0.959 (0.287)  Loss:  1.9736 (0.6692)  Acc@1: 25.0000 (73.3702)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2300/2388]  Time: 0.085 (0.287)  Loss:  1.0869 (0.6833)  Acc@1: 37.5000 (72.5201)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2350/2388]  Time: 0.864 (0.288)  Loss:  0.9160 (0.6944)  Acc@1: 56.2500 (71.8019)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2388/2388]  Time: 0.014 (0.287)  Loss:  1.8301 (0.7010)  Acc@1:  0.0000 (71.5415)  Acc@5: 100.0000 (100.0000)\n",
      "Current checkpoints:\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-6.pth.tar', 72.2350170112536)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-7.pth.tar', 71.54148128762104)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-5.pth.tar', 68.68359068306727)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-4.pth.tar', 66.8803978016226)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-2.pth.tar', 66.4224025124313)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-3.pth.tar', 65.83355142632819)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-1.pth.tar', 60.52865741952368)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-0.pth.tar', 59.013347291285)\n",
      "\n",
      "Train: 8 [   0/2388 (  0%)]  Loss: 0.4877 (0.488)  Time: 1.893s,    8.45/s  (1.893s,    8.45/s)  LR: 3.273e-02  Data: 1.570 (1.570)\n",
      "Train: 8 [  50/2388 (  2%)]  Loss: 0.4121 (0.421)  Time: 0.345s,   46.40/s  (0.397s,   40.27/s)  LR: 3.273e-02  Data: 0.023 (0.076)\n",
      "Train: 8 [ 100/2388 (  4%)]  Loss: 0.4656 (0.416)  Time: 0.672s,   23.81/s  (0.386s,   41.49/s)  LR: 3.273e-02  Data: 0.351 (0.065)\n",
      "Train: 8 [ 150/2388 (  6%)]  Loss: 0.4087 (0.413)  Time: 0.358s,   44.70/s  (0.380s,   42.12/s)  LR: 3.273e-02  Data: 0.038 (0.059)\n",
      "Train: 8 [ 200/2388 (  8%)]  Loss: 0.3555 (0.415)  Time: 0.567s,   28.22/s  (0.378s,   42.33/s)  LR: 3.273e-02  Data: 0.246 (0.057)\n",
      "Train: 8 [ 250/2388 ( 10%)]  Loss: 0.4061 (0.415)  Time: 0.340s,   47.13/s  (0.375s,   42.61/s)  LR: 3.273e-02  Data: 0.020 (0.055)\n",
      "Train: 8 [ 300/2388 ( 13%)]  Loss: 0.4865 (0.413)  Time: 0.506s,   31.63/s  (0.373s,   42.85/s)  LR: 3.273e-02  Data: 0.194 (0.053)\n",
      "Train: 8 [ 350/2388 ( 15%)]  Loss: 0.3614 (0.414)  Time: 0.353s,   45.31/s  (0.373s,   42.90/s)  LR: 3.273e-02  Data: 0.030 (0.052)\n",
      "Train: 8 [ 400/2388 ( 17%)]  Loss: 0.4687 (0.414)  Time: 0.460s,   34.77/s  (0.373s,   42.86/s)  LR: 3.273e-02  Data: 0.139 (0.052)\n",
      "Train: 8 [ 450/2388 ( 19%)]  Loss: 0.3795 (0.413)  Time: 0.343s,   46.71/s  (0.372s,   42.97/s)  LR: 3.273e-02  Data: 0.021 (0.052)\n",
      "Train: 8 [ 500/2388 ( 21%)]  Loss: 0.5390 (0.412)  Time: 0.441s,   36.30/s  (0.372s,   43.03/s)  LR: 3.273e-02  Data: 0.119 (0.051)\n",
      "Train: 8 [ 550/2388 ( 23%)]  Loss: 0.4649 (0.411)  Time: 0.342s,   46.75/s  (0.371s,   43.10/s)  LR: 3.273e-02  Data: 0.022 (0.050)\n",
      "Train: 8 [ 600/2388 ( 25%)]  Loss: 0.4440 (0.412)  Time: 0.401s,   39.87/s  (0.371s,   43.13/s)  LR: 3.273e-02  Data: 0.081 (0.050)\n",
      "Train: 8 [ 650/2388 ( 27%)]  Loss: 0.3974 (0.410)  Time: 0.367s,   43.55/s  (0.371s,   43.11/s)  LR: 3.273e-02  Data: 0.048 (0.050)\n",
      "Train: 8 [ 700/2388 ( 29%)]  Loss: 0.4175 (0.411)  Time: 0.345s,   46.44/s  (0.371s,   43.16/s)  LR: 3.273e-02  Data: 0.023 (0.050)\n",
      "Train: 8 [ 750/2388 ( 31%)]  Loss: 0.4593 (0.411)  Time: 0.404s,   39.63/s  (0.371s,   43.17/s)  LR: 3.273e-02  Data: 0.082 (0.050)\n",
      "Train: 8 [ 800/2388 ( 34%)]  Loss: 0.3916 (0.411)  Time: 0.341s,   46.94/s  (0.371s,   43.13/s)  LR: 3.273e-02  Data: 0.020 (0.050)\n",
      "Train: 8 [ 850/2388 ( 36%)]  Loss: 0.4733 (0.411)  Time: 0.371s,   43.12/s  (0.371s,   43.07/s)  LR: 3.273e-02  Data: 0.050 (0.051)\n",
      "Train: 8 [ 900/2388 ( 38%)]  Loss: 0.3864 (0.411)  Time: 0.340s,   47.02/s  (0.372s,   43.03/s)  LR: 3.273e-02  Data: 0.019 (0.051)\n",
      "Train: 8 [ 950/2388 ( 40%)]  Loss: 0.4135 (0.411)  Time: 0.496s,   32.29/s  (0.372s,   43.03/s)  LR: 3.273e-02  Data: 0.174 (0.051)\n",
      "Train: 8 [1000/2388 ( 42%)]  Loss: 0.4065 (0.411)  Time: 0.338s,   47.27/s  (0.372s,   43.02/s)  LR: 3.273e-02  Data: 0.019 (0.051)\n",
      "Train: 8 [1050/2388 ( 44%)]  Loss: 0.2931 (0.411)  Time: 0.489s,   32.70/s  (0.371s,   43.09/s)  LR: 3.273e-02  Data: 0.168 (0.050)\n",
      "Train: 8 [1100/2388 ( 46%)]  Loss: 0.4088 (0.411)  Time: 0.342s,   46.75/s  (0.372s,   43.05/s)  LR: 3.273e-02  Data: 0.019 (0.051)\n",
      "Train: 8 [1150/2388 ( 48%)]  Loss: 0.3556 (0.411)  Time: 0.503s,   31.84/s  (0.371s,   43.07/s)  LR: 3.273e-02  Data: 0.181 (0.051)\n",
      "Train: 8 [1200/2388 ( 50%)]  Loss: 0.4705 (0.411)  Time: 0.339s,   47.24/s  (0.371s,   43.08/s)  LR: 3.273e-02  Data: 0.018 (0.051)\n",
      "Train: 8 [1250/2388 ( 52%)]  Loss: 0.3841 (0.411)  Time: 0.420s,   38.05/s  (0.371s,   43.10/s)  LR: 3.273e-02  Data: 0.088 (0.050)\n",
      "Train: 8 [1300/2388 ( 54%)]  Loss: 0.4943 (0.412)  Time: 0.341s,   46.95/s  (0.371s,   43.10/s)  LR: 3.273e-02  Data: 0.020 (0.050)\n",
      "Train: 8 [1350/2388 ( 57%)]  Loss: 0.3803 (0.411)  Time: 0.405s,   39.51/s  (0.371s,   43.13/s)  LR: 3.273e-02  Data: 0.084 (0.050)\n",
      "Train: 8 [1400/2388 ( 59%)]  Loss: 0.3943 (0.411)  Time: 0.340s,   47.07/s  (0.371s,   43.12/s)  LR: 3.273e-02  Data: 0.020 (0.050)\n",
      "Train: 8 [1450/2388 ( 61%)]  Loss: 0.3677 (0.411)  Time: 0.530s,   30.21/s  (0.371s,   43.11/s)  LR: 3.273e-02  Data: 0.208 (0.050)\n",
      "Train: 8 [1500/2388 ( 63%)]  Loss: 0.3611 (0.411)  Time: 0.339s,   47.14/s  (0.371s,   43.14/s)  LR: 3.273e-02  Data: 0.019 (0.050)\n",
      "Train: 8 [1550/2388 ( 65%)]  Loss: 0.4926 (0.411)  Time: 0.519s,   30.86/s  (0.371s,   43.13/s)  LR: 3.273e-02  Data: 0.197 (0.050)\n",
      "Train: 8 [1600/2388 ( 67%)]  Loss: 0.4497 (0.411)  Time: 0.347s,   46.06/s  (0.371s,   43.15/s)  LR: 3.273e-02  Data: 0.027 (0.050)\n",
      "Train: 8 [1650/2388 ( 69%)]  Loss: 0.4190 (0.411)  Time: 0.583s,   27.47/s  (0.371s,   43.15/s)  LR: 3.273e-02  Data: 0.261 (0.050)\n",
      "Train: 8 [1700/2388 ( 71%)]  Loss: 0.5468 (0.411)  Time: 0.341s,   46.89/s  (0.371s,   43.16/s)  LR: 3.273e-02  Data: 0.021 (0.050)\n",
      "Train: 8 [1750/2388 ( 73%)]  Loss: 0.3881 (0.411)  Time: 0.501s,   31.95/s  (0.370s,   43.19/s)  LR: 3.273e-02  Data: 0.180 (0.050)\n",
      "Train: 8 [1800/2388 ( 75%)]  Loss: 0.4788 (0.411)  Time: 0.353s,   45.38/s  (0.370s,   43.19/s)  LR: 3.273e-02  Data: 0.031 (0.050)\n",
      "Train: 8 [1850/2388 ( 78%)]  Loss: 0.4502 (0.410)  Time: 0.427s,   37.46/s  (0.371s,   43.18/s)  LR: 3.273e-02  Data: 0.106 (0.050)\n",
      "Train: 8 [1900/2388 ( 80%)]  Loss: 0.3936 (0.411)  Time: 0.340s,   47.03/s  (0.370s,   43.19/s)  LR: 3.273e-02  Data: 0.020 (0.050)\n",
      "Train: 8 [1950/2388 ( 82%)]  Loss: 0.4136 (0.411)  Time: 0.516s,   31.02/s  (0.371s,   43.15/s)  LR: 3.273e-02  Data: 0.196 (0.050)\n",
      "Train: 8 [2000/2388 ( 84%)]  Loss: 0.5752 (0.411)  Time: 0.339s,   47.24/s  (0.371s,   43.17/s)  LR: 3.273e-02  Data: 0.019 (0.050)\n",
      "Train: 8 [2050/2388 ( 86%)]  Loss: 0.4054 (0.411)  Time: 0.466s,   34.30/s  (0.371s,   43.17/s)  LR: 3.273e-02  Data: 0.146 (0.050)\n",
      "Train: 8 [2100/2388 ( 88%)]  Loss: 0.3700 (0.411)  Time: 0.339s,   47.20/s  (0.371s,   43.17/s)  LR: 3.273e-02  Data: 0.019 (0.050)\n",
      "Train: 8 [2150/2388 ( 90%)]  Loss: 0.4774 (0.411)  Time: 0.415s,   38.58/s  (0.371s,   43.17/s)  LR: 3.273e-02  Data: 0.094 (0.050)\n",
      "Train: 8 [2200/2388 ( 92%)]  Loss: 0.3971 (0.411)  Time: 0.343s,   46.71/s  (0.371s,   43.18/s)  LR: 3.273e-02  Data: 0.020 (0.050)\n",
      "Train: 8 [2250/2388 ( 94%)]  Loss: 0.4130 (0.411)  Time: 0.405s,   39.49/s  (0.371s,   43.18/s)  LR: 3.273e-02  Data: 0.084 (0.050)\n",
      "Train: 8 [2300/2388 ( 96%)]  Loss: 0.4211 (0.411)  Time: 0.339s,   47.17/s  (0.370s,   43.20/s)  LR: 3.273e-02  Data: 0.019 (0.050)\n",
      "Train: 8 [2350/2388 ( 98%)]  Loss: 0.4162 (0.411)  Time: 0.456s,   35.11/s  (0.370s,   43.21/s)  LR: 3.273e-02  Data: 0.135 (0.049)\n",
      "Train: 8 [2387/2388 (100%)]  Loss: 0.3182 (0.411)  Time: 0.319s,   50.11/s  (0.370s,   43.22/s)  LR: 3.273e-02  Data: 0.000 (0.049)\n",
      "Test: [   0/2388]  Time: 1.234 (1.234)  Loss:  0.2795 (0.2795)  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [  50/2388]  Time: 0.089 (0.295)  Loss:  0.1615 (0.2812)  Acc@1: 93.7500 (95.5882)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 100/2388]  Time: 0.086 (0.292)  Loss:  0.2288 (0.2907)  Acc@1: 100.0000 (94.9876)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 150/2388]  Time: 0.091 (0.290)  Loss:  0.2549 (0.3149)  Acc@1: 100.0000 (93.0050)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 200/2388]  Time: 0.092 (0.286)  Loss:  0.4893 (0.3275)  Acc@1: 75.0000 (91.7600)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 250/2388]  Time: 0.092 (0.288)  Loss:  0.2947 (0.3387)  Acc@1: 87.5000 (90.9363)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 300/2388]  Time: 0.087 (0.286)  Loss:  0.2186 (0.3383)  Acc@1: 93.7500 (90.9468)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 350/2388]  Time: 0.093 (0.287)  Loss:  0.2415 (0.3400)  Acc@1: 93.7500 (90.9188)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 400/2388]  Time: 0.092 (0.286)  Loss:  0.1997 (0.3373)  Acc@1: 100.0000 (91.0224)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 450/2388]  Time: 0.092 (0.287)  Loss:  0.2367 (0.3357)  Acc@1: 93.7500 (91.0477)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 500/2388]  Time: 0.086 (0.286)  Loss:  0.7852 (0.3323)  Acc@1: 68.7500 (91.1427)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 550/2388]  Time: 0.093 (0.287)  Loss:  0.7002 (0.3351)  Acc@1: 75.0000 (91.0050)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 600/2388]  Time: 0.086 (0.286)  Loss:  0.4431 (0.3447)  Acc@1: 81.2500 (90.4950)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 650/2388]  Time: 0.094 (0.287)  Loss:  0.2065 (0.3453)  Acc@1: 100.0000 (90.4186)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 700/2388]  Time: 0.087 (0.286)  Loss:  0.4407 (0.3493)  Acc@1: 87.5000 (90.2907)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 750/2388]  Time: 0.092 (0.287)  Loss:  0.6562 (0.3547)  Acc@1: 75.0000 (90.0882)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 800/2388]  Time: 0.086 (0.286)  Loss:  0.3523 (0.3596)  Acc@1: 87.5000 (89.8408)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 850/2388]  Time: 0.093 (0.287)  Loss:  0.3474 (0.3607)  Acc@1: 87.5000 (89.7694)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 900/2388]  Time: 0.086 (0.287)  Loss:  0.1044 (0.3587)  Acc@1: 93.7500 (89.7752)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 950/2388]  Time: 0.092 (0.287)  Loss:  0.3633 (0.3569)  Acc@1: 87.5000 (89.8134)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1000/2388]  Time: 0.087 (0.287)  Loss:  0.3372 (0.3557)  Acc@1: 87.5000 (89.7977)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1050/2388]  Time: 0.092 (0.287)  Loss:  0.2510 (0.3557)  Acc@1: 93.7500 (89.7598)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1100/2388]  Time: 0.087 (0.287)  Loss:  0.3042 (0.3550)  Acc@1: 93.7500 (89.8218)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1150/2388]  Time: 0.090 (0.288)  Loss:  0.4111 (0.3540)  Acc@1: 75.0000 (89.8621)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1200/2388]  Time: 0.087 (0.287)  Loss:  0.2389 (0.3514)  Acc@1: 87.5000 (89.9667)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1250/2388]  Time: 0.093 (0.288)  Loss:  0.4939 (0.3486)  Acc@1: 81.2500 (90.1279)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1300/2388]  Time: 0.087 (0.287)  Loss:  0.2463 (0.3475)  Acc@1: 93.7500 (90.2143)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1350/2388]  Time: 0.092 (0.288)  Loss:  0.2632 (0.3466)  Acc@1: 87.5000 (90.3035)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1400/2388]  Time: 0.086 (0.287)  Loss:  0.3486 (0.3453)  Acc@1: 93.7500 (90.3863)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1450/2388]  Time: 0.093 (0.288)  Loss:  1.2373 (0.3721)  Acc@1: 50.0000 (89.0722)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1500/2388]  Time: 0.087 (0.288)  Loss:  1.8223 (0.4037)  Acc@1: 12.5000 (87.4459)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1550/2388]  Time: 0.094 (0.288)  Loss:  2.2773 (0.4294)  Acc@1: 25.0000 (86.1098)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1600/2388]  Time: 0.087 (0.288)  Loss:  1.3652 (0.4499)  Acc@1: 37.5000 (85.0679)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1650/2388]  Time: 0.092 (0.288)  Loss:  2.1055 (0.4808)  Acc@1:  0.0000 (83.4986)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1700/2388]  Time: 0.086 (0.287)  Loss:  0.4592 (0.5010)  Acc@1: 75.0000 (82.3266)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1750/2388]  Time: 0.093 (0.287)  Loss:  0.5879 (0.5177)  Acc@1: 68.7500 (81.3107)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1800/2388]  Time: 0.087 (0.287)  Loss:  0.9854 (0.5326)  Acc@1: 50.0000 (80.3443)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1850/2388]  Time: 0.092 (0.287)  Loss:  1.0967 (0.5457)  Acc@1: 43.7500 (79.4706)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1900/2388]  Time: 0.086 (0.287)  Loss:  1.8486 (0.5581)  Acc@1: 31.2500 (78.7941)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1950/2388]  Time: 0.092 (0.287)  Loss:  1.5088 (0.5727)  Acc@1: 25.0000 (77.9216)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2000/2388]  Time: 0.087 (0.287)  Loss:  0.7686 (0.5816)  Acc@1: 62.5000 (77.3176)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2050/2388]  Time: 0.092 (0.287)  Loss:  1.0889 (0.5929)  Acc@1: 43.7500 (76.5755)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2100/2388]  Time: 0.086 (0.287)  Loss:  0.3799 (0.5980)  Acc@1: 81.2500 (76.1274)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2150/2388]  Time: 0.086 (0.287)  Loss:  0.3108 (0.6051)  Acc@1: 87.5000 (75.6450)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2200/2388]  Time: 0.087 (0.287)  Loss:  1.4434 (0.6151)  Acc@1: 37.5000 (75.0426)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2250/2388]  Time: 0.093 (0.287)  Loss:  0.6748 (0.6289)  Acc@1: 68.7500 (74.2975)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2300/2388]  Time: 0.087 (0.287)  Loss:  1.0566 (0.6440)  Acc@1: 62.5000 (73.5713)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2350/2388]  Time: 0.096 (0.287)  Loss:  1.3799 (0.6527)  Acc@1: 50.0000 (73.1045)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2388/2388]  Time: 0.014 (0.287)  Loss:  0.6426 (0.6674)  Acc@1: 50.0000 (72.5569)  Acc@5: 100.0000 (100.0000)\n",
      "Current checkpoints:\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-8.pth.tar', 72.55692227165663)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-6.pth.tar', 72.2350170112536)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-7.pth.tar', 71.54148128762104)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-5.pth.tar', 68.68359068306727)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-4.pth.tar', 66.8803978016226)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-2.pth.tar', 66.4224025124313)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-3.pth.tar', 65.83355142632819)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-1.pth.tar', 60.52865741952368)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-0.pth.tar', 59.013347291285)\n",
      "\n",
      "Train: 9 [   0/2388 (  0%)]  Loss: 0.3899 (0.390)  Time: 1.869s,    8.56/s  (1.869s,    8.56/s)  LR: 2.891e-02  Data: 1.548 (1.548)\n",
      "Train: 9 [  50/2388 (  2%)]  Loss: 0.4091 (0.416)  Time: 0.338s,   47.33/s  (0.401s,   39.88/s)  LR: 2.891e-02  Data: 0.018 (0.080)\n",
      "Train: 9 [ 100/2388 (  4%)]  Loss: 0.4065 (0.420)  Time: 0.425s,   37.69/s  (0.387s,   41.31/s)  LR: 2.891e-02  Data: 0.103 (0.066)\n",
      "Train: 9 [ 150/2388 (  6%)]  Loss: 0.4197 (0.418)  Time: 0.340s,   47.00/s  (0.383s,   41.73/s)  LR: 2.891e-02  Data: 0.020 (0.063)\n",
      "Train: 9 [ 200/2388 (  8%)]  Loss: 0.3259 (0.412)  Time: 0.390s,   41.03/s  (0.379s,   42.26/s)  LR: 2.891e-02  Data: 0.068 (0.058)\n",
      "Train: 9 [ 250/2388 ( 10%)]  Loss: 0.4875 (0.410)  Time: 0.409s,   39.13/s  (0.377s,   42.43/s)  LR: 2.891e-02  Data: 0.089 (0.056)\n",
      "Train: 9 [ 300/2388 ( 13%)]  Loss: 0.3976 (0.410)  Time: 0.349s,   45.80/s  (0.375s,   42.67/s)  LR: 2.891e-02  Data: 0.029 (0.054)\n",
      "Train: 9 [ 350/2388 ( 15%)]  Loss: 0.3642 (0.409)  Time: 0.500s,   31.97/s  (0.376s,   42.61/s)  LR: 2.891e-02  Data: 0.179 (0.055)\n",
      "Train: 9 [ 400/2388 ( 17%)]  Loss: 0.4134 (0.409)  Time: 0.342s,   46.76/s  (0.374s,   42.78/s)  LR: 2.891e-02  Data: 0.018 (0.053)\n",
      "Train: 9 [ 450/2388 ( 19%)]  Loss: 0.3892 (0.410)  Time: 0.645s,   24.81/s  (0.374s,   42.80/s)  LR: 2.891e-02  Data: 0.325 (0.053)\n",
      "Train: 9 [ 500/2388 ( 21%)]  Loss: 0.4656 (0.410)  Time: 0.340s,   47.11/s  (0.373s,   42.93/s)  LR: 2.891e-02  Data: 0.020 (0.052)\n",
      "Train: 9 [ 550/2388 ( 23%)]  Loss: 0.3811 (0.410)  Time: 0.564s,   28.38/s  (0.373s,   42.91/s)  LR: 2.891e-02  Data: 0.244 (0.052)\n",
      "Train: 9 [ 600/2388 ( 25%)]  Loss: 0.3605 (0.410)  Time: 0.339s,   47.19/s  (0.373s,   42.92/s)  LR: 2.891e-02  Data: 0.018 (0.052)\n",
      "Train: 9 [ 650/2388 ( 27%)]  Loss: 0.3582 (0.409)  Time: 0.396s,   40.40/s  (0.373s,   42.94/s)  LR: 2.891e-02  Data: 0.075 (0.052)\n",
      "Train: 9 [ 700/2388 ( 29%)]  Loss: 0.3415 (0.408)  Time: 0.340s,   47.07/s  (0.372s,   42.96/s)  LR: 2.891e-02  Data: 0.019 (0.051)\n",
      "Train: 9 [ 750/2388 ( 31%)]  Loss: 0.4587 (0.408)  Time: 0.442s,   36.24/s  (0.372s,   43.04/s)  LR: 2.891e-02  Data: 0.120 (0.051)\n",
      "Train: 9 [ 800/2388 ( 34%)]  Loss: 0.4749 (0.409)  Time: 0.338s,   47.28/s  (0.372s,   43.05/s)  LR: 2.891e-02  Data: 0.018 (0.051)\n",
      "Train: 9 [ 850/2388 ( 36%)]  Loss: 0.4284 (0.409)  Time: 0.389s,   41.14/s  (0.372s,   43.04/s)  LR: 2.891e-02  Data: 0.069 (0.051)\n",
      "Train: 9 [ 900/2388 ( 38%)]  Loss: 0.3280 (0.409)  Time: 0.342s,   46.75/s  (0.371s,   43.09/s)  LR: 2.891e-02  Data: 0.022 (0.050)\n",
      "Train: 9 [ 950/2388 ( 40%)]  Loss: 0.4838 (0.409)  Time: 0.420s,   38.06/s  (0.371s,   43.07/s)  LR: 2.891e-02  Data: 0.099 (0.051)\n",
      "Train: 9 [1000/2388 ( 42%)]  Loss: 0.5202 (0.409)  Time: 0.339s,   47.23/s  (0.372s,   43.05/s)  LR: 2.891e-02  Data: 0.019 (0.051)\n",
      "Train: 9 [1050/2388 ( 44%)]  Loss: 0.3318 (0.408)  Time: 0.448s,   35.74/s  (0.371s,   43.09/s)  LR: 2.891e-02  Data: 0.128 (0.050)\n",
      "Train: 9 [1100/2388 ( 46%)]  Loss: 0.3912 (0.408)  Time: 0.341s,   46.94/s  (0.372s,   43.05/s)  LR: 2.891e-02  Data: 0.019 (0.051)\n",
      "Train: 9 [1150/2388 ( 48%)]  Loss: 0.4530 (0.408)  Time: 0.526s,   30.41/s  (0.372s,   43.02/s)  LR: 2.891e-02  Data: 0.206 (0.051)\n",
      "Train: 9 [1200/2388 ( 50%)]  Loss: 0.4963 (0.408)  Time: 0.340s,   47.11/s  (0.372s,   43.04/s)  LR: 2.891e-02  Data: 0.018 (0.051)\n",
      "Train: 9 [1250/2388 ( 52%)]  Loss: 0.4431 (0.408)  Time: 0.494s,   32.40/s  (0.372s,   43.03/s)  LR: 2.891e-02  Data: 0.174 (0.051)\n",
      "Train: 9 [1300/2388 ( 54%)]  Loss: 0.3121 (0.408)  Time: 0.344s,   46.48/s  (0.372s,   43.04/s)  LR: 2.891e-02  Data: 0.020 (0.051)\n",
      "Train: 9 [1350/2388 ( 57%)]  Loss: 0.3156 (0.408)  Time: 0.577s,   27.72/s  (0.372s,   43.03/s)  LR: 2.891e-02  Data: 0.255 (0.051)\n",
      "Train: 9 [1400/2388 ( 59%)]  Loss: 0.3791 (0.408)  Time: 0.341s,   46.99/s  (0.372s,   43.02/s)  LR: 2.891e-02  Data: 0.020 (0.051)\n",
      "Train: 9 [1450/2388 ( 61%)]  Loss: 0.4659 (0.408)  Time: 0.493s,   32.45/s  (0.372s,   43.04/s)  LR: 2.891e-02  Data: 0.173 (0.051)\n",
      "Train: 9 [1500/2388 ( 63%)]  Loss: 0.3966 (0.408)  Time: 0.340s,   47.07/s  (0.372s,   43.02/s)  LR: 2.891e-02  Data: 0.020 (0.051)\n",
      "Train: 9 [1550/2388 ( 65%)]  Loss: 0.4280 (0.408)  Time: 0.414s,   38.69/s  (0.372s,   43.01/s)  LR: 2.891e-02  Data: 0.094 (0.051)\n",
      "Train: 9 [1600/2388 ( 67%)]  Loss: 0.3844 (0.408)  Time: 0.339s,   47.13/s  (0.372s,   43.05/s)  LR: 2.891e-02  Data: 0.019 (0.051)\n",
      "Train: 9 [1650/2388 ( 69%)]  Loss: 0.3356 (0.408)  Time: 0.428s,   37.35/s  (0.372s,   43.06/s)  LR: 2.891e-02  Data: 0.107 (0.051)\n",
      "Train: 9 [1700/2388 ( 71%)]  Loss: 0.3884 (0.408)  Time: 0.339s,   47.25/s  (0.371s,   43.08/s)  LR: 2.891e-02  Data: 0.019 (0.050)\n",
      "Train: 9 [1750/2388 ( 73%)]  Loss: 0.4599 (0.408)  Time: 0.479s,   33.40/s  (0.371s,   43.09/s)  LR: 2.891e-02  Data: 0.158 (0.050)\n",
      "Train: 9 [1800/2388 ( 75%)]  Loss: 0.3739 (0.407)  Time: 0.340s,   47.07/s  (0.371s,   43.11/s)  LR: 2.891e-02  Data: 0.020 (0.050)\n",
      "Train: 9 [1850/2388 ( 78%)]  Loss: 0.3565 (0.407)  Time: 0.490s,   32.66/s  (0.371s,   43.13/s)  LR: 2.891e-02  Data: 0.170 (0.050)\n",
      "Train: 9 [1900/2388 ( 80%)]  Loss: 0.3539 (0.407)  Time: 0.342s,   46.83/s  (0.371s,   43.15/s)  LR: 2.891e-02  Data: 0.021 (0.050)\n",
      "Train: 9 [1950/2388 ( 82%)]  Loss: 0.3980 (0.408)  Time: 0.464s,   34.47/s  (0.371s,   43.15/s)  LR: 2.891e-02  Data: 0.144 (0.050)\n",
      "Train: 9 [2000/2388 ( 84%)]  Loss: 0.3707 (0.408)  Time: 0.342s,   46.76/s  (0.371s,   43.15/s)  LR: 2.891e-02  Data: 0.022 (0.050)\n",
      "Train: 9 [2050/2388 ( 86%)]  Loss: 0.4048 (0.408)  Time: 0.612s,   26.12/s  (0.371s,   43.14/s)  LR: 2.891e-02  Data: 0.293 (0.050)\n",
      "Train: 9 [2100/2388 ( 88%)]  Loss: 0.4594 (0.408)  Time: 0.340s,   47.03/s  (0.371s,   43.12/s)  LR: 2.891e-02  Data: 0.020 (0.050)\n",
      "Train: 9 [2150/2388 ( 90%)]  Loss: 0.4339 (0.408)  Time: 0.456s,   35.11/s  (0.371s,   43.12/s)  LR: 2.891e-02  Data: 0.134 (0.050)\n",
      "Train: 9 [2200/2388 ( 92%)]  Loss: 0.3891 (0.408)  Time: 0.343s,   46.59/s  (0.371s,   43.12/s)  LR: 2.891e-02  Data: 0.020 (0.050)\n",
      "Train: 9 [2250/2388 ( 94%)]  Loss: 0.4263 (0.407)  Time: 0.492s,   32.50/s  (0.371s,   43.12/s)  LR: 2.891e-02  Data: 0.170 (0.050)\n",
      "Train: 9 [2300/2388 ( 96%)]  Loss: 0.3346 (0.407)  Time: 0.338s,   47.36/s  (0.371s,   43.14/s)  LR: 2.891e-02  Data: 0.018 (0.050)\n",
      "Train: 9 [2350/2388 ( 98%)]  Loss: 0.3449 (0.407)  Time: 0.492s,   32.52/s  (0.371s,   43.16/s)  LR: 2.891e-02  Data: 0.171 (0.050)\n",
      "Train: 9 [2387/2388 (100%)]  Loss: 0.4230 (0.407)  Time: 0.319s,   50.23/s  (0.371s,   43.16/s)  LR: 2.891e-02  Data: 0.000 (0.050)\n",
      "Test: [   0/2388]  Time: 1.249 (1.249)  Loss:  0.2415 (0.2415)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [  50/2388]  Time: 0.087 (0.293)  Loss:  0.1591 (0.3311)  Acc@1: 100.0000 (93.5049)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 100/2388]  Time: 0.560 (0.290)  Loss:  0.2334 (0.3465)  Acc@1: 100.0000 (92.6361)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 150/2388]  Time: 0.086 (0.285)  Loss:  0.1951 (0.3510)  Acc@1: 100.0000 (91.8460)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 200/2388]  Time: 0.833 (0.288)  Loss:  0.4336 (0.3562)  Acc@1: 81.2500 (91.2624)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 250/2388]  Time: 0.086 (0.286)  Loss:  0.2128 (0.3597)  Acc@1: 100.0000 (90.7371)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 300/2388]  Time: 0.892 (0.289)  Loss:  0.1962 (0.3585)  Acc@1: 93.7500 (90.6146)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 350/2388]  Time: 0.085 (0.288)  Loss:  0.2072 (0.3588)  Acc@1: 100.0000 (90.5627)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 400/2388]  Time: 0.847 (0.288)  Loss:  0.6167 (0.3557)  Acc@1: 87.5000 (90.5704)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 450/2388]  Time: 0.086 (0.287)  Loss:  0.1737 (0.3519)  Acc@1: 93.7500 (90.7289)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 500/2388]  Time: 0.872 (0.289)  Loss:  0.8511 (0.3456)  Acc@1: 75.0000 (90.9556)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 550/2388]  Time: 0.086 (0.289)  Loss:  0.9409 (0.3492)  Acc@1: 68.7500 (90.8802)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 600/2388]  Time: 0.841 (0.288)  Loss:  0.2971 (0.3563)  Acc@1: 87.5000 (90.4534)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 650/2388]  Time: 0.085 (0.288)  Loss:  0.1802 (0.3546)  Acc@1: 100.0000 (90.4954)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 700/2388]  Time: 0.844 (0.287)  Loss:  0.7261 (0.3617)  Acc@1: 81.2500 (90.2372)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 750/2388]  Time: 0.086 (0.287)  Loss:  0.5010 (0.3663)  Acc@1: 75.0000 (90.0632)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 800/2388]  Time: 0.873 (0.287)  Loss:  0.2473 (0.3718)  Acc@1: 93.7500 (89.8564)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 850/2388]  Time: 0.086 (0.286)  Loss:  0.2996 (0.3734)  Acc@1: 93.7500 (89.8061)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 900/2388]  Time: 0.826 (0.286)  Loss:  0.1647 (0.3722)  Acc@1: 93.7500 (89.7822)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 950/2388]  Time: 0.086 (0.286)  Loss:  0.6548 (0.3726)  Acc@1: 75.0000 (89.8068)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1000/2388]  Time: 0.862 (0.286)  Loss:  0.4121 (0.3716)  Acc@1: 93.7500 (89.8601)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1050/2388]  Time: 0.091 (0.285)  Loss:  0.8975 (0.3728)  Acc@1: 87.5000 (89.8430)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1100/2388]  Time: 0.869 (0.286)  Loss:  0.3215 (0.3735)  Acc@1: 87.5000 (89.8558)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1150/2388]  Time: 0.086 (0.286)  Loss:  1.4111 (0.3734)  Acc@1: 81.2500 (89.8838)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1200/2388]  Time: 0.874 (0.286)  Loss:  0.2644 (0.3718)  Acc@1: 81.2500 (89.9147)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1250/2388]  Time: 0.086 (0.286)  Loss:  0.5068 (0.3699)  Acc@1: 81.2500 (90.0829)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1300/2388]  Time: 0.861 (0.286)  Loss:  0.1810 (0.3678)  Acc@1: 100.0000 (90.1998)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1350/2388]  Time: 0.086 (0.286)  Loss:  0.1770 (0.3650)  Acc@1: 100.0000 (90.3220)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1400/2388]  Time: 1.022 (0.287)  Loss:  0.3086 (0.3630)  Acc@1: 93.7500 (90.4086)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1450/2388]  Time: 0.086 (0.287)  Loss:  0.7993 (0.3840)  Acc@1: 62.5000 (89.3220)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1500/2388]  Time: 0.887 (0.287)  Loss:  1.4678 (0.4069)  Acc@1: 43.7500 (88.1454)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1550/2388]  Time: 0.085 (0.287)  Loss:  2.2012 (0.4268)  Acc@1: 31.2500 (87.0729)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1600/2388]  Time: 0.864 (0.287)  Loss:  1.9688 (0.4410)  Acc@1: 18.7500 (86.2664)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1650/2388]  Time: 0.085 (0.287)  Loss:  3.0938 (0.4734)  Acc@1:  0.0000 (84.7782)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1700/2388]  Time: 0.868 (0.287)  Loss:  0.4963 (0.4949)  Acc@1: 81.2500 (83.4913)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1750/2388]  Time: 0.086 (0.287)  Loss:  0.5801 (0.5117)  Acc@1: 75.0000 (82.2994)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1800/2388]  Time: 0.859 (0.287)  Loss:  0.8232 (0.5275)  Acc@1: 68.7500 (81.2500)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1850/2388]  Time: 0.086 (0.287)  Loss:  1.7900 (0.5408)  Acc@1: 25.0000 (80.2843)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1900/2388]  Time: 0.863 (0.287)  Loss:  1.5010 (0.5527)  Acc@1: 25.0000 (79.5765)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1950/2388]  Time: 0.085 (0.287)  Loss:  1.7744 (0.5675)  Acc@1: 12.5000 (78.6360)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2000/2388]  Time: 0.824 (0.287)  Loss:  1.0010 (0.5807)  Acc@1: 56.2500 (77.8017)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2050/2388]  Time: 0.086 (0.287)  Loss:  1.1309 (0.5944)  Acc@1: 31.2500 (76.9137)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2100/2388]  Time: 0.851 (0.287)  Loss:  0.5620 (0.6017)  Acc@1: 75.0000 (76.2821)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2150/2388]  Time: 0.085 (0.287)  Loss:  0.6650 (0.6108)  Acc@1: 75.0000 (75.6247)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2200/2388]  Time: 0.864 (0.287)  Loss:  1.0361 (0.6190)  Acc@1: 31.2500 (74.9915)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2250/2388]  Time: 0.091 (0.287)  Loss:  0.8154 (0.6298)  Acc@1: 56.2500 (74.2337)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2300/2388]  Time: 0.927 (0.287)  Loss:  0.8579 (0.6426)  Acc@1: 50.0000 (73.4681)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2350/2388]  Time: 0.086 (0.287)  Loss:  1.1816 (0.6499)  Acc@1: 56.2500 (72.9184)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2388/2388]  Time: 0.014 (0.287)  Loss:  1.4600 (0.6591)  Acc@1:  0.0000 (72.6093)  Acc@5: 100.0000 (100.0000)\n",
      "Current checkpoints:\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-9.pth.tar', 72.60926459042136)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-8.pth.tar', 72.55692227165663)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-6.pth.tar', 72.2350170112536)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-7.pth.tar', 71.54148128762104)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-5.pth.tar', 68.68359068306727)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-4.pth.tar', 66.8803978016226)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-2.pth.tar', 66.4224025124313)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-3.pth.tar', 65.83355142632819)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-1.pth.tar', 60.52865741952368)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-0.pth.tar', 59.013347291285)\n",
      "\n",
      "Train: 10 [   0/2388 (  0%)]  Loss: 0.3084 (0.308)  Time: 1.870s,    8.56/s  (1.870s,    8.56/s)  LR: 2.500e-02  Data: 1.549 (1.549)\n",
      "Train: 10 [  50/2388 (  2%)]  Loss: 0.3957 (0.415)  Time: 0.338s,   47.36/s  (0.399s,   40.14/s)  LR: 2.500e-02  Data: 0.018 (0.078)\n",
      "Train: 10 [ 100/2388 (  4%)]  Loss: 0.3621 (0.409)  Time: 0.432s,   37.05/s  (0.383s,   41.74/s)  LR: 2.500e-02  Data: 0.111 (0.062)\n",
      "Train: 10 [ 150/2388 (  6%)]  Loss: 0.5035 (0.410)  Time: 0.341s,   46.94/s  (0.379s,   42.17/s)  LR: 2.500e-02  Data: 0.019 (0.058)\n",
      "Train: 10 [ 200/2388 (  8%)]  Loss: 0.3515 (0.407)  Time: 0.420s,   38.11/s  (0.378s,   42.33/s)  LR: 2.500e-02  Data: 0.099 (0.057)\n",
      "Train: 10 [ 250/2388 ( 10%)]  Loss: 0.3387 (0.406)  Time: 0.341s,   46.85/s  (0.375s,   42.61/s)  LR: 2.500e-02  Data: 0.021 (0.055)\n",
      "Train: 10 [ 300/2388 ( 13%)]  Loss: 0.2818 (0.405)  Time: 0.455s,   35.18/s  (0.375s,   42.68/s)  LR: 2.500e-02  Data: 0.134 (0.054)\n",
      "Train: 10 [ 350/2388 ( 15%)]  Loss: 0.4576 (0.407)  Time: 0.351s,   45.57/s  (0.374s,   42.74/s)  LR: 2.500e-02  Data: 0.031 (0.054)\n",
      "Train: 10 [ 400/2388 ( 17%)]  Loss: 0.3572 (0.407)  Time: 0.506s,   31.64/s  (0.375s,   42.66/s)  LR: 2.500e-02  Data: 0.184 (0.054)\n",
      "Train: 10 [ 450/2388 ( 19%)]  Loss: 0.4338 (0.407)  Time: 0.342s,   46.79/s  (0.375s,   42.70/s)  LR: 2.500e-02  Data: 0.021 (0.054)\n",
      "Train: 10 [ 500/2388 ( 21%)]  Loss: 0.3843 (0.408)  Time: 0.414s,   38.61/s  (0.375s,   42.72/s)  LR: 2.500e-02  Data: 0.091 (0.054)\n",
      "Train: 10 [ 550/2388 ( 23%)]  Loss: 0.3859 (0.407)  Time: 0.340s,   47.03/s  (0.374s,   42.82/s)  LR: 2.500e-02  Data: 0.020 (0.053)\n",
      "Train: 10 [ 600/2388 ( 25%)]  Loss: 0.3441 (0.406)  Time: 0.588s,   27.19/s  (0.374s,   42.82/s)  LR: 2.500e-02  Data: 0.267 (0.053)\n",
      "Train: 10 [ 650/2388 ( 27%)]  Loss: 0.4518 (0.406)  Time: 0.362s,   44.21/s  (0.373s,   42.87/s)  LR: 2.500e-02  Data: 0.040 (0.052)\n",
      "Train: 10 [ 700/2388 ( 29%)]  Loss: 0.3599 (0.406)  Time: 0.342s,   46.76/s  (0.372s,   42.95/s)  LR: 2.500e-02  Data: 0.022 (0.052)\n",
      "Train: 10 [ 750/2388 ( 31%)]  Loss: 0.4930 (0.405)  Time: 0.420s,   38.12/s  (0.372s,   42.96/s)  LR: 2.500e-02  Data: 0.099 (0.052)\n",
      "Train: 10 [ 800/2388 ( 34%)]  Loss: 0.4867 (0.406)  Time: 0.389s,   41.10/s  (0.372s,   42.97/s)  LR: 2.500e-02  Data: 0.069 (0.051)\n",
      "Train: 10 [ 850/2388 ( 36%)]  Loss: 0.4600 (0.406)  Time: 0.346s,   46.28/s  (0.373s,   42.93/s)  LR: 2.500e-02  Data: 0.026 (0.052)\n",
      "Train: 10 [ 900/2388 ( 38%)]  Loss: 0.4104 (0.406)  Time: 0.678s,   23.61/s  (0.373s,   42.93/s)  LR: 2.500e-02  Data: 0.358 (0.052)\n",
      "Train: 10 [ 950/2388 ( 40%)]  Loss: 0.4367 (0.407)  Time: 0.349s,   45.85/s  (0.373s,   42.91/s)  LR: 2.500e-02  Data: 0.029 (0.052)\n",
      "Train: 10 [1000/2388 ( 42%)]  Loss: 0.3331 (0.406)  Time: 0.455s,   35.13/s  (0.373s,   42.93/s)  LR: 2.500e-02  Data: 0.130 (0.052)\n",
      "Train: 10 [1050/2388 ( 44%)]  Loss: 0.3858 (0.407)  Time: 0.350s,   45.78/s  (0.372s,   42.97/s)  LR: 2.500e-02  Data: 0.029 (0.051)\n",
      "Train: 10 [1100/2388 ( 46%)]  Loss: 0.4307 (0.407)  Time: 0.551s,   29.03/s  (0.372s,   42.97/s)  LR: 2.500e-02  Data: 0.231 (0.051)\n",
      "Train: 10 [1150/2388 ( 48%)]  Loss: 0.5183 (0.407)  Time: 0.347s,   46.11/s  (0.373s,   42.95/s)  LR: 2.500e-02  Data: 0.027 (0.052)\n",
      "Train: 10 [1200/2388 ( 50%)]  Loss: 0.3597 (0.407)  Time: 0.498s,   32.12/s  (0.372s,   42.99/s)  LR: 2.500e-02  Data: 0.177 (0.051)\n",
      "Train: 10 [1250/2388 ( 52%)]  Loss: 0.4332 (0.407)  Time: 0.404s,   39.58/s  (0.372s,   43.00/s)  LR: 2.500e-02  Data: 0.083 (0.051)\n",
      "Train: 10 [1300/2388 ( 54%)]  Loss: 0.3910 (0.406)  Time: 0.347s,   46.13/s  (0.372s,   43.02/s)  LR: 2.500e-02  Data: 0.026 (0.051)\n",
      "Train: 10 [1350/2388 ( 57%)]  Loss: 0.4524 (0.407)  Time: 0.477s,   33.57/s  (0.372s,   43.03/s)  LR: 2.500e-02  Data: 0.154 (0.051)\n",
      "Train: 10 [1400/2388 ( 59%)]  Loss: 0.3636 (0.407)  Time: 0.349s,   45.89/s  (0.372s,   43.06/s)  LR: 2.500e-02  Data: 0.027 (0.051)\n",
      "Train: 10 [1450/2388 ( 61%)]  Loss: 0.3490 (0.406)  Time: 0.343s,   46.59/s  (0.372s,   43.04/s)  LR: 2.500e-02  Data: 0.023 (0.051)\n",
      "Train: 10 [1500/2388 ( 63%)]  Loss: 0.3971 (0.406)  Time: 0.503s,   31.80/s  (0.372s,   43.01/s)  LR: 2.500e-02  Data: 0.182 (0.051)\n",
      "Train: 10 [1550/2388 ( 65%)]  Loss: 0.3957 (0.406)  Time: 0.411s,   38.88/s  (0.372s,   43.00/s)  LR: 2.500e-02  Data: 0.091 (0.051)\n",
      "Train: 10 [1600/2388 ( 67%)]  Loss: 0.4282 (0.406)  Time: 0.343s,   46.62/s  (0.372s,   42.99/s)  LR: 2.500e-02  Data: 0.023 (0.051)\n",
      "Train: 10 [1650/2388 ( 69%)]  Loss: 0.3524 (0.406)  Time: 0.351s,   45.60/s  (0.372s,   43.00/s)  LR: 2.500e-02  Data: 0.031 (0.051)\n",
      "Train: 10 [1700/2388 ( 71%)]  Loss: 0.3963 (0.406)  Time: 0.469s,   34.08/s  (0.372s,   43.00/s)  LR: 2.500e-02  Data: 0.148 (0.051)\n",
      "Train: 10 [1750/2388 ( 73%)]  Loss: 0.4874 (0.407)  Time: 0.467s,   34.30/s  (0.372s,   43.00/s)  LR: 2.500e-02  Data: 0.146 (0.051)\n",
      "Train: 10 [1800/2388 ( 75%)]  Loss: 0.4898 (0.407)  Time: 0.340s,   47.06/s  (0.372s,   43.00/s)  LR: 2.500e-02  Data: 0.020 (0.051)\n",
      "Train: 10 [1850/2388 ( 78%)]  Loss: 0.4666 (0.407)  Time: 0.409s,   39.15/s  (0.372s,   43.01/s)  LR: 2.500e-02  Data: 0.089 (0.051)\n",
      "Train: 10 [1900/2388 ( 80%)]  Loss: 0.4364 (0.406)  Time: 0.340s,   47.01/s  (0.372s,   43.00/s)  LR: 2.500e-02  Data: 0.019 (0.051)\n",
      "Train: 10 [1950/2388 ( 82%)]  Loss: 0.3503 (0.407)  Time: 0.422s,   37.89/s  (0.372s,   43.02/s)  LR: 2.500e-02  Data: 0.094 (0.051)\n",
      "Train: 10 [2000/2388 ( 84%)]  Loss: 0.4556 (0.406)  Time: 0.339s,   47.17/s  (0.372s,   43.03/s)  LR: 2.500e-02  Data: 0.019 (0.051)\n",
      "Train: 10 [2050/2388 ( 86%)]  Loss: 0.3856 (0.406)  Time: 0.483s,   33.12/s  (0.372s,   43.03/s)  LR: 2.500e-02  Data: 0.162 (0.051)\n",
      "Train: 10 [2100/2388 ( 88%)]  Loss: 0.4230 (0.406)  Time: 0.340s,   47.02/s  (0.372s,   43.03/s)  LR: 2.500e-02  Data: 0.020 (0.051)\n",
      "Train: 10 [2150/2388 ( 90%)]  Loss: 0.3745 (0.406)  Time: 0.443s,   36.16/s  (0.372s,   43.03/s)  LR: 2.500e-02  Data: 0.121 (0.051)\n",
      "Train: 10 [2200/2388 ( 92%)]  Loss: 0.4051 (0.406)  Time: 0.342s,   46.84/s  (0.372s,   43.06/s)  LR: 2.500e-02  Data: 0.020 (0.051)\n",
      "Train: 10 [2250/2388 ( 94%)]  Loss: 0.3281 (0.406)  Time: 0.483s,   33.14/s  (0.372s,   43.06/s)  LR: 2.500e-02  Data: 0.163 (0.051)\n",
      "Train: 10 [2300/2388 ( 96%)]  Loss: 0.4379 (0.406)  Time: 0.340s,   47.04/s  (0.371s,   43.08/s)  LR: 2.500e-02  Data: 0.019 (0.051)\n",
      "Train: 10 [2350/2388 ( 98%)]  Loss: 0.3627 (0.406)  Time: 0.422s,   37.94/s  (0.371s,   43.09/s)  LR: 2.500e-02  Data: 0.099 (0.051)\n",
      "Train: 10 [2387/2388 (100%)]  Loss: 0.4307 (0.406)  Time: 0.319s,   50.14/s  (0.371s,   43.09/s)  LR: 2.500e-02  Data: 0.000 (0.050)\n",
      "Test: [   0/2388]  Time: 1.267 (1.267)  Loss:  0.3354 (0.3354)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [  50/2388]  Time: 0.086 (0.299)  Loss:  0.1786 (0.4191)  Acc@1: 93.7500 (90.0735)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 100/2388]  Time: 0.091 (0.296)  Loss:  0.3079 (0.4509)  Acc@1: 100.0000 (88.1188)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 150/2388]  Time: 0.086 (0.291)  Loss:  0.2944 (0.4299)  Acc@1: 100.0000 (88.5762)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 200/2388]  Time: 0.092 (0.292)  Loss:  0.4094 (0.4201)  Acc@1: 87.5000 (88.7127)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 250/2388]  Time: 0.086 (0.290)  Loss:  0.3550 (0.4154)  Acc@1: 87.5000 (88.6952)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 300/2388]  Time: 0.092 (0.291)  Loss:  0.2079 (0.4063)  Acc@1: 93.7500 (88.9120)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 350/2388]  Time: 0.086 (0.289)  Loss:  0.3376 (0.4011)  Acc@1: 81.2500 (88.9245)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 400/2388]  Time: 0.093 (0.290)  Loss:  0.4175 (0.3922)  Acc@1: 93.7500 (89.1054)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 450/2388]  Time: 0.092 (0.289)  Loss:  0.0980 (0.3864)  Acc@1: 100.0000 (89.2877)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 500/2388]  Time: 0.092 (0.290)  Loss:  0.6831 (0.3801)  Acc@1: 75.0000 (89.3588)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 550/2388]  Time: 0.086 (0.288)  Loss:  1.0469 (0.3843)  Acc@1: 68.7500 (89.2922)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 600/2388]  Time: 0.091 (0.288)  Loss:  0.2637 (0.3893)  Acc@1: 87.5000 (89.0079)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 650/2388]  Time: 0.091 (0.287)  Loss:  0.2683 (0.3871)  Acc@1: 100.0000 (89.0553)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 700/2388]  Time: 0.093 (0.287)  Loss:  0.6226 (0.3895)  Acc@1: 87.5000 (88.9622)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 750/2388]  Time: 0.089 (0.287)  Loss:  0.4675 (0.3925)  Acc@1: 81.2500 (88.7483)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 800/2388]  Time: 0.093 (0.287)  Loss:  0.2903 (0.3963)  Acc@1: 100.0000 (88.5768)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 850/2388]  Time: 0.086 (0.287)  Loss:  0.4976 (0.3969)  Acc@1: 75.0000 (88.5576)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 900/2388]  Time: 0.093 (0.287)  Loss:  0.0750 (0.3949)  Acc@1: 100.0000 (88.6376)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 950/2388]  Time: 0.086 (0.287)  Loss:  0.7632 (0.3932)  Acc@1: 68.7500 (88.7093)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1000/2388]  Time: 0.092 (0.287)  Loss:  0.3118 (0.3910)  Acc@1: 100.0000 (88.8299)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1050/2388]  Time: 0.086 (0.287)  Loss:  0.7993 (0.3911)  Acc@1: 75.0000 (88.7785)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1100/2388]  Time: 0.092 (0.287)  Loss:  0.3171 (0.3913)  Acc@1: 87.5000 (88.7829)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1150/2388]  Time: 0.086 (0.287)  Loss:  1.1328 (0.3909)  Acc@1: 81.2500 (88.8141)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1200/2388]  Time: 0.093 (0.288)  Loss:  0.3374 (0.3904)  Acc@1: 87.5000 (88.8634)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1250/2388]  Time: 0.086 (0.287)  Loss:  0.8921 (0.3882)  Acc@1: 75.0000 (88.9488)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1300/2388]  Time: 0.093 (0.288)  Loss:  0.3052 (0.3861)  Acc@1: 93.7500 (89.0517)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1350/2388]  Time: 0.086 (0.288)  Loss:  0.2343 (0.3843)  Acc@1: 100.0000 (89.1423)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1400/2388]  Time: 0.092 (0.288)  Loss:  0.4482 (0.3824)  Acc@1: 93.7500 (89.1997)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1450/2388]  Time: 0.086 (0.288)  Loss:  0.7026 (0.3945)  Acc@1: 68.7500 (88.5898)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1500/2388]  Time: 0.092 (0.288)  Loss:  1.4170 (0.4157)  Acc@1: 56.2500 (87.4958)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1550/2388]  Time: 0.087 (0.288)  Loss:  2.0020 (0.4306)  Acc@1: 25.0000 (86.5651)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1600/2388]  Time: 0.091 (0.288)  Loss:  1.1045 (0.4444)  Acc@1: 31.2500 (85.7472)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1650/2388]  Time: 0.085 (0.288)  Loss:  2.4316 (0.4703)  Acc@1:  0.0000 (84.5094)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1700/2388]  Time: 0.092 (0.288)  Loss:  0.6567 (0.4898)  Acc@1: 68.7500 (83.4840)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1750/2388]  Time: 0.087 (0.288)  Loss:  0.4102 (0.5025)  Acc@1: 75.0000 (82.5778)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1800/2388]  Time: 0.093 (0.288)  Loss:  0.8838 (0.5132)  Acc@1: 56.2500 (81.7705)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1850/2388]  Time: 0.092 (0.288)  Loss:  1.6250 (0.5224)  Acc@1: 37.5000 (81.0677)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1900/2388]  Time: 0.092 (0.289)  Loss:  1.8887 (0.5338)  Acc@1: 25.0000 (80.4741)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1950/2388]  Time: 0.087 (0.288)  Loss:  1.5361 (0.5458)  Acc@1: 18.7500 (79.6611)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2000/2388]  Time: 0.092 (0.288)  Loss:  0.7188 (0.5529)  Acc@1: 75.0000 (79.1011)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2050/2388]  Time: 0.086 (0.288)  Loss:  1.0820 (0.5616)  Acc@1: 43.7500 (78.4099)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2100/2388]  Time: 0.091 (0.288)  Loss:  0.2323 (0.5648)  Acc@1: 93.7500 (78.0164)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2150/2388]  Time: 0.087 (0.288)  Loss:  0.5317 (0.5711)  Acc@1: 68.7500 (77.5482)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2200/2388]  Time: 0.094 (0.288)  Loss:  0.7959 (0.5783)  Acc@1: 62.5000 (77.0360)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2250/2388]  Time: 0.086 (0.288)  Loss:  0.6538 (0.5878)  Acc@1: 62.5000 (76.3799)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2300/2388]  Time: 0.092 (0.288)  Loss:  0.7715 (0.6006)  Acc@1: 56.2500 (75.7361)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2350/2388]  Time: 0.092 (0.288)  Loss:  1.1133 (0.6058)  Acc@1: 50.0000 (75.3988)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2388/2388]  Time: 0.014 (0.288)  Loss:  1.0928 (0.6079)  Acc@1: 50.0000 (75.2944)  Acc@5: 100.0000 (100.0000)\n",
      "Current checkpoints:\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-10.pth.tar', 75.29442554305156)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-9.pth.tar', 72.60926459042136)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-8.pth.tar', 72.55692227165663)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-6.pth.tar', 72.2350170112536)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-7.pth.tar', 71.54148128762104)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-5.pth.tar', 68.68359068306727)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-4.pth.tar', 66.8803978016226)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-2.pth.tar', 66.4224025124313)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-3.pth.tar', 65.83355142632819)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-1.pth.tar', 60.52865741952368)\n",
      "\n",
      "Train: 11 [   0/2388 (  0%)]  Loss: 0.3719 (0.372)  Time: 2.019s,    7.93/s  (2.019s,    7.93/s)  LR: 2.109e-02  Data: 1.698 (1.698)\n",
      "Train: 11 [  50/2388 (  2%)]  Loss: 0.4332 (0.400)  Time: 0.344s,   46.55/s  (0.402s,   39.79/s)  LR: 2.109e-02  Data: 0.022 (0.081)\n",
      "Train: 11 [ 100/2388 (  4%)]  Loss: 0.3863 (0.399)  Time: 0.413s,   38.72/s  (0.383s,   41.74/s)  LR: 2.109e-02  Data: 0.092 (0.062)\n",
      "Train: 11 [ 150/2388 (  6%)]  Loss: 0.4392 (0.401)  Time: 0.351s,   45.52/s  (0.376s,   42.59/s)  LR: 2.109e-02  Data: 0.029 (0.055)\n",
      "Train: 11 [ 200/2388 (  8%)]  Loss: 0.3988 (0.400)  Time: 0.405s,   39.49/s  (0.375s,   42.70/s)  LR: 2.109e-02  Data: 0.084 (0.054)\n",
      "Train: 11 [ 250/2388 ( 10%)]  Loss: 0.4098 (0.401)  Time: 0.353s,   45.33/s  (0.372s,   42.96/s)  LR: 2.109e-02  Data: 0.033 (0.052)\n",
      "Train: 11 [ 300/2388 ( 13%)]  Loss: 0.4227 (0.401)  Time: 0.470s,   34.01/s  (0.373s,   42.87/s)  LR: 2.109e-02  Data: 0.150 (0.052)\n",
      "Train: 11 [ 350/2388 ( 15%)]  Loss: 0.4758 (0.401)  Time: 0.353s,   45.36/s  (0.372s,   42.98/s)  LR: 2.109e-02  Data: 0.031 (0.051)\n",
      "Train: 11 [ 400/2388 ( 17%)]  Loss: 0.4345 (0.400)  Time: 0.356s,   45.00/s  (0.372s,   42.99/s)  LR: 2.109e-02  Data: 0.035 (0.051)\n",
      "Train: 11 [ 450/2388 ( 19%)]  Loss: 0.4754 (0.401)  Time: 0.633s,   25.27/s  (0.373s,   42.90/s)  LR: 2.109e-02  Data: 0.309 (0.052)\n",
      "Train: 11 [ 500/2388 ( 21%)]  Loss: 0.4609 (0.400)  Time: 0.347s,   46.07/s  (0.372s,   42.99/s)  LR: 2.109e-02  Data: 0.027 (0.051)\n",
      "Train: 11 [ 550/2388 ( 23%)]  Loss: 0.3587 (0.400)  Time: 0.443s,   36.09/s  (0.372s,   43.00/s)  LR: 2.109e-02  Data: 0.123 (0.051)\n",
      "Train: 11 [ 600/2388 ( 25%)]  Loss: 0.4231 (0.401)  Time: 0.373s,   42.89/s  (0.372s,   43.00/s)  LR: 2.109e-02  Data: 0.048 (0.051)\n",
      "Train: 11 [ 650/2388 ( 27%)]  Loss: 0.4665 (0.401)  Time: 0.442s,   36.18/s  (0.372s,   42.97/s)  LR: 2.109e-02  Data: 0.121 (0.051)\n",
      "Train: 11 [ 700/2388 ( 29%)]  Loss: 0.4513 (0.401)  Time: 0.342s,   46.77/s  (0.372s,   43.05/s)  LR: 2.109e-02  Data: 0.022 (0.051)\n",
      "Train: 11 [ 750/2388 ( 31%)]  Loss: 0.2842 (0.401)  Time: 0.396s,   40.38/s  (0.372s,   43.04/s)  LR: 2.109e-02  Data: 0.075 (0.051)\n",
      "Train: 11 [ 800/2388 ( 34%)]  Loss: 0.4191 (0.401)  Time: 0.354s,   45.23/s  (0.371s,   43.08/s)  LR: 2.109e-02  Data: 0.032 (0.051)\n",
      "Train: 11 [ 850/2388 ( 36%)]  Loss: 0.3907 (0.401)  Time: 0.519s,   30.82/s  (0.371s,   43.08/s)  LR: 2.109e-02  Data: 0.198 (0.051)\n",
      "Train: 11 [ 900/2388 ( 38%)]  Loss: 0.3605 (0.401)  Time: 0.339s,   47.23/s  (0.371s,   43.11/s)  LR: 2.109e-02  Data: 0.019 (0.050)\n",
      "Train: 11 [ 950/2388 ( 40%)]  Loss: 0.3647 (0.401)  Time: 0.398s,   40.24/s  (0.371s,   43.13/s)  LR: 2.109e-02  Data: 0.076 (0.050)\n",
      "Train: 11 [1000/2388 ( 42%)]  Loss: 0.3672 (0.402)  Time: 0.338s,   47.27/s  (0.371s,   43.10/s)  LR: 2.109e-02  Data: 0.018 (0.050)\n",
      "Train: 11 [1050/2388 ( 44%)]  Loss: 0.4229 (0.402)  Time: 0.444s,   36.07/s  (0.371s,   43.11/s)  LR: 2.109e-02  Data: 0.122 (0.050)\n",
      "Train: 11 [1100/2388 ( 46%)]  Loss: 0.4405 (0.402)  Time: 0.341s,   46.87/s  (0.371s,   43.11/s)  LR: 2.109e-02  Data: 0.020 (0.050)\n",
      "Train: 11 [1150/2388 ( 48%)]  Loss: 0.3807 (0.402)  Time: 0.415s,   38.56/s  (0.371s,   43.13/s)  LR: 2.109e-02  Data: 0.093 (0.050)\n",
      "Train: 11 [1200/2388 ( 50%)]  Loss: 0.4431 (0.402)  Time: 0.339s,   47.21/s  (0.371s,   43.16/s)  LR: 2.109e-02  Data: 0.019 (0.050)\n",
      "Train: 11 [1250/2388 ( 52%)]  Loss: 0.4298 (0.402)  Time: 0.496s,   32.25/s  (0.371s,   43.17/s)  LR: 2.109e-02  Data: 0.174 (0.050)\n",
      "Train: 11 [1300/2388 ( 54%)]  Loss: 0.3533 (0.402)  Time: 0.339s,   47.14/s  (0.370s,   43.20/s)  LR: 2.109e-02  Data: 0.020 (0.050)\n",
      "Train: 11 [1350/2388 ( 57%)]  Loss: 0.4265 (0.402)  Time: 0.431s,   37.08/s  (0.370s,   43.20/s)  LR: 2.109e-02  Data: 0.112 (0.050)\n",
      "Train: 11 [1400/2388 ( 59%)]  Loss: 0.3900 (0.402)  Time: 0.340s,   47.12/s  (0.370s,   43.22/s)  LR: 2.109e-02  Data: 0.019 (0.049)\n",
      "Train: 11 [1450/2388 ( 61%)]  Loss: 0.4196 (0.402)  Time: 0.446s,   35.87/s  (0.370s,   43.21/s)  LR: 2.109e-02  Data: 0.125 (0.049)\n",
      "Train: 11 [1500/2388 ( 63%)]  Loss: 0.4072 (0.402)  Time: 0.339s,   47.22/s  (0.370s,   43.23/s)  LR: 2.109e-02  Data: 0.019 (0.049)\n",
      "Train: 11 [1550/2388 ( 65%)]  Loss: 0.4047 (0.402)  Time: 0.429s,   37.26/s  (0.370s,   43.22/s)  LR: 2.109e-02  Data: 0.109 (0.049)\n",
      "Train: 11 [1600/2388 ( 67%)]  Loss: 0.4571 (0.402)  Time: 0.341s,   46.94/s  (0.370s,   43.21/s)  LR: 2.109e-02  Data: 0.021 (0.049)\n",
      "Train: 11 [1650/2388 ( 69%)]  Loss: 0.4056 (0.402)  Time: 0.499s,   32.08/s  (0.371s,   43.18/s)  LR: 2.109e-02  Data: 0.177 (0.050)\n",
      "Train: 11 [1700/2388 ( 71%)]  Loss: 0.4035 (0.402)  Time: 0.339s,   47.16/s  (0.370s,   43.21/s)  LR: 2.109e-02  Data: 0.018 (0.049)\n",
      "Train: 11 [1750/2388 ( 73%)]  Loss: 0.3911 (0.402)  Time: 0.415s,   38.57/s  (0.370s,   43.21/s)  LR: 2.109e-02  Data: 0.092 (0.049)\n",
      "Train: 11 [1800/2388 ( 75%)]  Loss: 0.3916 (0.402)  Time: 0.340s,   47.04/s  (0.370s,   43.20/s)  LR: 2.109e-02  Data: 0.020 (0.050)\n",
      "Train: 11 [1850/2388 ( 78%)]  Loss: 0.3721 (0.402)  Time: 0.477s,   33.55/s  (0.370s,   43.19/s)  LR: 2.109e-02  Data: 0.157 (0.050)\n",
      "Train: 11 [1900/2388 ( 80%)]  Loss: 0.4667 (0.402)  Time: 0.341s,   46.92/s  (0.370s,   43.20/s)  LR: 2.109e-02  Data: 0.020 (0.050)\n",
      "Train: 11 [1950/2388 ( 82%)]  Loss: 0.3591 (0.403)  Time: 0.494s,   32.40/s  (0.370s,   43.20/s)  LR: 2.109e-02  Data: 0.173 (0.049)\n",
      "Train: 11 [2000/2388 ( 84%)]  Loss: 0.3070 (0.402)  Time: 0.340s,   46.99/s  (0.370s,   43.21/s)  LR: 2.109e-02  Data: 0.020 (0.049)\n",
      "Train: 11 [2050/2388 ( 86%)]  Loss: 0.3657 (0.403)  Time: 0.477s,   33.54/s  (0.370s,   43.21/s)  LR: 2.109e-02  Data: 0.156 (0.049)\n",
      "Train: 11 [2100/2388 ( 88%)]  Loss: 0.3849 (0.403)  Time: 0.341s,   46.88/s  (0.370s,   43.22/s)  LR: 2.109e-02  Data: 0.020 (0.049)\n",
      "Train: 11 [2150/2388 ( 90%)]  Loss: 0.4547 (0.403)  Time: 0.390s,   41.04/s  (0.370s,   43.24/s)  LR: 2.109e-02  Data: 0.070 (0.049)\n",
      "Train: 11 [2200/2388 ( 92%)]  Loss: 0.4386 (0.403)  Time: 0.339s,   47.14/s  (0.370s,   43.25/s)  LR: 2.109e-02  Data: 0.019 (0.049)\n",
      "Train: 11 [2250/2388 ( 94%)]  Loss: 0.4382 (0.403)  Time: 0.533s,   30.03/s  (0.370s,   43.25/s)  LR: 2.109e-02  Data: 0.212 (0.049)\n",
      "Train: 11 [2300/2388 ( 96%)]  Loss: 0.3437 (0.403)  Time: 0.339s,   47.21/s  (0.370s,   43.26/s)  LR: 2.109e-02  Data: 0.019 (0.049)\n",
      "Train: 11 [2350/2388 ( 98%)]  Loss: 0.4296 (0.403)  Time: 0.457s,   35.03/s  (0.370s,   43.25/s)  LR: 2.109e-02  Data: 0.136 (0.049)\n",
      "Train: 11 [2387/2388 (100%)]  Loss: 0.4398 (0.403)  Time: 0.319s,   50.23/s  (0.370s,   43.24/s)  LR: 2.109e-02  Data: 0.000 (0.049)\n",
      "Test: [   0/2388]  Time: 1.299 (1.299)  Loss:  0.3286 (0.3286)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [  50/2388]  Time: 0.090 (0.298)  Loss:  0.2020 (0.3834)  Acc@1: 87.5000 (90.3186)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 100/2388]  Time: 0.087 (0.295)  Loss:  0.3098 (0.3973)  Acc@1: 100.0000 (90.2847)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 150/2388]  Time: 0.092 (0.289)  Loss:  0.3569 (0.4046)  Acc@1: 100.0000 (89.7351)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 200/2388]  Time: 0.091 (0.291)  Loss:  0.2961 (0.4098)  Acc@1: 81.2500 (89.1169)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 250/2388]  Time: 0.101 (0.289)  Loss:  0.3037 (0.4142)  Acc@1: 93.7500 (88.7699)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 300/2388]  Time: 0.085 (0.289)  Loss:  0.1859 (0.4083)  Acc@1: 93.7500 (88.6420)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 350/2388]  Time: 0.156 (0.287)  Loss:  0.3254 (0.4053)  Acc@1: 87.5000 (88.6574)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 400/2388]  Time: 0.092 (0.288)  Loss:  0.2003 (0.3975)  Acc@1: 100.0000 (89.0898)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 450/2388]  Time: 0.286 (0.288)  Loss:  0.2168 (0.3956)  Acc@1: 93.7500 (89.0798)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 500/2388]  Time: 0.086 (0.288)  Loss:  0.7490 (0.3883)  Acc@1: 75.0000 (89.2964)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 550/2388]  Time: 0.643 (0.288)  Loss:  0.5859 (0.3922)  Acc@1: 75.0000 (89.1221)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 600/2388]  Time: 0.086 (0.288)  Loss:  0.4231 (0.4023)  Acc@1: 81.2500 (88.5087)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 650/2388]  Time: 0.808 (0.288)  Loss:  0.4717 (0.4045)  Acc@1: 87.5000 (88.4409)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 700/2388]  Time: 0.085 (0.288)  Loss:  0.8286 (0.4097)  Acc@1: 87.5000 (88.1419)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 750/2388]  Time: 0.678 (0.288)  Loss:  0.4834 (0.4129)  Acc@1: 75.0000 (87.8662)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 800/2388]  Time: 0.091 (0.288)  Loss:  0.3633 (0.4155)  Acc@1: 87.5000 (87.7263)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 850/2388]  Time: 0.868 (0.288)  Loss:  0.5684 (0.4163)  Acc@1: 81.2500 (87.6102)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 900/2388]  Time: 0.086 (0.288)  Loss:  0.1772 (0.4146)  Acc@1: 93.7500 (87.6387)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 950/2388]  Time: 0.861 (0.288)  Loss:  0.3523 (0.4140)  Acc@1: 87.5000 (87.6972)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1000/2388]  Time: 0.085 (0.288)  Loss:  0.3960 (0.4130)  Acc@1: 93.7500 (87.8122)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1050/2388]  Time: 0.916 (0.289)  Loss:  0.3867 (0.4138)  Acc@1: 93.7500 (87.7438)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1100/2388]  Time: 0.085 (0.288)  Loss:  0.3547 (0.4157)  Acc@1: 93.7500 (87.5908)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1150/2388]  Time: 0.928 (0.289)  Loss:  0.9038 (0.4158)  Acc@1: 93.7500 (87.5652)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1200/2388]  Time: 0.085 (0.288)  Loss:  0.2542 (0.4159)  Acc@1: 93.7500 (87.4532)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1250/2388]  Time: 0.862 (0.289)  Loss:  0.6221 (0.4155)  Acc@1: 81.2500 (87.4750)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1300/2388]  Time: 0.086 (0.288)  Loss:  0.2812 (0.4157)  Acc@1: 93.7500 (87.4183)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1350/2388]  Time: 0.886 (0.288)  Loss:  0.3450 (0.4146)  Acc@1: 93.7500 (87.5000)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1400/2388]  Time: 0.086 (0.288)  Loss:  0.3645 (0.4133)  Acc@1: 87.5000 (87.5758)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1450/2388]  Time: 0.906 (0.288)  Loss:  0.8955 (0.4347)  Acc@1: 68.7500 (86.5438)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1500/2388]  Time: 0.086 (0.288)  Loss:  1.8799 (0.4642)  Acc@1:  6.2500 (85.0641)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1550/2388]  Time: 0.864 (0.288)  Loss:  1.6768 (0.4841)  Acc@1: 37.5000 (84.0748)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1600/2388]  Time: 0.085 (0.288)  Loss:  1.1113 (0.5023)  Acc@1: 50.0000 (83.0965)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1650/2388]  Time: 0.872 (0.288)  Loss:  1.6201 (0.5280)  Acc@1: 25.0000 (81.8141)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1700/2388]  Time: 0.095 (0.287)  Loss:  0.4133 (0.5363)  Acc@1: 81.2500 (81.1177)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1750/2388]  Time: 0.859 (0.288)  Loss:  0.4382 (0.5425)  Acc@1: 75.0000 (80.4576)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1800/2388]  Time: 0.086 (0.288)  Loss:  0.6064 (0.5480)  Acc@1: 68.7500 (79.8341)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1850/2388]  Time: 0.868 (0.288)  Loss:  1.0547 (0.5521)  Acc@1: 31.2500 (79.2848)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1900/2388]  Time: 0.085 (0.288)  Loss:  1.2402 (0.5565)  Acc@1: 31.2500 (78.8138)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1950/2388]  Time: 0.859 (0.288)  Loss:  0.9004 (0.5604)  Acc@1: 37.5000 (78.3605)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2000/2388]  Time: 0.086 (0.287)  Loss:  0.5747 (0.5619)  Acc@1: 75.0000 (78.0578)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2050/2388]  Time: 0.922 (0.287)  Loss:  0.8853 (0.5656)  Acc@1: 43.7500 (77.5567)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2100/2388]  Time: 0.085 (0.287)  Loss:  0.2971 (0.5657)  Acc@1: 93.7500 (77.3382)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2150/2388]  Time: 0.831 (0.287)  Loss:  0.4563 (0.5678)  Acc@1: 81.2500 (76.9874)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2200/2388]  Time: 0.086 (0.287)  Loss:  0.7866 (0.5706)  Acc@1: 50.0000 (76.6839)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2250/2388]  Time: 0.851 (0.288)  Loss:  0.8345 (0.5750)  Acc@1: 50.0000 (76.2578)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2300/2388]  Time: 0.086 (0.288)  Loss:  0.3188 (0.5801)  Acc@1: 87.5000 (75.8801)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2350/2388]  Time: 0.837 (0.288)  Loss:  1.0332 (0.5832)  Acc@1: 62.5000 (75.5875)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2388/2388]  Time: 0.014 (0.287)  Loss:  0.9463 (0.5855)  Acc@1: 50.0000 (75.4802)  Acc@5: 100.0000 (100.0000)\n",
      "Current checkpoints:\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-11.pth.tar', 75.48024077466631)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-10.pth.tar', 75.29442554305156)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-9.pth.tar', 72.60926459042136)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-8.pth.tar', 72.55692227165663)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-6.pth.tar', 72.2350170112536)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-7.pth.tar', 71.54148128762104)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-5.pth.tar', 68.68359068306727)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-4.pth.tar', 66.8803978016226)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-2.pth.tar', 66.4224025124313)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-3.pth.tar', 65.83355142632819)\n",
      "\n",
      "Train: 12 [   0/2388 (  0%)]  Loss: 0.3811 (0.381)  Time: 1.882s,    8.50/s  (1.882s,    8.50/s)  LR: 1.728e-02  Data: 1.561 (1.561)\n",
      "Train: 12 [  50/2388 (  2%)]  Loss: 0.3668 (0.406)  Time: 0.341s,   46.91/s  (0.406s,   39.39/s)  LR: 1.728e-02  Data: 0.020 (0.085)\n",
      "Train: 12 [ 100/2388 (  4%)]  Loss: 0.2827 (0.397)  Time: 0.340s,   47.07/s  (0.385s,   41.52/s)  LR: 1.728e-02  Data: 0.019 (0.065)\n",
      "Train: 12 [ 150/2388 (  6%)]  Loss: 0.3574 (0.397)  Time: 0.343s,   46.64/s  (0.378s,   42.33/s)  LR: 1.728e-02  Data: 0.023 (0.057)\n",
      "Train: 12 [ 200/2388 (  8%)]  Loss: 0.4804 (0.399)  Time: 0.466s,   34.33/s  (0.376s,   42.58/s)  LR: 1.728e-02  Data: 0.145 (0.055)\n",
      "Train: 12 [ 250/2388 ( 10%)]  Loss: 0.3958 (0.401)  Time: 0.349s,   45.89/s  (0.374s,   42.78/s)  LR: 1.728e-02  Data: 0.027 (0.053)\n",
      "Train: 12 [ 300/2388 ( 13%)]  Loss: 0.4237 (0.400)  Time: 0.428s,   37.37/s  (0.373s,   42.86/s)  LR: 1.728e-02  Data: 0.108 (0.052)\n",
      "Train: 12 [ 350/2388 ( 15%)]  Loss: 0.4012 (0.399)  Time: 0.345s,   46.36/s  (0.372s,   42.96/s)  LR: 1.728e-02  Data: 0.022 (0.052)\n",
      "Train: 12 [ 400/2388 ( 17%)]  Loss: 0.4426 (0.399)  Time: 0.453s,   35.29/s  (0.372s,   42.96/s)  LR: 1.728e-02  Data: 0.131 (0.052)\n",
      "Train: 12 [ 450/2388 ( 19%)]  Loss: 0.4115 (0.399)  Time: 0.342s,   46.74/s  (0.372s,   42.95/s)  LR: 1.728e-02  Data: 0.022 (0.052)\n",
      "Train: 12 [ 500/2388 ( 21%)]  Loss: 0.4330 (0.399)  Time: 0.342s,   46.85/s  (0.372s,   43.01/s)  LR: 1.728e-02  Data: 0.021 (0.051)\n",
      "Train: 12 [ 550/2388 ( 23%)]  Loss: 0.3576 (0.400)  Time: 0.484s,   33.03/s  (0.371s,   43.09/s)  LR: 1.728e-02  Data: 0.164 (0.050)\n",
      "Train: 12 [ 600/2388 ( 25%)]  Loss: 0.4759 (0.399)  Time: 0.342s,   46.84/s  (0.371s,   43.12/s)  LR: 1.728e-02  Data: 0.021 (0.050)\n",
      "Train: 12 [ 650/2388 ( 27%)]  Loss: 0.4482 (0.400)  Time: 0.342s,   46.81/s  (0.371s,   43.13/s)  LR: 1.728e-02  Data: 0.022 (0.050)\n",
      "Train: 12 [ 700/2388 ( 29%)]  Loss: 0.5282 (0.400)  Time: 0.343s,   46.67/s  (0.371s,   43.08/s)  LR: 1.728e-02  Data: 0.022 (0.050)\n",
      "Train: 12 [ 750/2388 ( 31%)]  Loss: 0.3856 (0.400)  Time: 0.452s,   35.42/s  (0.372s,   43.05/s)  LR: 1.728e-02  Data: 0.130 (0.051)\n",
      "Train: 12 [ 800/2388 ( 34%)]  Loss: 0.3737 (0.400)  Time: 0.347s,   46.12/s  (0.371s,   43.11/s)  LR: 1.728e-02  Data: 0.026 (0.050)\n",
      "Train: 12 [ 850/2388 ( 36%)]  Loss: 0.3643 (0.400)  Time: 0.397s,   40.27/s  (0.371s,   43.08/s)  LR: 1.728e-02  Data: 0.076 (0.051)\n",
      "Train: 12 [ 900/2388 ( 38%)]  Loss: 0.3510 (0.400)  Time: 0.346s,   46.29/s  (0.371s,   43.09/s)  LR: 1.728e-02  Data: 0.024 (0.050)\n",
      "Train: 12 [ 950/2388 ( 40%)]  Loss: 0.4183 (0.400)  Time: 0.445s,   35.94/s  (0.371s,   43.09/s)  LR: 1.728e-02  Data: 0.124 (0.050)\n",
      "Train: 12 [1000/2388 ( 42%)]  Loss: 0.3727 (0.400)  Time: 0.340s,   47.00/s  (0.372s,   43.03/s)  LR: 1.728e-02  Data: 0.020 (0.051)\n",
      "Train: 12 [1050/2388 ( 44%)]  Loss: 0.3856 (0.399)  Time: 0.458s,   34.95/s  (0.372s,   43.03/s)  LR: 1.728e-02  Data: 0.136 (0.051)\n",
      "Train: 12 [1100/2388 ( 46%)]  Loss: 0.3991 (0.399)  Time: 0.338s,   47.28/s  (0.371s,   43.07/s)  LR: 1.728e-02  Data: 0.018 (0.051)\n",
      "Train: 12 [1150/2388 ( 48%)]  Loss: 0.3541 (0.399)  Time: 0.444s,   36.04/s  (0.372s,   43.06/s)  LR: 1.728e-02  Data: 0.124 (0.051)\n",
      "Train: 12 [1200/2388 ( 50%)]  Loss: 0.3786 (0.399)  Time: 0.338s,   47.34/s  (0.372s,   43.05/s)  LR: 1.728e-02  Data: 0.018 (0.051)\n",
      "Train: 12 [1250/2388 ( 52%)]  Loss: 0.4924 (0.399)  Time: 0.457s,   34.98/s  (0.372s,   43.05/s)  LR: 1.728e-02  Data: 0.131 (0.051)\n",
      "Train: 12 [1300/2388 ( 54%)]  Loss: 0.3704 (0.399)  Time: 0.342s,   46.74/s  (0.372s,   43.06/s)  LR: 1.728e-02  Data: 0.022 (0.051)\n",
      "Train: 12 [1350/2388 ( 57%)]  Loss: 0.3117 (0.398)  Time: 0.538s,   29.76/s  (0.372s,   43.04/s)  LR: 1.728e-02  Data: 0.216 (0.051)\n",
      "Train: 12 [1400/2388 ( 59%)]  Loss: 0.2639 (0.398)  Time: 0.350s,   45.68/s  (0.372s,   43.07/s)  LR: 1.728e-02  Data: 0.028 (0.051)\n",
      "Train: 12 [1450/2388 ( 61%)]  Loss: 0.3321 (0.398)  Time: 0.521s,   30.73/s  (0.372s,   43.04/s)  LR: 1.728e-02  Data: 0.199 (0.051)\n",
      "Train: 12 [1500/2388 ( 63%)]  Loss: 0.3903 (0.398)  Time: 0.349s,   45.83/s  (0.372s,   43.03/s)  LR: 1.728e-02  Data: 0.029 (0.051)\n",
      "Train: 12 [1550/2388 ( 65%)]  Loss: 0.3966 (0.398)  Time: 0.450s,   35.55/s  (0.372s,   43.03/s)  LR: 1.728e-02  Data: 0.128 (0.051)\n",
      "Train: 12 [1600/2388 ( 67%)]  Loss: 0.3520 (0.398)  Time: 0.343s,   46.68/s  (0.372s,   43.06/s)  LR: 1.728e-02  Data: 0.022 (0.051)\n",
      "Train: 12 [1650/2388 ( 69%)]  Loss: 0.4362 (0.398)  Time: 0.510s,   31.37/s  (0.371s,   43.07/s)  LR: 1.728e-02  Data: 0.189 (0.051)\n",
      "Train: 12 [1700/2388 ( 71%)]  Loss: 0.4775 (0.398)  Time: 0.344s,   46.52/s  (0.372s,   43.07/s)  LR: 1.728e-02  Data: 0.023 (0.051)\n",
      "Train: 12 [1750/2388 ( 73%)]  Loss: 0.3427 (0.398)  Time: 0.462s,   34.63/s  (0.372s,   43.06/s)  LR: 1.728e-02  Data: 0.140 (0.051)\n",
      "Train: 12 [1800/2388 ( 75%)]  Loss: 0.3822 (0.398)  Time: 0.341s,   46.87/s  (0.371s,   43.07/s)  LR: 1.728e-02  Data: 0.019 (0.051)\n",
      "Train: 12 [1850/2388 ( 78%)]  Loss: 0.3531 (0.398)  Time: 0.432s,   37.07/s  (0.372s,   43.05/s)  LR: 1.728e-02  Data: 0.110 (0.051)\n",
      "Train: 12 [1900/2388 ( 80%)]  Loss: 0.3306 (0.398)  Time: 0.350s,   45.66/s  (0.372s,   43.05/s)  LR: 1.728e-02  Data: 0.029 (0.051)\n",
      "Train: 12 [1950/2388 ( 82%)]  Loss: 0.4496 (0.398)  Time: 0.500s,   31.98/s  (0.372s,   43.06/s)  LR: 1.728e-02  Data: 0.179 (0.051)\n",
      "Train: 12 [2000/2388 ( 84%)]  Loss: 0.3925 (0.398)  Time: 0.342s,   46.75/s  (0.371s,   43.07/s)  LR: 1.728e-02  Data: 0.021 (0.051)\n",
      "Train: 12 [2050/2388 ( 86%)]  Loss: 0.4706 (0.398)  Time: 0.505s,   31.68/s  (0.372s,   43.06/s)  LR: 1.728e-02  Data: 0.185 (0.051)\n",
      "Train: 12 [2100/2388 ( 88%)]  Loss: 0.3662 (0.398)  Time: 0.342s,   46.84/s  (0.372s,   43.05/s)  LR: 1.728e-02  Data: 0.021 (0.051)\n",
      "Train: 12 [2150/2388 ( 90%)]  Loss: 0.3263 (0.398)  Time: 0.359s,   44.62/s  (0.371s,   43.07/s)  LR: 1.728e-02  Data: 0.037 (0.051)\n",
      "Train: 12 [2200/2388 ( 92%)]  Loss: 0.3507 (0.398)  Time: 0.340s,   47.03/s  (0.371s,   43.07/s)  LR: 1.728e-02  Data: 0.019 (0.051)\n",
      "Train: 12 [2250/2388 ( 94%)]  Loss: 0.4562 (0.398)  Time: 0.602s,   26.58/s  (0.372s,   43.05/s)  LR: 1.728e-02  Data: 0.281 (0.051)\n",
      "Train: 12 [2300/2388 ( 96%)]  Loss: 0.4049 (0.399)  Time: 0.339s,   47.19/s  (0.372s,   43.03/s)  LR: 1.728e-02  Data: 0.019 (0.051)\n",
      "Train: 12 [2350/2388 ( 98%)]  Loss: 0.4085 (0.399)  Time: 0.444s,   36.05/s  (0.372s,   43.04/s)  LR: 1.728e-02  Data: 0.124 (0.051)\n",
      "Train: 12 [2387/2388 (100%)]  Loss: 0.4592 (0.399)  Time: 0.319s,   50.11/s  (0.372s,   43.03/s)  LR: 1.728e-02  Data: 0.000 (0.051)\n",
      "Test: [   0/2388]  Time: 1.227 (1.227)  Loss:  0.2627 (0.2627)  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [  50/2388]  Time: 0.090 (0.293)  Loss:  0.2876 (0.3562)  Acc@1: 87.5000 (89.2157)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 100/2388]  Time: 0.088 (0.293)  Loss:  0.3806 (0.3688)  Acc@1: 87.5000 (89.2327)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 150/2388]  Time: 0.090 (0.287)  Loss:  0.2374 (0.3670)  Acc@1: 100.0000 (88.9901)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 200/2388]  Time: 0.086 (0.287)  Loss:  0.4783 (0.3665)  Acc@1: 68.7500 (88.5261)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 250/2388]  Time: 0.091 (0.287)  Loss:  0.4006 (0.3695)  Acc@1: 93.7500 (87.9233)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 300/2388]  Time: 0.088 (0.285)  Loss:  0.1680 (0.3749)  Acc@1: 93.7500 (87.6038)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 350/2388]  Time: 0.092 (0.287)  Loss:  0.3735 (0.3785)  Acc@1: 81.2500 (87.4822)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 400/2388]  Time: 0.087 (0.287)  Loss:  0.4482 (0.3720)  Acc@1: 93.7500 (87.8273)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 450/2388]  Time: 0.091 (0.288)  Loss:  0.1053 (0.3728)  Acc@1: 93.7500 (87.8049)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 500/2388]  Time: 0.088 (0.287)  Loss:  0.7139 (0.3680)  Acc@1: 68.7500 (88.0240)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 550/2388]  Time: 0.092 (0.287)  Loss:  0.7734 (0.3725)  Acc@1: 75.0000 (87.9651)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 600/2388]  Time: 0.094 (0.286)  Loss:  0.3540 (0.3786)  Acc@1: 87.5000 (87.5936)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 650/2388]  Time: 0.091 (0.286)  Loss:  0.2778 (0.3767)  Acc@1: 93.7500 (87.6632)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 700/2388]  Time: 0.088 (0.285)  Loss:  0.6646 (0.3794)  Acc@1: 68.7500 (87.5624)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 750/2388]  Time: 0.091 (0.286)  Loss:  0.5796 (0.3834)  Acc@1: 81.2500 (87.3752)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 800/2388]  Time: 0.088 (0.285)  Loss:  0.2434 (0.3873)  Acc@1: 93.7500 (87.2113)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 850/2388]  Time: 0.092 (0.286)  Loss:  0.3997 (0.3871)  Acc@1: 81.2500 (87.1842)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 900/2388]  Time: 0.088 (0.286)  Loss:  0.0987 (0.3850)  Acc@1: 100.0000 (87.2225)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 950/2388]  Time: 0.092 (0.286)  Loss:  0.6514 (0.3856)  Acc@1: 68.7500 (87.1648)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1000/2388]  Time: 0.088 (0.285)  Loss:  0.2467 (0.3836)  Acc@1: 93.7500 (87.2690)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1050/2388]  Time: 0.093 (0.286)  Loss:  0.5215 (0.3839)  Acc@1: 87.5000 (87.2859)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1100/2388]  Time: 0.088 (0.286)  Loss:  0.3674 (0.3840)  Acc@1: 93.7500 (87.2616)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1150/2388]  Time: 0.093 (0.286)  Loss:  0.6753 (0.3846)  Acc@1: 87.5000 (87.2774)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1200/2388]  Time: 0.087 (0.286)  Loss:  0.2795 (0.3848)  Acc@1: 87.5000 (87.2346)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1250/2388]  Time: 0.093 (0.286)  Loss:  1.0312 (0.3827)  Acc@1: 68.7500 (87.3801)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1300/2388]  Time: 0.088 (0.286)  Loss:  0.1342 (0.3803)  Acc@1: 100.0000 (87.4520)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1350/2388]  Time: 0.093 (0.287)  Loss:  0.2220 (0.3768)  Acc@1: 93.7500 (87.6203)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1400/2388]  Time: 0.088 (0.286)  Loss:  0.3652 (0.3742)  Acc@1: 87.5000 (87.7587)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1450/2388]  Time: 0.090 (0.287)  Loss:  0.8223 (0.3925)  Acc@1: 68.7500 (86.9573)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1500/2388]  Time: 0.088 (0.287)  Loss:  1.6348 (0.4164)  Acc@1: 31.2500 (85.8844)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1550/2388]  Time: 0.093 (0.287)  Loss:  1.7539 (0.4378)  Acc@1: 25.0000 (84.9250)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1600/2388]  Time: 0.110 (0.286)  Loss:  1.2363 (0.4545)  Acc@1: 56.2500 (84.2169)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1650/2388]  Time: 0.092 (0.287)  Loss:  2.7539 (0.4858)  Acc@1:  0.0000 (83.0103)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1700/2388]  Time: 0.087 (0.287)  Loss:  0.4380 (0.4951)  Acc@1: 87.5000 (82.3707)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1750/2388]  Time: 0.093 (0.287)  Loss:  0.4109 (0.5045)  Acc@1: 81.2500 (81.6783)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1800/2388]  Time: 0.086 (0.287)  Loss:  0.2759 (0.5112)  Acc@1: 87.5000 (81.1251)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1850/2388]  Time: 0.092 (0.287)  Loss:  0.7651 (0.5150)  Acc@1: 68.7500 (80.7266)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1900/2388]  Time: 0.087 (0.287)  Loss:  1.3555 (0.5209)  Acc@1: 25.0000 (80.3097)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1950/2388]  Time: 0.095 (0.287)  Loss:  1.2617 (0.5273)  Acc@1: 31.2500 (79.7924)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2000/2388]  Time: 0.088 (0.287)  Loss:  0.4270 (0.5311)  Acc@1: 81.2500 (79.4103)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2050/2388]  Time: 0.092 (0.287)  Loss:  0.9048 (0.5364)  Acc@1: 43.7500 (78.9402)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2100/2388]  Time: 0.088 (0.287)  Loss:  0.2463 (0.5367)  Acc@1: 93.7500 (78.7690)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2150/2388]  Time: 0.093 (0.287)  Loss:  0.3076 (0.5384)  Acc@1: 93.7500 (78.5216)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2200/2388]  Time: 0.088 (0.287)  Loss:  0.6650 (0.5405)  Acc@1: 56.2500 (78.2854)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2250/2388]  Time: 0.092 (0.287)  Loss:  0.5903 (0.5460)  Acc@1: 81.2500 (77.8765)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2300/2388]  Time: 0.087 (0.287)  Loss:  0.6577 (0.5539)  Acc@1: 62.5000 (77.4337)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2350/2388]  Time: 0.092 (0.288)  Loss:  0.5815 (0.5573)  Acc@1: 68.7500 (77.1852)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2388/2388]  Time: 0.013 (0.288)  Loss:  1.9648 (0.5655)  Acc@1:  0.0000 (76.8987)  Acc@5: 100.0000 (100.0000)\n",
      "Current checkpoints:\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-12.pth.tar', 76.89871761319026)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-11.pth.tar', 75.48024077466631)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-10.pth.tar', 75.29442554305156)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-9.pth.tar', 72.60926459042136)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-8.pth.tar', 72.55692227165663)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-6.pth.tar', 72.2350170112536)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-7.pth.tar', 71.54148128762104)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-5.pth.tar', 68.68359068306727)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-4.pth.tar', 66.8803978016226)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-2.pth.tar', 66.4224025124313)\n",
      "\n",
      "Train: 13 [   0/2388 (  0%)]  Loss: 0.4120 (0.412)  Time: 1.866s,    8.57/s  (1.866s,    8.57/s)  LR: 1.365e-02  Data: 1.546 (1.546)\n",
      "Train: 13 [  50/2388 (  2%)]  Loss: 0.3596 (0.405)  Time: 0.342s,   46.84/s  (0.400s,   39.97/s)  LR: 1.365e-02  Data: 0.020 (0.080)\n",
      "Train: 13 [ 100/2388 (  4%)]  Loss: 0.3358 (0.397)  Time: 0.489s,   32.69/s  (0.386s,   41.49/s)  LR: 1.365e-02  Data: 0.168 (0.065)\n",
      "Train: 13 [ 150/2388 (  6%)]  Loss: 0.4731 (0.399)  Time: 0.339s,   47.23/s  (0.383s,   41.74/s)  LR: 1.365e-02  Data: 0.019 (0.063)\n",
      "Train: 13 [ 200/2388 (  8%)]  Loss: 0.4012 (0.400)  Time: 0.389s,   41.14/s  (0.379s,   42.22/s)  LR: 1.365e-02  Data: 0.067 (0.058)\n",
      "Train: 13 [ 250/2388 ( 10%)]  Loss: 0.4353 (0.400)  Time: 0.343s,   46.69/s  (0.377s,   42.46/s)  LR: 1.365e-02  Data: 0.022 (0.056)\n",
      "Train: 13 [ 300/2388 ( 13%)]  Loss: 0.3936 (0.400)  Time: 0.464s,   34.45/s  (0.375s,   42.69/s)  LR: 1.365e-02  Data: 0.144 (0.054)\n",
      "Train: 13 [ 350/2388 ( 15%)]  Loss: 0.4265 (0.401)  Time: 0.339s,   47.21/s  (0.376s,   42.61/s)  LR: 1.365e-02  Data: 0.018 (0.055)\n",
      "Train: 13 [ 400/2388 ( 17%)]  Loss: 0.3676 (0.401)  Time: 0.473s,   33.83/s  (0.375s,   42.72/s)  LR: 1.365e-02  Data: 0.152 (0.054)\n",
      "Train: 13 [ 450/2388 ( 19%)]  Loss: 0.4880 (0.401)  Time: 0.341s,   46.96/s  (0.374s,   42.77/s)  LR: 1.365e-02  Data: 0.019 (0.053)\n",
      "Train: 13 [ 500/2388 ( 21%)]  Loss: 0.4192 (0.401)  Time: 0.435s,   36.82/s  (0.374s,   42.81/s)  LR: 1.365e-02  Data: 0.114 (0.053)\n",
      "Train: 13 [ 550/2388 ( 23%)]  Loss: 0.3666 (0.401)  Time: 0.358s,   44.75/s  (0.373s,   42.86/s)  LR: 1.365e-02  Data: 0.037 (0.052)\n",
      "Train: 13 [ 600/2388 ( 25%)]  Loss: 0.3911 (0.401)  Time: 0.473s,   33.85/s  (0.374s,   42.83/s)  LR: 1.365e-02  Data: 0.152 (0.053)\n",
      "Train: 13 [ 650/2388 ( 27%)]  Loss: 0.3934 (0.401)  Time: 0.337s,   47.41/s  (0.373s,   42.94/s)  LR: 1.365e-02  Data: 0.017 (0.052)\n",
      "Train: 13 [ 700/2388 ( 29%)]  Loss: 0.4381 (0.401)  Time: 0.489s,   32.69/s  (0.372s,   42.99/s)  LR: 1.365e-02  Data: 0.168 (0.051)\n",
      "Train: 13 [ 750/2388 ( 31%)]  Loss: 0.4128 (0.401)  Time: 0.351s,   45.62/s  (0.372s,   43.05/s)  LR: 1.365e-02  Data: 0.024 (0.051)\n",
      "Train: 13 [ 800/2388 ( 34%)]  Loss: 0.3968 (0.401)  Time: 0.412s,   38.80/s  (0.372s,   43.04/s)  LR: 1.365e-02  Data: 0.092 (0.051)\n",
      "Train: 13 [ 850/2388 ( 36%)]  Loss: 0.4151 (0.400)  Time: 0.361s,   44.38/s  (0.371s,   43.09/s)  LR: 1.365e-02  Data: 0.040 (0.050)\n",
      "Train: 13 [ 900/2388 ( 38%)]  Loss: 0.3215 (0.400)  Time: 0.353s,   45.29/s  (0.371s,   43.09/s)  LR: 1.365e-02  Data: 0.034 (0.050)\n",
      "Train: 13 [ 950/2388 ( 40%)]  Loss: 0.4000 (0.400)  Time: 0.403s,   39.75/s  (0.371s,   43.11/s)  LR: 1.365e-02  Data: 0.082 (0.050)\n",
      "Train: 13 [1000/2388 ( 42%)]  Loss: 0.4084 (0.400)  Time: 0.362s,   44.21/s  (0.371s,   43.10/s)  LR: 1.365e-02  Data: 0.042 (0.050)\n",
      "Train: 13 [1050/2388 ( 44%)]  Loss: 0.4424 (0.400)  Time: 0.353s,   45.37/s  (0.371s,   43.09/s)  LR: 1.365e-02  Data: 0.033 (0.050)\n",
      "Train: 13 [1100/2388 ( 46%)]  Loss: 0.3688 (0.399)  Time: 0.532s,   30.07/s  (0.372s,   43.04/s)  LR: 1.365e-02  Data: 0.212 (0.051)\n",
      "Train: 13 [1150/2388 ( 48%)]  Loss: 0.4342 (0.399)  Time: 0.483s,   33.16/s  (0.372s,   43.04/s)  LR: 1.365e-02  Data: 0.161 (0.051)\n",
      "Train: 13 [1200/2388 ( 50%)]  Loss: 0.5028 (0.399)  Time: 0.341s,   46.90/s  (0.372s,   42.99/s)  LR: 1.365e-02  Data: 0.021 (0.051)\n",
      "Train: 13 [1250/2388 ( 52%)]  Loss: 0.3451 (0.399)  Time: 0.418s,   38.30/s  (0.372s,   42.97/s)  LR: 1.365e-02  Data: 0.095 (0.051)\n",
      "Train: 13 [1300/2388 ( 54%)]  Loss: 0.3585 (0.399)  Time: 0.366s,   43.74/s  (0.372s,   42.97/s)  LR: 1.365e-02  Data: 0.043 (0.051)\n",
      "Train: 13 [1350/2388 ( 57%)]  Loss: 0.5202 (0.398)  Time: 0.491s,   32.58/s  (0.373s,   42.94/s)  LR: 1.365e-02  Data: 0.169 (0.052)\n",
      "Train: 13 [1400/2388 ( 59%)]  Loss: 0.3219 (0.398)  Time: 0.339s,   47.24/s  (0.372s,   42.96/s)  LR: 1.365e-02  Data: 0.019 (0.052)\n",
      "Train: 13 [1450/2388 ( 61%)]  Loss: 0.5228 (0.399)  Time: 0.443s,   36.10/s  (0.372s,   42.96/s)  LR: 1.365e-02  Data: 0.122 (0.052)\n",
      "Train: 13 [1500/2388 ( 63%)]  Loss: 0.4213 (0.399)  Time: 0.340s,   47.00/s  (0.373s,   42.94/s)  LR: 1.365e-02  Data: 0.020 (0.052)\n",
      "Train: 13 [1550/2388 ( 65%)]  Loss: 0.4045 (0.399)  Time: 0.478s,   33.49/s  (0.372s,   42.97/s)  LR: 1.365e-02  Data: 0.157 (0.052)\n",
      "Train: 13 [1600/2388 ( 67%)]  Loss: 0.3896 (0.399)  Time: 0.341s,   46.86/s  (0.373s,   42.95/s)  LR: 1.365e-02  Data: 0.021 (0.052)\n",
      "Train: 13 [1650/2388 ( 69%)]  Loss: 0.4175 (0.398)  Time: 0.501s,   31.93/s  (0.372s,   42.97/s)  LR: 1.365e-02  Data: 0.180 (0.052)\n",
      "Train: 13 [1700/2388 ( 71%)]  Loss: 0.4219 (0.399)  Time: 0.337s,   47.47/s  (0.372s,   43.01/s)  LR: 1.365e-02  Data: 0.017 (0.051)\n",
      "Train: 13 [1750/2388 ( 73%)]  Loss: 0.4246 (0.399)  Time: 0.434s,   36.83/s  (0.372s,   43.04/s)  LR: 1.365e-02  Data: 0.113 (0.051)\n",
      "Train: 13 [1800/2388 ( 75%)]  Loss: 0.3614 (0.399)  Time: 0.341s,   46.96/s  (0.372s,   43.04/s)  LR: 1.365e-02  Data: 0.019 (0.051)\n",
      "Train: 13 [1850/2388 ( 78%)]  Loss: 0.4015 (0.399)  Time: 0.425s,   37.65/s  (0.372s,   43.05/s)  LR: 1.365e-02  Data: 0.105 (0.051)\n",
      "Train: 13 [1900/2388 ( 80%)]  Loss: 0.3205 (0.399)  Time: 0.338s,   47.32/s  (0.372s,   43.07/s)  LR: 1.365e-02  Data: 0.018 (0.051)\n",
      "Train: 13 [1950/2388 ( 82%)]  Loss: 0.4498 (0.398)  Time: 0.474s,   33.76/s  (0.371s,   43.09/s)  LR: 1.365e-02  Data: 0.154 (0.050)\n",
      "Train: 13 [2000/2388 ( 84%)]  Loss: 0.4063 (0.398)  Time: 0.361s,   44.32/s  (0.371s,   43.08/s)  LR: 1.365e-02  Data: 0.041 (0.051)\n",
      "Train: 13 [2050/2388 ( 86%)]  Loss: 0.3751 (0.398)  Time: 0.467s,   34.24/s  (0.371s,   43.07/s)  LR: 1.365e-02  Data: 0.146 (0.051)\n",
      "Train: 13 [2100/2388 ( 88%)]  Loss: 0.4432 (0.398)  Time: 0.341s,   46.86/s  (0.371s,   43.09/s)  LR: 1.365e-02  Data: 0.021 (0.050)\n",
      "Train: 13 [2150/2388 ( 90%)]  Loss: 0.3815 (0.398)  Time: 0.486s,   32.95/s  (0.371s,   43.10/s)  LR: 1.365e-02  Data: 0.164 (0.050)\n",
      "Train: 13 [2200/2388 ( 92%)]  Loss: 0.4014 (0.398)  Time: 0.342s,   46.80/s  (0.371s,   43.09/s)  LR: 1.365e-02  Data: 0.019 (0.050)\n",
      "Train: 13 [2250/2388 ( 94%)]  Loss: 0.3878 (0.398)  Time: 0.459s,   34.90/s  (0.371s,   43.11/s)  LR: 1.365e-02  Data: 0.132 (0.050)\n",
      "Train: 13 [2300/2388 ( 96%)]  Loss: 0.4416 (0.398)  Time: 0.343s,   46.61/s  (0.371s,   43.11/s)  LR: 1.365e-02  Data: 0.023 (0.050)\n",
      "Train: 13 [2350/2388 ( 98%)]  Loss: 0.4998 (0.398)  Time: 0.561s,   28.54/s  (0.371s,   43.11/s)  LR: 1.365e-02  Data: 0.239 (0.050)\n",
      "Train: 13 [2387/2388 (100%)]  Loss: 0.4722 (0.398)  Time: 0.319s,   50.20/s  (0.371s,   43.11/s)  LR: 1.365e-02  Data: 0.000 (0.050)\n",
      "Test: [   0/2388]  Time: 1.225 (1.225)  Loss:  0.1440 (0.1440)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [  50/2388]  Time: 0.088 (0.297)  Loss:  0.1809 (0.2851)  Acc@1: 93.7500 (93.9951)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 100/2388]  Time: 0.812 (0.295)  Loss:  0.2776 (0.2898)  Acc@1: 93.7500 (93.3168)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 150/2388]  Time: 0.087 (0.288)  Loss:  0.1737 (0.2832)  Acc@1: 100.0000 (93.0877)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 200/2388]  Time: 0.848 (0.288)  Loss:  0.2852 (0.2870)  Acc@1: 87.5000 (92.5062)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 250/2388]  Time: 0.093 (0.286)  Loss:  0.2216 (0.2894)  Acc@1: 93.7500 (92.1813)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 300/2388]  Time: 0.846 (0.287)  Loss:  0.1343 (0.2941)  Acc@1: 93.7500 (91.9850)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 350/2388]  Time: 0.086 (0.286)  Loss:  0.3245 (0.2951)  Acc@1: 87.5000 (92.0050)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 400/2388]  Time: 0.885 (0.287)  Loss:  0.1731 (0.2952)  Acc@1: 100.0000 (91.8329)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 450/2388]  Time: 0.086 (0.286)  Loss:  0.0671 (0.2961)  Acc@1: 100.0000 (91.7822)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 500/2388]  Time: 0.875 (0.287)  Loss:  0.4836 (0.2911)  Acc@1: 81.2500 (91.9785)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 550/2388]  Time: 0.086 (0.286)  Loss:  0.4548 (0.2931)  Acc@1: 81.2500 (91.9351)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 600/2388]  Time: 0.839 (0.287)  Loss:  0.3149 (0.2985)  Acc@1: 81.2500 (91.4725)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 650/2388]  Time: 0.086 (0.286)  Loss:  0.2654 (0.2983)  Acc@1: 93.7500 (91.4555)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 700/2388]  Time: 0.842 (0.286)  Loss:  0.5830 (0.3025)  Acc@1: 81.2500 (91.1912)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 750/2388]  Time: 0.087 (0.285)  Loss:  0.3584 (0.3064)  Acc@1: 87.5000 (90.9537)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 800/2388]  Time: 1.055 (0.286)  Loss:  0.3245 (0.3100)  Acc@1: 93.7500 (90.7772)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 850/2388]  Time: 0.086 (0.286)  Loss:  0.6050 (0.3106)  Acc@1: 75.0000 (90.6360)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 900/2388]  Time: 0.845 (0.286)  Loss:  0.0545 (0.3083)  Acc@1: 100.0000 (90.7048)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 950/2388]  Time: 0.086 (0.286)  Loss:  0.4226 (0.3079)  Acc@1: 87.5000 (90.7532)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1000/2388]  Time: 0.980 (0.286)  Loss:  0.3130 (0.3066)  Acc@1: 93.7500 (90.8529)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1050/2388]  Time: 0.086 (0.285)  Loss:  0.6455 (0.3075)  Acc@1: 81.2500 (90.7350)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1100/2388]  Time: 0.869 (0.286)  Loss:  0.2112 (0.3090)  Acc@1: 93.7500 (90.6562)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1150/2388]  Time: 0.085 (0.286)  Loss:  0.9136 (0.3100)  Acc@1: 87.5000 (90.6440)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1200/2388]  Time: 0.870 (0.287)  Loss:  0.2017 (0.3106)  Acc@1: 93.7500 (90.6588)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1250/2388]  Time: 0.086 (0.286)  Loss:  0.4421 (0.3083)  Acc@1: 87.5000 (90.7674)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1300/2388]  Time: 0.882 (0.287)  Loss:  0.1311 (0.3070)  Acc@1: 100.0000 (90.8148)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1350/2388]  Time: 0.091 (0.287)  Loss:  0.1870 (0.3051)  Acc@1: 93.7500 (90.9141)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1400/2388]  Time: 0.937 (0.287)  Loss:  0.4551 (0.3040)  Acc@1: 81.2500 (90.9440)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1450/2388]  Time: 0.086 (0.287)  Loss:  0.6558 (0.3326)  Acc@1: 75.0000 (89.4900)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1500/2388]  Time: 0.839 (0.287)  Loss:  1.8340 (0.3711)  Acc@1: 12.5000 (87.7082)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1550/2388]  Time: 0.086 (0.287)  Loss:  2.2754 (0.4046)  Acc@1: 25.0000 (86.1380)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1600/2388]  Time: 0.889 (0.287)  Loss:  1.5029 (0.4328)  Acc@1: 25.0000 (84.8766)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1650/2388]  Time: 0.085 (0.287)  Loss:  2.6367 (0.4714)  Acc@1:  6.2500 (83.2677)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1700/2388]  Time: 0.826 (0.287)  Loss:  0.5381 (0.4839)  Acc@1: 81.2500 (82.5654)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1750/2388]  Time: 0.085 (0.287)  Loss:  0.4751 (0.4954)  Acc@1: 75.0000 (81.8925)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1800/2388]  Time: 0.904 (0.287)  Loss:  0.4136 (0.5047)  Acc@1: 81.2500 (81.2326)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1850/2388]  Time: 0.086 (0.287)  Loss:  0.8853 (0.5103)  Acc@1: 56.2500 (80.7536)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1900/2388]  Time: 0.841 (0.287)  Loss:  1.2295 (0.5170)  Acc@1: 50.0000 (80.3623)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1950/2388]  Time: 0.085 (0.287)  Loss:  1.2891 (0.5253)  Acc@1: 37.5000 (79.8148)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2000/2388]  Time: 0.849 (0.287)  Loss:  0.6489 (0.5303)  Acc@1: 81.2500 (79.4103)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2050/2388]  Time: 0.086 (0.287)  Loss:  0.9087 (0.5367)  Acc@1: 50.0000 (78.9127)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2100/2388]  Time: 0.852 (0.287)  Loss:  0.3176 (0.5388)  Acc@1: 87.5000 (78.6292)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2150/2388]  Time: 0.086 (0.287)  Loss:  0.3943 (0.5422)  Acc@1: 87.5000 (78.3153)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2200/2388]  Time: 0.850 (0.287)  Loss:  0.7686 (0.5469)  Acc@1: 68.7500 (78.0270)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2250/2388]  Time: 0.086 (0.287)  Loss:  0.6611 (0.5558)  Acc@1: 62.5000 (77.5211)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2300/2388]  Time: 0.976 (0.287)  Loss:  0.9995 (0.5659)  Acc@1: 68.7500 (77.0480)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2350/2388]  Time: 0.087 (0.287)  Loss:  0.7095 (0.5714)  Acc@1: 62.5000 (76.7519)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2388/2388]  Time: 0.015 (0.287)  Loss:  1.8232 (0.5750)  Acc@1:  0.0000 (76.6187)  Acc@5: 100.0000 (100.0000)\n",
      "Current checkpoints:\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-12.pth.tar', 76.89871761319026)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-13.pth.tar', 76.618686207799)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-11.pth.tar', 75.48024077466631)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-10.pth.tar', 75.29442554305156)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-9.pth.tar', 72.60926459042136)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-8.pth.tar', 72.55692227165663)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-6.pth.tar', 72.2350170112536)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-7.pth.tar', 71.54148128762104)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-5.pth.tar', 68.68359068306727)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-4.pth.tar', 66.8803978016226)\n",
      "\n",
      "Train: 14 [   0/2388 (  0%)]  Loss: 0.3782 (0.378)  Time: 1.852s,    8.64/s  (1.852s,    8.64/s)  LR: 1.031e-02  Data: 1.532 (1.532)\n",
      "Train: 14 [  50/2388 (  2%)]  Loss: 0.4730 (0.378)  Time: 0.343s,   46.69/s  (0.405s,   39.50/s)  LR: 1.031e-02  Data: 0.022 (0.084)\n",
      "Train: 14 [ 100/2388 (  4%)]  Loss: 0.2949 (0.389)  Time: 0.496s,   32.27/s  (0.385s,   41.54/s)  LR: 1.031e-02  Data: 0.174 (0.064)\n",
      "Train: 14 [ 150/2388 (  6%)]  Loss: 0.4320 (0.388)  Time: 0.343s,   46.59/s  (0.379s,   42.26/s)  LR: 1.031e-02  Data: 0.021 (0.058)\n",
      "Train: 14 [ 200/2388 (  8%)]  Loss: 0.4912 (0.393)  Time: 0.497s,   32.20/s  (0.377s,   42.44/s)  LR: 1.031e-02  Data: 0.177 (0.056)\n",
      "Train: 14 [ 250/2388 ( 10%)]  Loss: 0.4104 (0.394)  Time: 0.343s,   46.60/s  (0.375s,   42.62/s)  LR: 1.031e-02  Data: 0.022 (0.054)\n",
      "Train: 14 [ 300/2388 ( 13%)]  Loss: 0.3982 (0.393)  Time: 0.470s,   34.07/s  (0.375s,   42.62/s)  LR: 1.031e-02  Data: 0.148 (0.054)\n",
      "Train: 14 [ 350/2388 ( 15%)]  Loss: 0.4580 (0.393)  Time: 0.342s,   46.82/s  (0.374s,   42.77/s)  LR: 1.031e-02  Data: 0.020 (0.053)\n",
      "Train: 14 [ 400/2388 ( 17%)]  Loss: 0.4267 (0.393)  Time: 0.638s,   25.08/s  (0.375s,   42.70/s)  LR: 1.031e-02  Data: 0.317 (0.054)\n",
      "Train: 14 [ 450/2388 ( 19%)]  Loss: 0.3521 (0.394)  Time: 0.340s,   47.13/s  (0.374s,   42.73/s)  LR: 1.031e-02  Data: 0.019 (0.054)\n",
      "Train: 14 [ 500/2388 ( 21%)]  Loss: 0.3627 (0.394)  Time: 0.391s,   40.91/s  (0.374s,   42.76/s)  LR: 1.031e-02  Data: 0.070 (0.053)\n",
      "Train: 14 [ 550/2388 ( 23%)]  Loss: 0.3614 (0.395)  Time: 0.340s,   47.00/s  (0.374s,   42.82/s)  LR: 1.031e-02  Data: 0.019 (0.053)\n",
      "Train: 14 [ 600/2388 ( 25%)]  Loss: 0.4082 (0.396)  Time: 0.395s,   40.54/s  (0.374s,   42.78/s)  LR: 1.031e-02  Data: 0.074 (0.053)\n",
      "Train: 14 [ 650/2388 ( 27%)]  Loss: 0.4752 (0.395)  Time: 0.705s,   22.69/s  (0.374s,   42.79/s)  LR: 1.031e-02  Data: 0.384 (0.053)\n",
      "Train: 14 [ 700/2388 ( 29%)]  Loss: 0.3813 (0.396)  Time: 0.342s,   46.73/s  (0.373s,   42.84/s)  LR: 1.031e-02  Data: 0.021 (0.053)\n",
      "Train: 14 [ 750/2388 ( 31%)]  Loss: 0.4305 (0.396)  Time: 0.403s,   39.70/s  (0.373s,   42.90/s)  LR: 1.031e-02  Data: 0.082 (0.052)\n",
      "Train: 14 [ 800/2388 ( 34%)]  Loss: 0.3976 (0.396)  Time: 0.344s,   46.53/s  (0.373s,   42.92/s)  LR: 1.031e-02  Data: 0.022 (0.052)\n",
      "Train: 14 [ 850/2388 ( 36%)]  Loss: 0.4696 (0.396)  Time: 0.394s,   40.63/s  (0.373s,   42.91/s)  LR: 1.031e-02  Data: 0.073 (0.052)\n",
      "Train: 14 [ 900/2388 ( 38%)]  Loss: 0.3761 (0.397)  Time: 0.414s,   38.63/s  (0.373s,   42.94/s)  LR: 1.031e-02  Data: 0.094 (0.052)\n",
      "Train: 14 [ 950/2388 ( 40%)]  Loss: 0.4368 (0.396)  Time: 0.424s,   37.75/s  (0.372s,   42.96/s)  LR: 1.031e-02  Data: 0.102 (0.052)\n",
      "Train: 14 [1000/2388 ( 42%)]  Loss: 0.4016 (0.396)  Time: 0.346s,   46.23/s  (0.372s,   43.02/s)  LR: 1.031e-02  Data: 0.024 (0.051)\n",
      "Train: 14 [1050/2388 ( 44%)]  Loss: 0.2880 (0.396)  Time: 0.586s,   27.31/s  (0.372s,   42.99/s)  LR: 1.031e-02  Data: 0.266 (0.051)\n",
      "Train: 14 [1100/2388 ( 46%)]  Loss: 0.3809 (0.396)  Time: 0.342s,   46.75/s  (0.372s,   43.02/s)  LR: 1.031e-02  Data: 0.021 (0.051)\n",
      "Train: 14 [1150/2388 ( 48%)]  Loss: 0.3038 (0.397)  Time: 0.637s,   25.12/s  (0.373s,   42.92/s)  LR: 1.031e-02  Data: 0.317 (0.052)\n",
      "Train: 14 [1200/2388 ( 50%)]  Loss: 0.3324 (0.396)  Time: 0.349s,   45.80/s  (0.373s,   42.93/s)  LR: 1.031e-02  Data: 0.028 (0.052)\n",
      "Train: 14 [1250/2388 ( 52%)]  Loss: 0.4692 (0.396)  Time: 0.420s,   38.06/s  (0.372s,   42.95/s)  LR: 1.031e-02  Data: 0.099 (0.052)\n",
      "Train: 14 [1300/2388 ( 54%)]  Loss: 0.3441 (0.396)  Time: 0.342s,   46.82/s  (0.372s,   42.98/s)  LR: 1.031e-02  Data: 0.021 (0.051)\n",
      "Train: 14 [1350/2388 ( 57%)]  Loss: 0.3920 (0.396)  Time: 0.342s,   46.80/s  (0.372s,   42.99/s)  LR: 1.031e-02  Data: 0.021 (0.051)\n",
      "Train: 14 [1400/2388 ( 59%)]  Loss: 0.4062 (0.396)  Time: 0.344s,   46.51/s  (0.372s,   43.01/s)  LR: 1.031e-02  Data: 0.023 (0.051)\n",
      "Train: 14 [1450/2388 ( 61%)]  Loss: 0.3923 (0.397)  Time: 0.475s,   33.68/s  (0.372s,   43.00/s)  LR: 1.031e-02  Data: 0.153 (0.051)\n",
      "Train: 14 [1500/2388 ( 63%)]  Loss: 0.3922 (0.396)  Time: 0.339s,   47.17/s  (0.372s,   43.01/s)  LR: 1.031e-02  Data: 0.019 (0.051)\n",
      "Train: 14 [1550/2388 ( 65%)]  Loss: 0.3323 (0.396)  Time: 0.482s,   33.19/s  (0.372s,   43.02/s)  LR: 1.031e-02  Data: 0.162 (0.051)\n",
      "Train: 14 [1600/2388 ( 67%)]  Loss: 0.4396 (0.396)  Time: 0.363s,   44.14/s  (0.372s,   43.03/s)  LR: 1.031e-02  Data: 0.041 (0.051)\n",
      "Train: 14 [1650/2388 ( 69%)]  Loss: 0.4229 (0.396)  Time: 0.417s,   38.34/s  (0.372s,   43.04/s)  LR: 1.031e-02  Data: 0.095 (0.051)\n",
      "Train: 14 [1700/2388 ( 71%)]  Loss: 0.4798 (0.396)  Time: 0.339s,   47.23/s  (0.372s,   43.03/s)  LR: 1.031e-02  Data: 0.019 (0.051)\n",
      "Train: 14 [1750/2388 ( 73%)]  Loss: 0.4359 (0.396)  Time: 0.420s,   38.07/s  (0.372s,   43.05/s)  LR: 1.031e-02  Data: 0.099 (0.051)\n",
      "Train: 14 [1800/2388 ( 75%)]  Loss: 0.4323 (0.396)  Time: 0.340s,   47.03/s  (0.372s,   43.06/s)  LR: 1.031e-02  Data: 0.020 (0.051)\n",
      "Train: 14 [1850/2388 ( 78%)]  Loss: 0.4129 (0.395)  Time: 0.537s,   29.80/s  (0.372s,   43.06/s)  LR: 1.031e-02  Data: 0.216 (0.051)\n",
      "Train: 14 [1900/2388 ( 80%)]  Loss: 0.3942 (0.395)  Time: 0.341s,   46.94/s  (0.371s,   43.09/s)  LR: 1.031e-02  Data: 0.020 (0.050)\n",
      "Train: 14 [1950/2388 ( 82%)]  Loss: 0.4502 (0.395)  Time: 0.502s,   31.87/s  (0.371s,   43.09/s)  LR: 1.031e-02  Data: 0.177 (0.050)\n",
      "Train: 14 [2000/2388 ( 84%)]  Loss: 0.4492 (0.395)  Time: 0.340s,   47.05/s  (0.371s,   43.11/s)  LR: 1.031e-02  Data: 0.019 (0.050)\n",
      "Train: 14 [2050/2388 ( 86%)]  Loss: 0.3867 (0.395)  Time: 0.524s,   30.54/s  (0.371s,   43.12/s)  LR: 1.031e-02  Data: 0.202 (0.050)\n",
      "Train: 14 [2100/2388 ( 88%)]  Loss: 0.3624 (0.395)  Time: 0.340s,   47.06/s  (0.371s,   43.13/s)  LR: 1.031e-02  Data: 0.020 (0.050)\n",
      "Train: 14 [2150/2388 ( 90%)]  Loss: 0.3980 (0.395)  Time: 0.397s,   40.26/s  (0.371s,   43.13/s)  LR: 1.031e-02  Data: 0.076 (0.050)\n",
      "Train: 14 [2200/2388 ( 92%)]  Loss: 0.3296 (0.395)  Time: 0.341s,   46.98/s  (0.371s,   43.13/s)  LR: 1.031e-02  Data: 0.020 (0.050)\n",
      "Train: 14 [2250/2388 ( 94%)]  Loss: 0.3007 (0.395)  Time: 0.443s,   36.09/s  (0.371s,   43.12/s)  LR: 1.031e-02  Data: 0.123 (0.050)\n",
      "Train: 14 [2300/2388 ( 96%)]  Loss: 0.3631 (0.395)  Time: 0.339s,   47.21/s  (0.371s,   43.14/s)  LR: 1.031e-02  Data: 0.019 (0.050)\n",
      "Train: 14 [2350/2388 ( 98%)]  Loss: 0.3029 (0.395)  Time: 0.406s,   39.45/s  (0.371s,   43.16/s)  LR: 1.031e-02  Data: 0.084 (0.050)\n",
      "Train: 14 [2387/2388 (100%)]  Loss: 0.4134 (0.395)  Time: 0.320s,   50.07/s  (0.371s,   43.17/s)  LR: 1.031e-02  Data: 0.000 (0.050)\n",
      "Test: [   0/2388]  Time: 1.350 (1.350)  Loss:  0.1753 (0.1753)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [  50/2388]  Time: 0.087 (0.304)  Loss:  0.1442 (0.2603)  Acc@1: 93.7500 (94.7304)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 100/2388]  Time: 0.091 (0.297)  Loss:  0.2744 (0.2566)  Acc@1: 93.7500 (94.6163)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 150/2388]  Time: 0.087 (0.289)  Loss:  0.3035 (0.2621)  Acc@1: 93.7500 (93.8742)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 200/2388]  Time: 0.091 (0.289)  Loss:  0.3486 (0.2794)  Acc@1: 81.2500 (92.6617)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 250/2388]  Time: 0.088 (0.288)  Loss:  0.4253 (0.2872)  Acc@1: 87.5000 (92.1315)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 300/2388]  Time: 0.091 (0.289)  Loss:  0.2272 (0.2926)  Acc@1: 93.7500 (91.9643)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 350/2388]  Time: 0.088 (0.288)  Loss:  0.2637 (0.2954)  Acc@1: 87.5000 (91.8269)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 400/2388]  Time: 0.094 (0.289)  Loss:  0.1624 (0.2963)  Acc@1: 93.7500 (91.6926)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 450/2388]  Time: 0.087 (0.288)  Loss:  0.1044 (0.2965)  Acc@1: 93.7500 (91.7267)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 500/2388]  Time: 0.093 (0.288)  Loss:  0.4233 (0.2933)  Acc@1: 75.0000 (91.7665)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 550/2388]  Time: 0.088 (0.287)  Loss:  0.3464 (0.2947)  Acc@1: 75.0000 (91.6969)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 600/2388]  Time: 0.092 (0.287)  Loss:  0.3074 (0.2998)  Acc@1: 81.2500 (91.2542)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 650/2388]  Time: 0.088 (0.286)  Loss:  0.3616 (0.3022)  Acc@1: 93.7500 (91.0810)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 700/2388]  Time: 0.091 (0.286)  Loss:  0.5015 (0.3069)  Acc@1: 87.5000 (90.7899)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 750/2388]  Time: 0.087 (0.286)  Loss:  0.3181 (0.3113)  Acc@1: 93.7500 (90.4877)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 800/2388]  Time: 0.093 (0.286)  Loss:  0.3245 (0.3154)  Acc@1: 93.7500 (90.2856)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 850/2388]  Time: 0.088 (0.285)  Loss:  0.3613 (0.3167)  Acc@1: 87.5000 (90.1586)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 900/2388]  Time: 0.091 (0.286)  Loss:  0.0746 (0.3161)  Acc@1: 100.0000 (90.1360)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 950/2388]  Time: 0.087 (0.285)  Loss:  0.3674 (0.3162)  Acc@1: 81.2500 (90.1420)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1000/2388]  Time: 0.095 (0.285)  Loss:  0.3687 (0.3163)  Acc@1: 93.7500 (90.1661)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1050/2388]  Time: 0.086 (0.285)  Loss:  0.5962 (0.3163)  Acc@1: 87.5000 (90.1522)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1100/2388]  Time: 0.091 (0.285)  Loss:  0.2505 (0.3173)  Acc@1: 100.0000 (90.1226)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1150/2388]  Time: 0.091 (0.285)  Loss:  0.8105 (0.3175)  Acc@1: 93.7500 (90.1281)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1200/2388]  Time: 0.093 (0.285)  Loss:  0.2805 (0.3181)  Acc@1: 87.5000 (90.1488)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1250/2388]  Time: 0.088 (0.285)  Loss:  0.3491 (0.3172)  Acc@1: 87.5000 (90.2228)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1300/2388]  Time: 0.092 (0.285)  Loss:  0.3452 (0.3163)  Acc@1: 87.5000 (90.2767)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1350/2388]  Time: 0.095 (0.285)  Loss:  0.2244 (0.3155)  Acc@1: 93.7500 (90.3405)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1400/2388]  Time: 0.092 (0.286)  Loss:  0.3523 (0.3149)  Acc@1: 87.5000 (90.3774)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1450/2388]  Time: 0.087 (0.286)  Loss:  0.7632 (0.3503)  Acc@1: 68.7500 (88.8525)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1500/2388]  Time: 0.093 (0.286)  Loss:  2.0762 (0.3897)  Acc@1: 12.5000 (87.1128)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1550/2388]  Time: 0.088 (0.286)  Loss:  1.7861 (0.4239)  Acc@1: 25.0000 (85.4852)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1600/2388]  Time: 0.093 (0.286)  Loss:  1.5752 (0.4522)  Acc@1: 31.2500 (84.2169)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1650/2388]  Time: 0.088 (0.286)  Loss:  2.6348 (0.4920)  Acc@1:  0.0000 (82.6393)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1700/2388]  Time: 0.091 (0.286)  Loss:  0.6201 (0.5031)  Acc@1: 68.7500 (81.8783)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1750/2388]  Time: 0.088 (0.286)  Loss:  0.3054 (0.5092)  Acc@1: 81.2500 (81.3285)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1800/2388]  Time: 0.093 (0.287)  Loss:  0.2996 (0.5145)  Acc@1: 87.5000 (80.7676)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1850/2388]  Time: 0.089 (0.286)  Loss:  0.7881 (0.5167)  Acc@1: 62.5000 (80.4025)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1900/2388]  Time: 0.090 (0.287)  Loss:  1.2637 (0.5216)  Acc@1: 31.2500 (80.0368)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1950/2388]  Time: 0.088 (0.287)  Loss:  1.0205 (0.5263)  Acc@1: 37.5000 (79.6066)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2000/2388]  Time: 0.093 (0.287)  Loss:  0.4302 (0.5279)  Acc@1: 87.5000 (79.3041)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2050/2388]  Time: 0.088 (0.287)  Loss:  0.8242 (0.5299)  Acc@1: 43.7500 (78.9645)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2100/2388]  Time: 0.094 (0.287)  Loss:  0.2201 (0.5292)  Acc@1: 100.0000 (78.8464)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2150/2388]  Time: 0.087 (0.287)  Loss:  0.3523 (0.5299)  Acc@1: 87.5000 (78.6407)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2200/2388]  Time: 0.092 (0.287)  Loss:  0.5479 (0.5319)  Acc@1: 75.0000 (78.4019)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2250/2388]  Time: 0.088 (0.287)  Loss:  0.5566 (0.5364)  Acc@1: 81.2500 (77.9348)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2300/2388]  Time: 0.092 (0.287)  Loss:  0.6235 (0.5426)  Acc@1: 68.7500 (77.4799)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2350/2388]  Time: 0.088 (0.287)  Loss:  0.5625 (0.5434)  Acc@1: 62.5000 (77.3368)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2388/2388]  Time: 0.014 (0.287)  Loss:  2.6758 (0.5509)  Acc@1:  0.0000 (77.1421)  Acc@5: 100.0000 (100.0000)\n",
      "Current checkpoints:\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-14.pth.tar', 77.14210939544621)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-12.pth.tar', 76.89871761319026)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-13.pth.tar', 76.618686207799)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-11.pth.tar', 75.48024077466631)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-10.pth.tar', 75.29442554305156)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-9.pth.tar', 72.60926459042136)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-8.pth.tar', 72.55692227165663)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-6.pth.tar', 72.2350170112536)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-7.pth.tar', 71.54148128762104)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-5.pth.tar', 68.68359068306727)\n",
      "\n",
      "Train: 15 [   0/2388 (  0%)]  Loss: 0.3802 (0.380)  Time: 1.867s,    8.57/s  (1.867s,    8.57/s)  LR: 7.323e-03  Data: 1.546 (1.546)\n",
      "Train: 15 [  50/2388 (  2%)]  Loss: 0.4217 (0.399)  Time: 0.344s,   46.51/s  (0.397s,   40.29/s)  LR: 7.323e-03  Data: 0.022 (0.076)\n",
      "Train: 15 [ 100/2388 (  4%)]  Loss: 0.3941 (0.394)  Time: 0.448s,   35.70/s  (0.386s,   41.41/s)  LR: 7.323e-03  Data: 0.126 (0.065)\n",
      "Train: 15 [ 150/2388 (  6%)]  Loss: 0.4082 (0.393)  Time: 0.339s,   47.24/s  (0.382s,   41.83/s)  LR: 7.323e-03  Data: 0.019 (0.061)\n",
      "Train: 15 [ 200/2388 (  8%)]  Loss: 0.3430 (0.389)  Time: 0.386s,   41.43/s  (0.379s,   42.20/s)  LR: 7.323e-03  Data: 0.066 (0.058)\n",
      "Train: 15 [ 250/2388 ( 10%)]  Loss: 0.3206 (0.389)  Time: 0.349s,   45.86/s  (0.378s,   42.37/s)  LR: 7.323e-03  Data: 0.029 (0.057)\n",
      "Train: 15 [ 300/2388 ( 13%)]  Loss: 0.4339 (0.389)  Time: 0.447s,   35.77/s  (0.376s,   42.54/s)  LR: 7.323e-03  Data: 0.126 (0.055)\n",
      "Train: 15 [ 350/2388 ( 15%)]  Loss: 0.3839 (0.390)  Time: 0.364s,   43.99/s  (0.375s,   42.65/s)  LR: 7.323e-03  Data: 0.042 (0.054)\n",
      "Train: 15 [ 400/2388 ( 17%)]  Loss: 0.3695 (0.390)  Time: 0.423s,   37.78/s  (0.375s,   42.72/s)  LR: 7.323e-03  Data: 0.102 (0.054)\n",
      "Train: 15 [ 450/2388 ( 19%)]  Loss: 0.3755 (0.390)  Time: 0.353s,   45.26/s  (0.374s,   42.82/s)  LR: 7.323e-03  Data: 0.033 (0.053)\n",
      "Train: 15 [ 500/2388 ( 21%)]  Loss: 0.4305 (0.390)  Time: 0.443s,   36.14/s  (0.373s,   42.85/s)  LR: 7.323e-03  Data: 0.121 (0.053)\n",
      "Train: 15 [ 550/2388 ( 23%)]  Loss: 0.3251 (0.390)  Time: 0.352s,   45.43/s  (0.373s,   42.94/s)  LR: 7.323e-03  Data: 0.032 (0.052)\n",
      "Train: 15 [ 600/2388 ( 25%)]  Loss: 0.4174 (0.389)  Time: 0.406s,   39.36/s  (0.372s,   42.96/s)  LR: 7.323e-03  Data: 0.086 (0.052)\n",
      "Train: 15 [ 650/2388 ( 27%)]  Loss: 0.2924 (0.390)  Time: 0.342s,   46.82/s  (0.372s,   43.00/s)  LR: 7.323e-03  Data: 0.021 (0.051)\n",
      "Train: 15 [ 700/2388 ( 29%)]  Loss: 0.3186 (0.390)  Time: 0.500s,   32.03/s  (0.372s,   43.02/s)  LR: 7.323e-03  Data: 0.178 (0.051)\n",
      "Train: 15 [ 750/2388 ( 31%)]  Loss: 0.5709 (0.391)  Time: 0.351s,   45.61/s  (0.372s,   43.01/s)  LR: 7.323e-03  Data: 0.030 (0.051)\n",
      "Train: 15 [ 800/2388 ( 34%)]  Loss: 0.3546 (0.391)  Time: 0.504s,   31.75/s  (0.373s,   42.92/s)  LR: 7.323e-03  Data: 0.183 (0.052)\n",
      "Train: 15 [ 850/2388 ( 36%)]  Loss: 0.3432 (0.391)  Time: 0.339s,   47.19/s  (0.372s,   42.98/s)  LR: 7.323e-03  Data: 0.019 (0.051)\n",
      "Train: 15 [ 900/2388 ( 38%)]  Loss: 0.3428 (0.390)  Time: 0.427s,   37.45/s  (0.372s,   42.96/s)  LR: 7.323e-03  Data: 0.106 (0.052)\n",
      "Train: 15 [ 950/2388 ( 40%)]  Loss: 0.3233 (0.390)  Time: 0.340s,   47.06/s  (0.372s,   43.00/s)  LR: 7.323e-03  Data: 0.018 (0.051)\n",
      "Train: 15 [1000/2388 ( 42%)]  Loss: 0.3670 (0.390)  Time: 0.390s,   41.06/s  (0.372s,   43.03/s)  LR: 7.323e-03  Data: 0.065 (0.051)\n",
      "Train: 15 [1050/2388 ( 44%)]  Loss: 0.3134 (0.390)  Time: 0.341s,   46.99/s  (0.372s,   43.07/s)  LR: 7.323e-03  Data: 0.019 (0.051)\n",
      "Train: 15 [1100/2388 ( 46%)]  Loss: 0.4226 (0.390)  Time: 0.438s,   36.49/s  (0.371s,   43.11/s)  LR: 7.323e-03  Data: 0.119 (0.050)\n",
      "Train: 15 [1150/2388 ( 48%)]  Loss: 0.4199 (0.390)  Time: 0.343s,   46.63/s  (0.371s,   43.12/s)  LR: 7.323e-03  Data: 0.023 (0.050)\n",
      "Train: 15 [1200/2388 ( 50%)]  Loss: 0.4391 (0.390)  Time: 0.425s,   37.62/s  (0.371s,   43.08/s)  LR: 7.323e-03  Data: 0.105 (0.051)\n",
      "Train: 15 [1250/2388 ( 52%)]  Loss: 0.3674 (0.390)  Time: 0.342s,   46.82/s  (0.372s,   43.06/s)  LR: 7.323e-03  Data: 0.021 (0.051)\n",
      "Train: 15 [1300/2388 ( 54%)]  Loss: 0.4342 (0.391)  Time: 0.450s,   35.55/s  (0.372s,   43.06/s)  LR: 7.323e-03  Data: 0.129 (0.051)\n",
      "Train: 15 [1350/2388 ( 57%)]  Loss: 0.3975 (0.391)  Time: 0.343s,   46.61/s  (0.372s,   43.06/s)  LR: 7.323e-03  Data: 0.021 (0.051)\n",
      "Train: 15 [1400/2388 ( 59%)]  Loss: 0.4109 (0.391)  Time: 0.450s,   35.59/s  (0.371s,   43.08/s)  LR: 7.323e-03  Data: 0.129 (0.051)\n",
      "Train: 15 [1450/2388 ( 61%)]  Loss: 0.4291 (0.391)  Time: 0.340s,   47.07/s  (0.371s,   43.09/s)  LR: 7.323e-03  Data: 0.019 (0.050)\n",
      "Train: 15 [1500/2388 ( 63%)]  Loss: 0.3941 (0.392)  Time: 0.465s,   34.44/s  (0.371s,   43.07/s)  LR: 7.323e-03  Data: 0.144 (0.051)\n",
      "Train: 15 [1550/2388 ( 65%)]  Loss: 0.4003 (0.392)  Time: 0.349s,   45.84/s  (0.371s,   43.07/s)  LR: 7.323e-03  Data: 0.027 (0.051)\n",
      "Train: 15 [1600/2388 ( 67%)]  Loss: 0.4393 (0.392)  Time: 0.400s,   39.99/s  (0.371s,   43.07/s)  LR: 7.323e-03  Data: 0.080 (0.051)\n",
      "Train: 15 [1650/2388 ( 69%)]  Loss: 0.3454 (0.392)  Time: 0.355s,   45.13/s  (0.371s,   43.08/s)  LR: 7.323e-03  Data: 0.035 (0.051)\n",
      "Train: 15 [1700/2388 ( 71%)]  Loss: 0.4727 (0.392)  Time: 0.557s,   28.73/s  (0.371s,   43.09/s)  LR: 7.323e-03  Data: 0.237 (0.051)\n",
      "Train: 15 [1750/2388 ( 73%)]  Loss: 0.4040 (0.391)  Time: 0.343s,   46.70/s  (0.371s,   43.10/s)  LR: 7.323e-03  Data: 0.023 (0.050)\n",
      "Train: 15 [1800/2388 ( 75%)]  Loss: 0.3471 (0.391)  Time: 0.343s,   46.68/s  (0.371s,   43.10/s)  LR: 7.323e-03  Data: 0.021 (0.050)\n",
      "Train: 15 [1850/2388 ( 78%)]  Loss: 0.4640 (0.391)  Time: 0.543s,   29.48/s  (0.371s,   43.08/s)  LR: 7.323e-03  Data: 0.221 (0.051)\n",
      "Train: 15 [1900/2388 ( 80%)]  Loss: 0.5353 (0.391)  Time: 0.349s,   45.82/s  (0.372s,   43.07/s)  LR: 7.323e-03  Data: 0.029 (0.051)\n",
      "Train: 15 [1950/2388 ( 82%)]  Loss: 0.4001 (0.391)  Time: 0.504s,   31.72/s  (0.371s,   43.08/s)  LR: 7.323e-03  Data: 0.183 (0.051)\n",
      "Train: 15 [2000/2388 ( 84%)]  Loss: 0.4175 (0.391)  Time: 0.346s,   46.19/s  (0.371s,   43.09/s)  LR: 7.323e-03  Data: 0.026 (0.051)\n",
      "Train: 15 [2050/2388 ( 86%)]  Loss: 0.4178 (0.391)  Time: 0.581s,   27.52/s  (0.371s,   43.07/s)  LR: 7.323e-03  Data: 0.260 (0.051)\n",
      "Train: 15 [2100/2388 ( 88%)]  Loss: 0.4456 (0.391)  Time: 0.339s,   47.24/s  (0.371s,   43.09/s)  LR: 7.323e-03  Data: 0.019 (0.051)\n",
      "Train: 15 [2150/2388 ( 90%)]  Loss: 0.3612 (0.391)  Time: 0.425s,   37.62/s  (0.371s,   43.09/s)  LR: 7.323e-03  Data: 0.105 (0.050)\n",
      "Train: 15 [2200/2388 ( 92%)]  Loss: 0.3264 (0.391)  Time: 0.339s,   47.17/s  (0.371s,   43.10/s)  LR: 7.323e-03  Data: 0.019 (0.050)\n",
      "Train: 15 [2250/2388 ( 94%)]  Loss: 0.3387 (0.391)  Time: 0.474s,   33.74/s  (0.371s,   43.12/s)  LR: 7.323e-03  Data: 0.153 (0.050)\n",
      "Train: 15 [2300/2388 ( 96%)]  Loss: 0.3453 (0.391)  Time: 0.340s,   47.12/s  (0.371s,   43.12/s)  LR: 7.323e-03  Data: 0.020 (0.050)\n",
      "Train: 15 [2350/2388 ( 98%)]  Loss: 0.3838 (0.391)  Time: 0.403s,   39.74/s  (0.371s,   43.14/s)  LR: 7.323e-03  Data: 0.082 (0.050)\n",
      "Train: 15 [2387/2388 (100%)]  Loss: 0.4754 (0.391)  Time: 0.319s,   50.17/s  (0.371s,   43.15/s)  LR: 7.323e-03  Data: 0.000 (0.050)\n",
      "Test: [   0/2388]  Time: 1.289 (1.289)  Loss:  0.1766 (0.1766)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [  50/2388]  Time: 0.094 (0.295)  Loss:  0.1876 (0.3006)  Acc@1: 87.5000 (91.9118)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 100/2388]  Time: 0.087 (0.294)  Loss:  0.2015 (0.2988)  Acc@1: 93.7500 (91.4604)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 150/2388]  Time: 0.157 (0.287)  Loss:  0.1057 (0.2901)  Acc@1: 100.0000 (91.0182)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 200/2388]  Time: 0.087 (0.287)  Loss:  0.3569 (0.2850)  Acc@1: 81.2500 (90.9204)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 250/2388]  Time: 0.474 (0.288)  Loss:  0.3545 (0.2889)  Acc@1: 87.5000 (90.6375)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 300/2388]  Time: 0.086 (0.286)  Loss:  0.1520 (0.2884)  Acc@1: 93.7500 (90.7184)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 350/2388]  Time: 1.121 (0.289)  Loss:  0.2571 (0.2898)  Acc@1: 87.5000 (90.7051)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 400/2388]  Time: 0.085 (0.288)  Loss:  0.1475 (0.2877)  Acc@1: 100.0000 (90.7575)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 450/2388]  Time: 0.889 (0.289)  Loss:  0.0846 (0.2904)  Acc@1: 100.0000 (90.7151)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 500/2388]  Time: 0.091 (0.288)  Loss:  0.5610 (0.2880)  Acc@1: 75.0000 (90.7685)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 550/2388]  Time: 0.834 (0.289)  Loss:  0.4880 (0.2932)  Acc@1: 75.0000 (90.7328)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 600/2388]  Time: 0.087 (0.288)  Loss:  0.3186 (0.2991)  Acc@1: 81.2500 (90.3598)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 650/2388]  Time: 0.898 (0.288)  Loss:  0.2123 (0.2977)  Acc@1: 87.5000 (90.3418)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 700/2388]  Time: 0.087 (0.288)  Loss:  0.6392 (0.3022)  Acc@1: 87.5000 (90.1658)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 750/2388]  Time: 0.823 (0.288)  Loss:  0.3181 (0.3077)  Acc@1: 87.5000 (89.8885)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 800/2388]  Time: 0.093 (0.287)  Loss:  0.2690 (0.3122)  Acc@1: 87.5000 (89.7160)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 850/2388]  Time: 0.844 (0.287)  Loss:  0.4939 (0.3126)  Acc@1: 87.5000 (89.7327)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 900/2388]  Time: 0.086 (0.287)  Loss:  0.0582 (0.3108)  Acc@1: 100.0000 (89.6920)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 950/2388]  Time: 0.882 (0.287)  Loss:  0.5649 (0.3098)  Acc@1: 68.7500 (89.7148)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1000/2388]  Time: 0.087 (0.286)  Loss:  0.2827 (0.3095)  Acc@1: 87.5000 (89.7165)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1050/2388]  Time: 0.882 (0.287)  Loss:  0.7300 (0.3113)  Acc@1: 81.2500 (89.5992)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1100/2388]  Time: 0.086 (0.287)  Loss:  0.2136 (0.3164)  Acc@1: 93.7500 (89.4584)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1150/2388]  Time: 0.838 (0.287)  Loss:  1.1025 (0.3196)  Acc@1: 93.7500 (89.4277)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1200/2388]  Time: 0.088 (0.287)  Loss:  0.2717 (0.3220)  Acc@1: 93.7500 (89.4099)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1250/2388]  Time: 0.864 (0.287)  Loss:  0.3796 (0.3210)  Acc@1: 87.5000 (89.4884)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1300/2388]  Time: 0.087 (0.286)  Loss:  0.1672 (0.3202)  Acc@1: 93.7500 (89.5129)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1350/2388]  Time: 0.916 (0.287)  Loss:  0.2330 (0.3185)  Acc@1: 93.7500 (89.5864)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1400/2388]  Time: 0.087 (0.287)  Loss:  0.3889 (0.3172)  Acc@1: 81.2500 (89.6056)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1450/2388]  Time: 0.843 (0.287)  Loss:  0.2487 (0.3374)  Acc@1: 93.7500 (88.6759)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1500/2388]  Time: 0.085 (0.287)  Loss:  1.1768 (0.3637)  Acc@1: 25.0000 (87.4667)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1550/2388]  Time: 0.833 (0.286)  Loss:  1.8975 (0.3873)  Acc@1: 31.2500 (86.3354)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1600/2388]  Time: 0.086 (0.286)  Loss:  0.8408 (0.4017)  Acc@1: 62.5000 (85.6067)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1650/2388]  Time: 0.875 (0.286)  Loss:  1.3887 (0.4233)  Acc@1: 43.7500 (84.5170)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1700/2388]  Time: 0.086 (0.286)  Loss:  0.4990 (0.4345)  Acc@1: 81.2500 (83.9580)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1750/2388]  Time: 0.884 (0.287)  Loss:  0.5190 (0.4509)  Acc@1: 81.2500 (83.2025)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1800/2388]  Time: 0.086 (0.287)  Loss:  0.3972 (0.4649)  Acc@1: 87.5000 (82.5201)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1850/2388]  Time: 0.843 (0.287)  Loss:  1.1152 (0.4734)  Acc@1: 37.5000 (82.0165)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1900/2388]  Time: 0.085 (0.287)  Loss:  1.1221 (0.4810)  Acc@1: 43.7500 (81.6149)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1950/2388]  Time: 0.842 (0.287)  Loss:  1.5361 (0.4910)  Acc@1: 37.5000 (81.1090)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2000/2388]  Time: 0.086 (0.287)  Loss:  0.5332 (0.4980)  Acc@1: 81.2500 (80.6784)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2050/2388]  Time: 0.959 (0.287)  Loss:  0.8008 (0.5065)  Acc@1: 50.0000 (80.2231)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2100/2388]  Time: 0.086 (0.287)  Loss:  0.2198 (0.5101)  Acc@1: 93.7500 (79.9500)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2150/2388]  Time: 0.877 (0.287)  Loss:  0.2795 (0.5146)  Acc@1: 93.7500 (79.6693)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2200/2388]  Time: 0.086 (0.287)  Loss:  0.5981 (0.5196)  Acc@1: 75.0000 (79.4270)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2250/2388]  Time: 0.893 (0.287)  Loss:  0.7632 (0.5281)  Acc@1: 62.5000 (78.9427)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2300/2388]  Time: 0.086 (0.287)  Loss:  0.8457 (0.5389)  Acc@1: 68.7500 (78.4985)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2350/2388]  Time: 0.857 (0.287)  Loss:  0.7852 (0.5434)  Acc@1: 62.5000 (78.2406)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2388/2388]  Time: 0.013 (0.287)  Loss:  1.6445 (0.5453)  Acc@1:  0.0000 (78.1890)  Acc@5: 100.0000 (100.0000)\n",
      "Current checkpoints:\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-15.pth.tar', 78.18895577074065)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-14.pth.tar', 77.14210939544621)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-12.pth.tar', 76.89871761319026)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-13.pth.tar', 76.618686207799)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-11.pth.tar', 75.48024077466631)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-10.pth.tar', 75.29442554305156)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-9.pth.tar', 72.60926459042136)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-8.pth.tar', 72.55692227165663)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-6.pth.tar', 72.2350170112536)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-7.pth.tar', 71.54148128762104)\n",
      "\n",
      "Train: 16 [   0/2388 (  0%)]  Loss: 0.4121 (0.412)  Time: 2.306s,    6.94/s  (2.306s,    6.94/s)  LR: 4.775e-03  Data: 1.870 (1.870)\n",
      "Train: 16 [  50/2388 (  2%)]  Loss: 0.3499 (0.394)  Time: 0.346s,   46.23/s  (0.400s,   39.98/s)  LR: 4.775e-03  Data: 0.025 (0.077)\n",
      "Train: 16 [ 100/2388 (  4%)]  Loss: 0.3514 (0.391)  Time: 0.411s,   38.90/s  (0.384s,   41.67/s)  LR: 4.775e-03  Data: 0.092 (0.062)\n",
      "Train: 16 [ 150/2388 (  6%)]  Loss: 0.4140 (0.390)  Time: 0.343s,   46.65/s  (0.380s,   42.05/s)  LR: 4.775e-03  Data: 0.022 (0.059)\n",
      "Train: 16 [ 200/2388 (  8%)]  Loss: 0.5268 (0.390)  Time: 0.559s,   28.63/s  (0.377s,   42.39/s)  LR: 4.775e-03  Data: 0.237 (0.056)\n",
      "Train: 16 [ 250/2388 ( 10%)]  Loss: 0.4452 (0.390)  Time: 0.418s,   38.25/s  (0.378s,   42.35/s)  LR: 4.775e-03  Data: 0.099 (0.057)\n",
      "Train: 16 [ 300/2388 ( 13%)]  Loss: 0.3890 (0.389)  Time: 0.344s,   46.49/s  (0.377s,   42.45/s)  LR: 4.775e-03  Data: 0.023 (0.056)\n",
      "Train: 16 [ 350/2388 ( 15%)]  Loss: 0.4008 (0.389)  Time: 0.417s,   38.37/s  (0.376s,   42.58/s)  LR: 4.775e-03  Data: 0.097 (0.055)\n",
      "Train: 16 [ 400/2388 ( 17%)]  Loss: 0.3901 (0.388)  Time: 0.340s,   47.10/s  (0.375s,   42.64/s)  LR: 4.775e-03  Data: 0.020 (0.055)\n",
      "Train: 16 [ 450/2388 ( 19%)]  Loss: 0.4110 (0.388)  Time: 0.458s,   34.91/s  (0.375s,   42.69/s)  LR: 4.775e-03  Data: 0.139 (0.054)\n",
      "Train: 16 [ 500/2388 ( 21%)]  Loss: 0.3843 (0.388)  Time: 0.339s,   47.26/s  (0.375s,   42.69/s)  LR: 4.775e-03  Data: 0.019 (0.054)\n",
      "Train: 16 [ 550/2388 ( 23%)]  Loss: 0.4547 (0.387)  Time: 0.462s,   34.60/s  (0.374s,   42.80/s)  LR: 4.775e-03  Data: 0.143 (0.053)\n",
      "Train: 16 [ 600/2388 ( 25%)]  Loss: 0.4150 (0.387)  Time: 0.345s,   46.38/s  (0.373s,   42.87/s)  LR: 4.775e-03  Data: 0.025 (0.053)\n",
      "Train: 16 [ 650/2388 ( 27%)]  Loss: 0.3055 (0.387)  Time: 0.542s,   29.54/s  (0.373s,   42.86/s)  LR: 4.775e-03  Data: 0.221 (0.053)\n",
      "Train: 16 [ 700/2388 ( 29%)]  Loss: 0.3192 (0.387)  Time: 0.347s,   46.16/s  (0.373s,   42.89/s)  LR: 4.775e-03  Data: 0.027 (0.052)\n",
      "Train: 16 [ 750/2388 ( 31%)]  Loss: 0.5257 (0.388)  Time: 0.503s,   31.81/s  (0.373s,   42.94/s)  LR: 4.775e-03  Data: 0.182 (0.052)\n",
      "Train: 16 [ 800/2388 ( 34%)]  Loss: 0.5015 (0.388)  Time: 0.351s,   45.54/s  (0.372s,   42.98/s)  LR: 4.775e-03  Data: 0.031 (0.052)\n",
      "Train: 16 [ 850/2388 ( 36%)]  Loss: 0.3866 (0.388)  Time: 0.462s,   34.62/s  (0.373s,   42.94/s)  LR: 4.775e-03  Data: 0.141 (0.052)\n",
      "Train: 16 [ 900/2388 ( 38%)]  Loss: 0.3514 (0.388)  Time: 0.341s,   46.94/s  (0.372s,   42.98/s)  LR: 4.775e-03  Data: 0.019 (0.052)\n",
      "Train: 16 [ 950/2388 ( 40%)]  Loss: 0.3914 (0.389)  Time: 0.371s,   43.15/s  (0.372s,   42.96/s)  LR: 4.775e-03  Data: 0.050 (0.052)\n",
      "Train: 16 [1000/2388 ( 42%)]  Loss: 0.3377 (0.389)  Time: 0.341s,   46.89/s  (0.372s,   42.97/s)  LR: 4.775e-03  Data: 0.021 (0.052)\n",
      "Train: 16 [1050/2388 ( 44%)]  Loss: 0.4165 (0.389)  Time: 0.493s,   32.43/s  (0.373s,   42.93/s)  LR: 4.775e-03  Data: 0.174 (0.052)\n",
      "Train: 16 [1100/2388 ( 46%)]  Loss: 0.3154 (0.389)  Time: 0.339s,   47.22/s  (0.372s,   42.95/s)  LR: 4.775e-03  Data: 0.018 (0.052)\n",
      "Train: 16 [1150/2388 ( 48%)]  Loss: 0.4037 (0.389)  Time: 0.439s,   36.46/s  (0.372s,   42.99/s)  LR: 4.775e-03  Data: 0.119 (0.052)\n",
      "Train: 16 [1200/2388 ( 50%)]  Loss: 0.3985 (0.389)  Time: 0.339s,   47.13/s  (0.372s,   42.98/s)  LR: 4.775e-03  Data: 0.019 (0.052)\n",
      "Train: 16 [1250/2388 ( 52%)]  Loss: 0.3234 (0.389)  Time: 0.543s,   29.46/s  (0.372s,   42.98/s)  LR: 4.775e-03  Data: 0.224 (0.052)\n",
      "Train: 16 [1300/2388 ( 54%)]  Loss: 0.4153 (0.389)  Time: 0.340s,   47.13/s  (0.372s,   42.96/s)  LR: 4.775e-03  Data: 0.020 (0.052)\n",
      "Train: 16 [1350/2388 ( 57%)]  Loss: 0.3689 (0.389)  Time: 0.466s,   34.37/s  (0.373s,   42.95/s)  LR: 4.775e-03  Data: 0.143 (0.052)\n",
      "Train: 16 [1400/2388 ( 59%)]  Loss: 0.3905 (0.390)  Time: 0.349s,   45.86/s  (0.373s,   42.92/s)  LR: 4.775e-03  Data: 0.028 (0.052)\n",
      "Train: 16 [1450/2388 ( 61%)]  Loss: 0.4622 (0.390)  Time: 0.494s,   32.36/s  (0.373s,   42.92/s)  LR: 4.775e-03  Data: 0.175 (0.052)\n",
      "Train: 16 [1500/2388 ( 63%)]  Loss: 0.5328 (0.389)  Time: 0.340s,   47.07/s  (0.373s,   42.95/s)  LR: 4.775e-03  Data: 0.020 (0.052)\n",
      "Train: 16 [1550/2388 ( 65%)]  Loss: 0.3786 (0.389)  Time: 0.400s,   39.96/s  (0.373s,   42.94/s)  LR: 4.775e-03  Data: 0.081 (0.052)\n",
      "Train: 16 [1600/2388 ( 67%)]  Loss: 0.4003 (0.389)  Time: 0.349s,   45.91/s  (0.373s,   42.92/s)  LR: 4.775e-03  Data: 0.028 (0.052)\n",
      "Train: 16 [1650/2388 ( 69%)]  Loss: 0.3784 (0.389)  Time: 0.497s,   32.17/s  (0.373s,   42.91/s)  LR: 4.775e-03  Data: 0.176 (0.052)\n",
      "Train: 16 [1700/2388 ( 71%)]  Loss: 0.4419 (0.389)  Time: 0.341s,   46.87/s  (0.373s,   42.93/s)  LR: 4.775e-03  Data: 0.022 (0.052)\n",
      "Train: 16 [1750/2388 ( 73%)]  Loss: 0.3630 (0.388)  Time: 0.536s,   29.84/s  (0.373s,   42.93/s)  LR: 4.775e-03  Data: 0.215 (0.052)\n",
      "Train: 16 [1800/2388 ( 75%)]  Loss: 0.3337 (0.389)  Time: 0.341s,   46.89/s  (0.373s,   42.94/s)  LR: 4.775e-03  Data: 0.022 (0.052)\n",
      "Train: 16 [1850/2388 ( 78%)]  Loss: 0.4017 (0.389)  Time: 0.686s,   23.33/s  (0.373s,   42.92/s)  LR: 4.775e-03  Data: 0.366 (0.052)\n",
      "Train: 16 [1900/2388 ( 80%)]  Loss: 0.4042 (0.389)  Time: 0.342s,   46.77/s  (0.373s,   42.94/s)  LR: 4.775e-03  Data: 0.018 (0.052)\n",
      "Train: 16 [1950/2388 ( 82%)]  Loss: 0.3423 (0.389)  Time: 0.433s,   36.98/s  (0.373s,   42.95/s)  LR: 4.775e-03  Data: 0.113 (0.052)\n",
      "Train: 16 [2000/2388 ( 84%)]  Loss: 0.4796 (0.390)  Time: 0.338s,   47.36/s  (0.373s,   42.94/s)  LR: 4.775e-03  Data: 0.019 (0.052)\n",
      "Train: 16 [2050/2388 ( 86%)]  Loss: 0.5256 (0.390)  Time: 0.423s,   37.85/s  (0.373s,   42.93/s)  LR: 4.775e-03  Data: 0.102 (0.052)\n",
      "Train: 16 [2100/2388 ( 88%)]  Loss: 0.3536 (0.389)  Time: 0.338s,   47.36/s  (0.373s,   42.94/s)  LR: 4.775e-03  Data: 0.019 (0.052)\n",
      "Train: 16 [2150/2388 ( 90%)]  Loss: 0.3814 (0.389)  Time: 0.409s,   39.16/s  (0.372s,   42.96/s)  LR: 4.775e-03  Data: 0.089 (0.052)\n",
      "Train: 16 [2200/2388 ( 92%)]  Loss: 0.4044 (0.389)  Time: 0.346s,   46.26/s  (0.373s,   42.95/s)  LR: 4.775e-03  Data: 0.019 (0.052)\n",
      "Train: 16 [2250/2388 ( 94%)]  Loss: 0.4274 (0.389)  Time: 0.486s,   32.95/s  (0.373s,   42.94/s)  LR: 4.775e-03  Data: 0.166 (0.052)\n",
      "Train: 16 [2300/2388 ( 96%)]  Loss: 0.3729 (0.389)  Time: 0.342s,   46.84/s  (0.373s,   42.95/s)  LR: 4.775e-03  Data: 0.021 (0.052)\n",
      "Train: 16 [2350/2388 ( 98%)]  Loss: 0.4060 (0.389)  Time: 0.463s,   34.55/s  (0.372s,   42.97/s)  LR: 4.775e-03  Data: 0.142 (0.052)\n",
      "Train: 16 [2387/2388 (100%)]  Loss: 0.4009 (0.389)  Time: 0.318s,   50.35/s  (0.372s,   42.98/s)  LR: 4.775e-03  Data: 0.000 (0.052)\n",
      "Test: [   0/2388]  Time: 1.235 (1.235)  Loss:  0.1991 (0.1991)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [  50/2388]  Time: 0.091 (0.297)  Loss:  0.2294 (0.3120)  Acc@1: 87.5000 (91.1765)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 100/2388]  Time: 0.087 (0.289)  Loss:  0.2318 (0.3333)  Acc@1: 100.0000 (90.4084)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 150/2388]  Time: 0.091 (0.289)  Loss:  0.1289 (0.3291)  Acc@1: 100.0000 (89.5281)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 200/2388]  Time: 0.087 (0.286)  Loss:  0.3252 (0.3223)  Acc@1: 87.5000 (89.3968)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 250/2388]  Time: 0.091 (0.287)  Loss:  0.3706 (0.3190)  Acc@1: 81.2500 (89.5418)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 300/2388]  Time: 0.087 (0.286)  Loss:  0.1448 (0.3116)  Acc@1: 100.0000 (89.8671)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 350/2388]  Time: 0.090 (0.287)  Loss:  0.2225 (0.3043)  Acc@1: 87.5000 (90.2244)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 400/2388]  Time: 0.087 (0.286)  Loss:  0.1986 (0.2966)  Acc@1: 93.7500 (90.4302)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 450/2388]  Time: 0.092 (0.287)  Loss:  0.1036 (0.2949)  Acc@1: 93.7500 (90.6042)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 500/2388]  Time: 0.087 (0.286)  Loss:  0.5581 (0.2895)  Acc@1: 75.0000 (90.7310)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 550/2388]  Time: 0.091 (0.286)  Loss:  0.6499 (0.2929)  Acc@1: 75.0000 (90.8008)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 600/2388]  Time: 0.087 (0.285)  Loss:  0.3330 (0.2991)  Acc@1: 81.2500 (90.4326)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 650/2388]  Time: 0.092 (0.285)  Loss:  0.2546 (0.2988)  Acc@1: 87.5000 (90.3706)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 700/2388]  Time: 0.087 (0.285)  Loss:  0.6133 (0.3021)  Acc@1: 93.7500 (90.1658)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 750/2388]  Time: 0.091 (0.285)  Loss:  0.3530 (0.3087)  Acc@1: 81.2500 (89.9051)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 800/2388]  Time: 0.088 (0.285)  Loss:  0.2935 (0.3119)  Acc@1: 87.5000 (89.7004)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 850/2388]  Time: 0.092 (0.285)  Loss:  0.6729 (0.3131)  Acc@1: 81.2500 (89.6225)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 900/2388]  Time: 0.087 (0.285)  Loss:  0.0705 (0.3115)  Acc@1: 100.0000 (89.6088)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 950/2388]  Time: 0.092 (0.285)  Loss:  0.4500 (0.3104)  Acc@1: 68.7500 (89.6359)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1000/2388]  Time: 0.087 (0.285)  Loss:  0.2964 (0.3101)  Acc@1: 87.5000 (89.6291)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1050/2388]  Time: 0.093 (0.285)  Loss:  0.5942 (0.3117)  Acc@1: 93.7500 (89.5100)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1100/2388]  Time: 0.088 (0.285)  Loss:  0.2378 (0.3145)  Acc@1: 93.7500 (89.4471)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1150/2388]  Time: 0.098 (0.285)  Loss:  1.0176 (0.3171)  Acc@1: 87.5000 (89.4277)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1200/2388]  Time: 0.088 (0.285)  Loss:  0.2817 (0.3180)  Acc@1: 87.5000 (89.3995)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1250/2388]  Time: 0.093 (0.285)  Loss:  0.4805 (0.3169)  Acc@1: 81.2500 (89.4634)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1300/2388]  Time: 0.087 (0.285)  Loss:  0.1791 (0.3172)  Acc@1: 93.7500 (89.4600)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1350/2388]  Time: 0.093 (0.286)  Loss:  0.2729 (0.3181)  Acc@1: 87.5000 (89.4430)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1400/2388]  Time: 0.088 (0.286)  Loss:  0.2883 (0.3174)  Acc@1: 87.5000 (89.4495)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1450/2388]  Time: 0.093 (0.286)  Loss:  0.3684 (0.3321)  Acc@1: 87.5000 (88.7276)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1500/2388]  Time: 0.087 (0.286)  Loss:  1.3115 (0.3610)  Acc@1: 31.2500 (87.4625)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1550/2388]  Time: 0.093 (0.286)  Loss:  1.7461 (0.3836)  Acc@1: 37.5000 (86.3838)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1600/2388]  Time: 0.088 (0.286)  Loss:  0.9199 (0.3992)  Acc@1: 43.7500 (85.6613)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1650/2388]  Time: 0.091 (0.286)  Loss:  1.6680 (0.4201)  Acc@1: 43.7500 (84.6949)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1700/2388]  Time: 0.088 (0.286)  Loss:  0.5298 (0.4300)  Acc@1: 81.2500 (84.1490)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1750/2388]  Time: 0.094 (0.286)  Loss:  0.4255 (0.4424)  Acc@1: 81.2500 (83.4273)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1800/2388]  Time: 0.093 (0.286)  Loss:  0.3840 (0.4539)  Acc@1: 87.5000 (82.7492)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1850/2388]  Time: 0.093 (0.286)  Loss:  0.8555 (0.4617)  Acc@1: 50.0000 (82.2089)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1900/2388]  Time: 0.088 (0.286)  Loss:  1.2021 (0.4690)  Acc@1: 50.0000 (81.8056)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1950/2388]  Time: 0.092 (0.287)  Loss:  1.4150 (0.4775)  Acc@1: 31.2500 (81.2948)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2000/2388]  Time: 0.087 (0.287)  Loss:  0.4722 (0.4827)  Acc@1: 81.2500 (80.9314)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2050/2388]  Time: 0.093 (0.287)  Loss:  1.0576 (0.4890)  Acc@1: 37.5000 (80.4912)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2100/2388]  Time: 0.111 (0.286)  Loss:  0.2612 (0.4914)  Acc@1: 93.7500 (80.2564)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2150/2388]  Time: 0.096 (0.287)  Loss:  0.3145 (0.4948)  Acc@1: 93.7500 (79.9948)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2200/2388]  Time: 0.087 (0.286)  Loss:  0.7725 (0.5004)  Acc@1: 56.2500 (79.6683)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2250/2388]  Time: 0.093 (0.287)  Loss:  0.6016 (0.5092)  Acc@1: 68.7500 (79.1426)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2300/2388]  Time: 0.087 (0.287)  Loss:  0.9390 (0.5192)  Acc@1: 68.7500 (78.7049)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2350/2388]  Time: 0.090 (0.287)  Loss:  0.6606 (0.5228)  Acc@1: 75.0000 (78.5118)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2388/2388]  Time: 0.015 (0.287)  Loss:  0.5103 (0.5223)  Acc@1: 50.0000 (78.5292)  Acc@5: 100.0000 (100.0000)\n",
      "Current checkpoints:\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-16.pth.tar', 78.52918084271133)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-15.pth.tar', 78.18895577074065)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-14.pth.tar', 77.14210939544621)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-12.pth.tar', 76.89871761319026)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-13.pth.tar', 76.618686207799)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-11.pth.tar', 75.48024077466631)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-10.pth.tar', 75.29442554305156)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-9.pth.tar', 72.60926459042136)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-8.pth.tar', 72.55692227165663)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-6.pth.tar', 72.2350170112536)\n",
      "\n",
      "Train: 17 [   0/2388 (  0%)]  Loss: 0.4475 (0.448)  Time: 1.899s,    8.43/s  (1.899s,    8.43/s)  LR: 2.726e-03  Data: 1.577 (1.577)\n",
      "Train: 17 [  50/2388 (  2%)]  Loss: 0.3755 (0.393)  Time: 0.340s,   47.09/s  (0.405s,   39.48/s)  LR: 2.726e-03  Data: 0.020 (0.085)\n",
      "Train: 17 [ 100/2388 (  4%)]  Loss: 0.5331 (0.393)  Time: 0.346s,   46.20/s  (0.387s,   41.32/s)  LR: 2.726e-03  Data: 0.026 (0.067)\n",
      "Train: 17 [ 150/2388 (  6%)]  Loss: 0.3792 (0.394)  Time: 0.345s,   46.40/s  (0.381s,   42.04/s)  LR: 2.726e-03  Data: 0.021 (0.060)\n",
      "Train: 17 [ 200/2388 (  8%)]  Loss: 0.4414 (0.393)  Time: 0.460s,   34.76/s  (0.379s,   42.18/s)  LR: 2.726e-03  Data: 0.138 (0.059)\n",
      "Train: 17 [ 250/2388 ( 10%)]  Loss: 0.3754 (0.390)  Time: 0.337s,   47.45/s  (0.378s,   42.31/s)  LR: 2.726e-03  Data: 0.017 (0.058)\n",
      "Train: 17 [ 300/2388 ( 13%)]  Loss: 0.2873 (0.390)  Time: 0.457s,   35.04/s  (0.377s,   42.40/s)  LR: 2.726e-03  Data: 0.135 (0.057)\n",
      "Train: 17 [ 350/2388 ( 15%)]  Loss: 0.3971 (0.389)  Time: 0.342s,   46.79/s  (0.376s,   42.53/s)  LR: 2.726e-03  Data: 0.020 (0.056)\n",
      "Train: 17 [ 400/2388 ( 17%)]  Loss: 0.3981 (0.387)  Time: 0.397s,   40.33/s  (0.375s,   42.67/s)  LR: 2.726e-03  Data: 0.076 (0.055)\n",
      "Train: 17 [ 450/2388 ( 19%)]  Loss: 0.4113 (0.387)  Time: 0.342s,   46.79/s  (0.375s,   42.71/s)  LR: 2.726e-03  Data: 0.021 (0.054)\n",
      "Train: 17 [ 500/2388 ( 21%)]  Loss: 0.4472 (0.386)  Time: 0.476s,   33.64/s  (0.375s,   42.63/s)  LR: 2.726e-03  Data: 0.156 (0.055)\n",
      "Train: 17 [ 550/2388 ( 23%)]  Loss: 0.3711 (0.387)  Time: 0.346s,   46.25/s  (0.375s,   42.65/s)  LR: 2.726e-03  Data: 0.023 (0.055)\n",
      "Train: 17 [ 600/2388 ( 25%)]  Loss: 0.3475 (0.386)  Time: 0.537s,   29.82/s  (0.375s,   42.67/s)  LR: 2.726e-03  Data: 0.217 (0.055)\n",
      "Train: 17 [ 650/2388 ( 27%)]  Loss: 0.4442 (0.386)  Time: 0.341s,   46.87/s  (0.375s,   42.71/s)  LR: 2.726e-03  Data: 0.022 (0.054)\n",
      "Train: 17 [ 700/2388 ( 29%)]  Loss: 0.4983 (0.386)  Time: 0.544s,   29.40/s  (0.374s,   42.77/s)  LR: 2.726e-03  Data: 0.224 (0.054)\n",
      "Train: 17 [ 750/2388 ( 31%)]  Loss: 0.3670 (0.386)  Time: 0.344s,   46.58/s  (0.374s,   42.80/s)  LR: 2.726e-03  Data: 0.022 (0.053)\n",
      "Train: 17 [ 800/2388 ( 34%)]  Loss: 0.3672 (0.386)  Time: 0.574s,   27.86/s  (0.374s,   42.78/s)  LR: 2.726e-03  Data: 0.254 (0.054)\n",
      "Train: 17 [ 850/2388 ( 36%)]  Loss: 0.4333 (0.387)  Time: 0.343s,   46.58/s  (0.374s,   42.83/s)  LR: 2.726e-03  Data: 0.024 (0.053)\n",
      "Train: 17 [ 900/2388 ( 38%)]  Loss: 0.3251 (0.387)  Time: 0.390s,   41.02/s  (0.373s,   42.88/s)  LR: 2.726e-03  Data: 0.069 (0.053)\n",
      "Train: 17 [ 950/2388 ( 40%)]  Loss: 0.4124 (0.387)  Time: 0.607s,   26.35/s  (0.373s,   42.87/s)  LR: 2.726e-03  Data: 0.286 (0.053)\n",
      "Train: 17 [1000/2388 ( 42%)]  Loss: 0.4377 (0.387)  Time: 0.359s,   44.60/s  (0.373s,   42.87/s)  LR: 2.726e-03  Data: 0.038 (0.053)\n",
      "Train: 17 [1050/2388 ( 44%)]  Loss: 0.3880 (0.387)  Time: 0.408s,   39.25/s  (0.373s,   42.88/s)  LR: 2.726e-03  Data: 0.085 (0.053)\n",
      "Train: 17 [1100/2388 ( 46%)]  Loss: 0.4159 (0.387)  Time: 0.341s,   46.96/s  (0.373s,   42.90/s)  LR: 2.726e-03  Data: 0.021 (0.053)\n",
      "Train: 17 [1150/2388 ( 48%)]  Loss: 0.3860 (0.387)  Time: 0.456s,   35.12/s  (0.373s,   42.95/s)  LR: 2.726e-03  Data: 0.135 (0.052)\n",
      "Train: 17 [1200/2388 ( 50%)]  Loss: 0.4786 (0.387)  Time: 0.343s,   46.67/s  (0.373s,   42.94/s)  LR: 2.726e-03  Data: 0.023 (0.052)\n",
      "Train: 17 [1250/2388 ( 52%)]  Loss: 0.3425 (0.387)  Time: 0.420s,   38.09/s  (0.373s,   42.95/s)  LR: 2.726e-03  Data: 0.100 (0.052)\n",
      "Train: 17 [1300/2388 ( 54%)]  Loss: 0.4328 (0.387)  Time: 0.338s,   47.30/s  (0.373s,   42.95/s)  LR: 2.726e-03  Data: 0.019 (0.052)\n",
      "Train: 17 [1350/2388 ( 57%)]  Loss: 0.4245 (0.387)  Time: 0.443s,   36.12/s  (0.373s,   42.91/s)  LR: 2.726e-03  Data: 0.122 (0.052)\n",
      "Train: 17 [1400/2388 ( 59%)]  Loss: 0.3787 (0.387)  Time: 0.339s,   47.22/s  (0.373s,   42.95/s)  LR: 2.726e-03  Data: 0.019 (0.052)\n",
      "Train: 17 [1450/2388 ( 61%)]  Loss: 0.3613 (0.387)  Time: 0.404s,   39.64/s  (0.372s,   42.98/s)  LR: 2.726e-03  Data: 0.083 (0.052)\n",
      "Train: 17 [1500/2388 ( 63%)]  Loss: 0.3883 (0.387)  Time: 0.340s,   47.03/s  (0.372s,   42.98/s)  LR: 2.726e-03  Data: 0.020 (0.052)\n",
      "Train: 17 [1550/2388 ( 65%)]  Loss: 0.2759 (0.387)  Time: 0.506s,   31.62/s  (0.372s,   42.98/s)  LR: 2.726e-03  Data: 0.185 (0.052)\n",
      "Train: 17 [1600/2388 ( 67%)]  Loss: 0.4466 (0.387)  Time: 0.338s,   47.32/s  (0.372s,   42.98/s)  LR: 2.726e-03  Data: 0.019 (0.052)\n",
      "Train: 17 [1650/2388 ( 69%)]  Loss: 0.3894 (0.386)  Time: 0.552s,   28.97/s  (0.372s,   43.00/s)  LR: 2.726e-03  Data: 0.231 (0.052)\n",
      "Train: 17 [1700/2388 ( 71%)]  Loss: 0.3521 (0.386)  Time: 0.340s,   47.11/s  (0.372s,   43.03/s)  LR: 2.726e-03  Data: 0.020 (0.051)\n",
      "Train: 17 [1750/2388 ( 73%)]  Loss: 0.2747 (0.386)  Time: 0.437s,   36.64/s  (0.372s,   43.01/s)  LR: 2.726e-03  Data: 0.116 (0.052)\n",
      "Train: 17 [1800/2388 ( 75%)]  Loss: 0.4259 (0.386)  Time: 0.340s,   47.00/s  (0.372s,   43.02/s)  LR: 2.726e-03  Data: 0.019 (0.052)\n",
      "Train: 17 [1850/2388 ( 78%)]  Loss: 0.3458 (0.386)  Time: 0.475s,   33.66/s  (0.372s,   43.04/s)  LR: 2.726e-03  Data: 0.154 (0.051)\n",
      "Train: 17 [1900/2388 ( 80%)]  Loss: 0.4411 (0.386)  Time: 0.339s,   47.20/s  (0.372s,   43.05/s)  LR: 2.726e-03  Data: 0.019 (0.051)\n",
      "Train: 17 [1950/2388 ( 82%)]  Loss: 0.4178 (0.386)  Time: 0.450s,   35.58/s  (0.372s,   43.06/s)  LR: 2.726e-03  Data: 0.129 (0.051)\n",
      "Train: 17 [2000/2388 ( 84%)]  Loss: 0.3538 (0.387)  Time: 0.337s,   47.41/s  (0.371s,   43.09/s)  LR: 2.726e-03  Data: 0.018 (0.051)\n",
      "Train: 17 [2050/2388 ( 86%)]  Loss: 0.4296 (0.387)  Time: 0.457s,   35.01/s  (0.371s,   43.10/s)  LR: 2.726e-03  Data: 0.137 (0.051)\n",
      "Train: 17 [2100/2388 ( 88%)]  Loss: 0.4673 (0.387)  Time: 0.345s,   46.36/s  (0.371s,   43.12/s)  LR: 2.726e-03  Data: 0.019 (0.051)\n",
      "Train: 17 [2150/2388 ( 90%)]  Loss: 0.3374 (0.387)  Time: 0.471s,   33.96/s  (0.371s,   43.14/s)  LR: 2.726e-03  Data: 0.149 (0.050)\n",
      "Train: 17 [2200/2388 ( 92%)]  Loss: 0.4335 (0.387)  Time: 0.340s,   47.09/s  (0.371s,   43.15/s)  LR: 2.726e-03  Data: 0.020 (0.050)\n",
      "Train: 17 [2250/2388 ( 94%)]  Loss: 0.3554 (0.387)  Time: 0.423s,   37.78/s  (0.371s,   43.15/s)  LR: 2.726e-03  Data: 0.103 (0.050)\n",
      "Train: 17 [2300/2388 ( 96%)]  Loss: 0.4049 (0.387)  Time: 0.338s,   47.28/s  (0.371s,   43.15/s)  LR: 2.726e-03  Data: 0.019 (0.050)\n",
      "Train: 17 [2350/2388 ( 98%)]  Loss: 0.4710 (0.388)  Time: 0.403s,   39.67/s  (0.371s,   43.16/s)  LR: 2.726e-03  Data: 0.083 (0.050)\n",
      "Train: 17 [2387/2388 (100%)]  Loss: 0.3781 (0.388)  Time: 0.319s,   50.22/s  (0.371s,   43.16/s)  LR: 2.726e-03  Data: 0.000 (0.050)\n",
      "Test: [   0/2388]  Time: 1.420 (1.420)  Loss:  0.1917 (0.1917)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [  50/2388]  Time: 0.093 (0.299)  Loss:  0.2297 (0.3072)  Acc@1: 87.5000 (92.1569)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 100/2388]  Time: 0.877 (0.295)  Loss:  0.2407 (0.3228)  Acc@1: 93.7500 (91.3366)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 150/2388]  Time: 0.086 (0.290)  Loss:  0.1632 (0.3291)  Acc@1: 100.0000 (90.0662)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 200/2388]  Time: 0.847 (0.291)  Loss:  0.3538 (0.3323)  Acc@1: 87.5000 (89.2102)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 250/2388]  Time: 0.086 (0.288)  Loss:  0.4253 (0.3367)  Acc@1: 81.2500 (88.8695)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 300/2388]  Time: 0.902 (0.290)  Loss:  0.1818 (0.3290)  Acc@1: 93.7500 (89.1819)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 350/2388]  Time: 0.086 (0.288)  Loss:  0.2307 (0.3242)  Acc@1: 87.5000 (89.4765)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 400/2388]  Time: 0.926 (0.289)  Loss:  0.1664 (0.3148)  Acc@1: 100.0000 (89.7911)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 450/2388]  Time: 0.087 (0.288)  Loss:  0.1143 (0.3120)  Acc@1: 93.7500 (89.9252)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 500/2388]  Time: 0.898 (0.290)  Loss:  0.5508 (0.3063)  Acc@1: 75.0000 (90.1073)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 550/2388]  Time: 0.086 (0.290)  Loss:  0.5176 (0.3118)  Acc@1: 75.0000 (90.1202)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 600/2388]  Time: 0.849 (0.289)  Loss:  0.3616 (0.3188)  Acc@1: 81.2500 (89.6631)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 650/2388]  Time: 0.086 (0.288)  Loss:  0.2913 (0.3192)  Acc@1: 87.5000 (89.6025)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 700/2388]  Time: 0.825 (0.288)  Loss:  0.8540 (0.3242)  Acc@1: 87.5000 (89.4258)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 750/2388]  Time: 0.092 (0.287)  Loss:  0.3926 (0.3308)  Acc@1: 81.2500 (89.1644)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 800/2388]  Time: 0.838 (0.288)  Loss:  0.3213 (0.3362)  Acc@1: 87.5000 (88.9357)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 850/2388]  Time: 0.086 (0.287)  Loss:  0.5234 (0.3363)  Acc@1: 81.2500 (88.9028)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 900/2388]  Time: 0.822 (0.287)  Loss:  0.0721 (0.3348)  Acc@1: 100.0000 (88.9220)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 950/2388]  Time: 0.086 (0.286)  Loss:  0.5601 (0.3334)  Acc@1: 68.7500 (88.9656)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1000/2388]  Time: 0.845 (0.286)  Loss:  0.3223 (0.3336)  Acc@1: 81.2500 (88.9673)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1050/2388]  Time: 0.086 (0.286)  Loss:  0.8706 (0.3353)  Acc@1: 81.2500 (88.8499)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1100/2388]  Time: 0.868 (0.286)  Loss:  0.2529 (0.3404)  Acc@1: 93.7500 (88.7318)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1150/2388]  Time: 0.087 (0.286)  Loss:  1.3330 (0.3430)  Acc@1: 87.5000 (88.7000)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1200/2388]  Time: 0.855 (0.286)  Loss:  0.3003 (0.3444)  Acc@1: 93.7500 (88.6709)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1250/2388]  Time: 0.086 (0.286)  Loss:  0.4893 (0.3428)  Acc@1: 87.5000 (88.7740)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1300/2388]  Time: 0.960 (0.287)  Loss:  0.2469 (0.3427)  Acc@1: 93.7500 (88.8019)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1350/2388]  Time: 0.092 (0.286)  Loss:  0.3748 (0.3429)  Acc@1: 81.2500 (88.7768)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1400/2388]  Time: 0.874 (0.287)  Loss:  0.5972 (0.3430)  Acc@1: 81.2500 (88.7580)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1450/2388]  Time: 0.086 (0.287)  Loss:  0.1644 (0.3552)  Acc@1: 100.0000 (88.1806)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1500/2388]  Time: 0.899 (0.287)  Loss:  1.1543 (0.3750)  Acc@1: 31.2500 (87.2543)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1550/2388]  Time: 0.086 (0.287)  Loss:  1.4668 (0.3914)  Acc@1: 37.5000 (86.4402)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1600/2388]  Time: 0.854 (0.287)  Loss:  0.5693 (0.3995)  Acc@1: 75.0000 (86.0322)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1650/2388]  Time: 0.086 (0.287)  Loss:  1.4648 (0.4143)  Acc@1: 43.7500 (85.3119)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1700/2388]  Time: 0.868 (0.287)  Loss:  0.4517 (0.4258)  Acc@1: 87.5000 (84.7075)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1750/2388]  Time: 0.086 (0.287)  Loss:  0.3887 (0.4382)  Acc@1: 81.2500 (84.0591)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1800/2388]  Time: 0.845 (0.287)  Loss:  0.3821 (0.4499)  Acc@1: 87.5000 (83.4259)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1850/2388]  Time: 0.086 (0.287)  Loss:  1.0059 (0.4573)  Acc@1: 43.7500 (82.9146)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1900/2388]  Time: 0.854 (0.287)  Loss:  1.3291 (0.4658)  Acc@1: 37.5000 (82.5289)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1950/2388]  Time: 0.087 (0.287)  Loss:  1.4648 (0.4747)  Acc@1: 37.5000 (82.0028)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2000/2388]  Time: 0.820 (0.287)  Loss:  0.6191 (0.4803)  Acc@1: 81.2500 (81.6436)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2050/2388]  Time: 0.086 (0.287)  Loss:  0.9053 (0.4870)  Acc@1: 37.5000 (81.2104)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2100/2388]  Time: 0.867 (0.287)  Loss:  0.1901 (0.4898)  Acc@1: 100.0000 (80.9525)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2150/2388]  Time: 0.086 (0.287)  Loss:  0.2808 (0.4936)  Acc@1: 93.7500 (80.6747)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2200/2388]  Time: 0.854 (0.287)  Loss:  0.6909 (0.4989)  Acc@1: 62.5000 (80.3868)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2250/2388]  Time: 0.091 (0.287)  Loss:  0.5981 (0.5074)  Acc@1: 68.7500 (79.8923)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2300/2388]  Time: 0.887 (0.287)  Loss:  0.7759 (0.5179)  Acc@1: 68.7500 (79.4654)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2350/2388]  Time: 0.086 (0.287)  Loss:  0.6777 (0.5213)  Acc@1: 75.0000 (79.2960)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2388/2388]  Time: 0.014 (0.287)  Loss:  0.5439 (0.5198)  Acc@1: 50.0000 (79.3431)  Acc@5: 100.0000 (100.0000)\n",
      "Current checkpoints:\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-17.pth.tar', 79.34310389950275)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-16.pth.tar', 78.52918084271133)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-15.pth.tar', 78.18895577074065)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-14.pth.tar', 77.14210939544621)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-12.pth.tar', 76.89871761319026)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-13.pth.tar', 76.618686207799)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-11.pth.tar', 75.48024077466631)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-10.pth.tar', 75.29442554305156)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-9.pth.tar', 72.60926459042136)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-8.pth.tar', 72.55692227165663)\n",
      "\n",
      "Train: 18 [   0/2388 (  0%)]  Loss: 0.3100 (0.310)  Time: 1.859s,    8.61/s  (1.859s,    8.61/s)  LR: 1.225e-03  Data: 1.538 (1.538)\n",
      "Train: 18 [  50/2388 (  2%)]  Loss: 0.2942 (0.378)  Time: 0.339s,   47.26/s  (0.399s,   40.14/s)  LR: 1.225e-03  Data: 0.018 (0.078)\n",
      "Train: 18 [ 100/2388 (  4%)]  Loss: 0.3554 (0.381)  Time: 0.441s,   36.27/s  (0.384s,   41.71/s)  LR: 1.225e-03  Data: 0.121 (0.063)\n",
      "Train: 18 [ 150/2388 (  6%)]  Loss: 0.3627 (0.381)  Time: 0.352s,   45.40/s  (0.377s,   42.41/s)  LR: 1.225e-03  Data: 0.032 (0.057)\n",
      "Train: 18 [ 200/2388 (  8%)]  Loss: 0.3435 (0.384)  Time: 0.472s,   33.86/s  (0.376s,   42.51/s)  LR: 1.225e-03  Data: 0.152 (0.056)\n",
      "Train: 18 [ 250/2388 ( 10%)]  Loss: 0.4082 (0.384)  Time: 0.338s,   47.31/s  (0.375s,   42.63/s)  LR: 1.225e-03  Data: 0.019 (0.055)\n",
      "Train: 18 [ 300/2388 ( 13%)]  Loss: 0.3554 (0.385)  Time: 0.407s,   39.27/s  (0.374s,   42.81/s)  LR: 1.225e-03  Data: 0.086 (0.053)\n",
      "Train: 18 [ 350/2388 ( 15%)]  Loss: 0.5708 (0.384)  Time: 0.477s,   33.56/s  (0.373s,   42.89/s)  LR: 1.225e-03  Data: 0.155 (0.053)\n",
      "Train: 18 [ 400/2388 ( 17%)]  Loss: 0.3405 (0.385)  Time: 0.344s,   46.49/s  (0.373s,   42.93/s)  LR: 1.225e-03  Data: 0.022 (0.052)\n",
      "Train: 18 [ 450/2388 ( 19%)]  Loss: 0.4626 (0.386)  Time: 0.453s,   35.36/s  (0.374s,   42.80/s)  LR: 1.225e-03  Data: 0.132 (0.053)\n",
      "Train: 18 [ 500/2388 ( 21%)]  Loss: 0.3452 (0.384)  Time: 0.340s,   47.08/s  (0.374s,   42.79/s)  LR: 1.225e-03  Data: 0.019 (0.054)\n",
      "Train: 18 [ 550/2388 ( 23%)]  Loss: 0.3318 (0.383)  Time: 0.617s,   25.91/s  (0.374s,   42.74/s)  LR: 1.225e-03  Data: 0.298 (0.054)\n",
      "Train: 18 [ 600/2388 ( 25%)]  Loss: 0.4130 (0.384)  Time: 0.342s,   46.79/s  (0.374s,   42.79/s)  LR: 1.225e-03  Data: 0.019 (0.053)\n",
      "Train: 18 [ 650/2388 ( 27%)]  Loss: 0.4465 (0.385)  Time: 0.428s,   37.36/s  (0.374s,   42.83/s)  LR: 1.225e-03  Data: 0.108 (0.053)\n",
      "Train: 18 [ 700/2388 ( 29%)]  Loss: 0.3744 (0.386)  Time: 0.339s,   47.18/s  (0.373s,   42.85/s)  LR: 1.225e-03  Data: 0.018 (0.053)\n",
      "Train: 18 [ 750/2388 ( 31%)]  Loss: 0.2791 (0.386)  Time: 0.404s,   39.59/s  (0.374s,   42.83/s)  LR: 1.225e-03  Data: 0.084 (0.053)\n",
      "Train: 18 [ 800/2388 ( 34%)]  Loss: 0.3632 (0.386)  Time: 0.339s,   47.22/s  (0.373s,   42.89/s)  LR: 1.225e-03  Data: 0.019 (0.053)\n",
      "Train: 18 [ 850/2388 ( 36%)]  Loss: 0.5099 (0.386)  Time: 0.401s,   39.85/s  (0.373s,   42.88/s)  LR: 1.225e-03  Data: 0.080 (0.053)\n",
      "Train: 18 [ 900/2388 ( 38%)]  Loss: 0.3139 (0.386)  Time: 0.338s,   47.40/s  (0.373s,   42.93/s)  LR: 1.225e-03  Data: 0.018 (0.052)\n",
      "Train: 18 [ 950/2388 ( 40%)]  Loss: 0.3953 (0.386)  Time: 0.464s,   34.45/s  (0.372s,   42.96/s)  LR: 1.225e-03  Data: 0.143 (0.052)\n",
      "Train: 18 [1000/2388 ( 42%)]  Loss: 0.3391 (0.386)  Time: 0.339s,   47.26/s  (0.372s,   43.02/s)  LR: 1.225e-03  Data: 0.018 (0.051)\n",
      "Train: 18 [1050/2388 ( 44%)]  Loss: 0.3828 (0.386)  Time: 0.384s,   41.62/s  (0.372s,   43.03/s)  LR: 1.225e-03  Data: 0.064 (0.051)\n",
      "Train: 18 [1100/2388 ( 46%)]  Loss: 0.3608 (0.386)  Time: 0.341s,   46.92/s  (0.372s,   43.01/s)  LR: 1.225e-03  Data: 0.019 (0.052)\n",
      "Train: 18 [1150/2388 ( 48%)]  Loss: 0.3525 (0.386)  Time: 0.481s,   33.26/s  (0.372s,   42.97/s)  LR: 1.225e-03  Data: 0.160 (0.052)\n",
      "Train: 18 [1200/2388 ( 50%)]  Loss: 0.3255 (0.386)  Time: 0.338s,   47.39/s  (0.372s,   43.00/s)  LR: 1.225e-03  Data: 0.018 (0.052)\n",
      "Train: 18 [1250/2388 ( 52%)]  Loss: 0.4428 (0.386)  Time: 0.519s,   30.83/s  (0.372s,   43.02/s)  LR: 1.225e-03  Data: 0.198 (0.052)\n",
      "Train: 18 [1300/2388 ( 54%)]  Loss: 0.4179 (0.386)  Time: 0.340s,   47.06/s  (0.372s,   43.02/s)  LR: 1.225e-03  Data: 0.020 (0.051)\n",
      "Train: 18 [1350/2388 ( 57%)]  Loss: 0.3779 (0.386)  Time: 0.473s,   33.81/s  (0.372s,   43.04/s)  LR: 1.225e-03  Data: 0.154 (0.051)\n",
      "Train: 18 [1400/2388 ( 59%)]  Loss: 0.3988 (0.386)  Time: 0.338s,   47.31/s  (0.372s,   43.04/s)  LR: 1.225e-03  Data: 0.019 (0.051)\n",
      "Train: 18 [1450/2388 ( 61%)]  Loss: 0.4321 (0.386)  Time: 0.453s,   35.29/s  (0.372s,   43.03/s)  LR: 1.225e-03  Data: 0.133 (0.051)\n",
      "Train: 18 [1500/2388 ( 63%)]  Loss: 0.3790 (0.386)  Time: 0.341s,   46.96/s  (0.372s,   43.06/s)  LR: 1.225e-03  Data: 0.020 (0.051)\n",
      "Train: 18 [1550/2388 ( 65%)]  Loss: 0.3917 (0.386)  Time: 0.500s,   32.03/s  (0.372s,   43.02/s)  LR: 1.225e-03  Data: 0.176 (0.051)\n",
      "Train: 18 [1600/2388 ( 67%)]  Loss: 0.4980 (0.386)  Time: 0.340s,   47.03/s  (0.372s,   43.03/s)  LR: 1.225e-03  Data: 0.020 (0.051)\n",
      "Train: 18 [1650/2388 ( 69%)]  Loss: 0.4971 (0.386)  Time: 0.507s,   31.53/s  (0.372s,   43.04/s)  LR: 1.225e-03  Data: 0.186 (0.051)\n",
      "Train: 18 [1700/2388 ( 71%)]  Loss: 0.4317 (0.386)  Time: 0.338s,   47.28/s  (0.372s,   43.07/s)  LR: 1.225e-03  Data: 0.019 (0.051)\n",
      "Train: 18 [1750/2388 ( 73%)]  Loss: 0.3805 (0.386)  Time: 0.445s,   35.99/s  (0.371s,   43.07/s)  LR: 1.225e-03  Data: 0.124 (0.051)\n",
      "Train: 18 [1800/2388 ( 75%)]  Loss: 0.4716 (0.386)  Time: 0.340s,   47.00/s  (0.371s,   43.09/s)  LR: 1.225e-03  Data: 0.020 (0.051)\n",
      "Train: 18 [1850/2388 ( 78%)]  Loss: 0.3585 (0.386)  Time: 0.476s,   33.60/s  (0.371s,   43.09/s)  LR: 1.225e-03  Data: 0.155 (0.051)\n",
      "Train: 18 [1900/2388 ( 80%)]  Loss: 0.4099 (0.386)  Time: 0.340s,   47.02/s  (0.371s,   43.08/s)  LR: 1.225e-03  Data: 0.020 (0.051)\n",
      "Train: 18 [1950/2388 ( 82%)]  Loss: 0.3639 (0.386)  Time: 0.478s,   33.49/s  (0.371s,   43.07/s)  LR: 1.225e-03  Data: 0.157 (0.051)\n",
      "Train: 18 [2000/2388 ( 84%)]  Loss: 0.3582 (0.386)  Time: 0.342s,   46.72/s  (0.371s,   43.09/s)  LR: 1.225e-03  Data: 0.019 (0.051)\n",
      "Train: 18 [2050/2388 ( 86%)]  Loss: 0.3303 (0.386)  Time: 0.448s,   35.70/s  (0.371s,   43.11/s)  LR: 1.225e-03  Data: 0.128 (0.051)\n",
      "Train: 18 [2100/2388 ( 88%)]  Loss: 0.3318 (0.386)  Time: 0.343s,   46.69/s  (0.371s,   43.10/s)  LR: 1.225e-03  Data: 0.019 (0.051)\n",
      "Train: 18 [2150/2388 ( 90%)]  Loss: 0.3038 (0.386)  Time: 0.431s,   37.12/s  (0.371s,   43.08/s)  LR: 1.225e-03  Data: 0.110 (0.051)\n",
      "Train: 18 [2200/2388 ( 92%)]  Loss: 0.3969 (0.386)  Time: 0.339s,   47.23/s  (0.371s,   43.09/s)  LR: 1.225e-03  Data: 0.019 (0.051)\n",
      "Train: 18 [2250/2388 ( 94%)]  Loss: 0.4302 (0.386)  Time: 0.455s,   35.18/s  (0.371s,   43.08/s)  LR: 1.225e-03  Data: 0.135 (0.051)\n",
      "Train: 18 [2300/2388 ( 96%)]  Loss: 0.3335 (0.386)  Time: 0.339s,   47.25/s  (0.371s,   43.10/s)  LR: 1.225e-03  Data: 0.019 (0.051)\n",
      "Train: 18 [2350/2388 ( 98%)]  Loss: 0.5065 (0.386)  Time: 0.560s,   28.57/s  (0.371s,   43.11/s)  LR: 1.225e-03  Data: 0.238 (0.051)\n",
      "Train: 18 [2387/2388 (100%)]  Loss: 0.3346 (0.387)  Time: 0.319s,   50.15/s  (0.371s,   43.10/s)  LR: 1.225e-03  Data: 0.000 (0.051)\n",
      "Test: [   0/2388]  Time: 1.308 (1.308)  Loss:  0.2051 (0.2051)  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [  50/2388]  Time: 0.087 (0.300)  Loss:  0.2424 (0.3008)  Acc@1: 87.5000 (91.6667)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 100/2388]  Time: 0.091 (0.295)  Loss:  0.2317 (0.3166)  Acc@1: 93.7500 (90.9653)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 150/2388]  Time: 0.088 (0.288)  Loss:  0.2021 (0.3307)  Acc@1: 100.0000 (89.6109)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 200/2388]  Time: 0.091 (0.289)  Loss:  0.3601 (0.3389)  Acc@1: 87.5000 (88.6505)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 250/2388]  Time: 0.087 (0.286)  Loss:  0.4338 (0.3424)  Acc@1: 81.2500 (88.3217)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 300/2388]  Time: 0.091 (0.289)  Loss:  0.1554 (0.3385)  Acc@1: 100.0000 (88.6213)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 350/2388]  Time: 0.088 (0.287)  Loss:  0.2991 (0.3335)  Acc@1: 87.5000 (88.8711)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 400/2388]  Time: 0.092 (0.288)  Loss:  0.1714 (0.3267)  Acc@1: 100.0000 (89.1365)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 450/2388]  Time: 0.087 (0.286)  Loss:  0.1290 (0.3249)  Acc@1: 93.7500 (89.2461)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 500/2388]  Time: 0.092 (0.287)  Loss:  0.5508 (0.3194)  Acc@1: 75.0000 (89.4960)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 550/2388]  Time: 0.087 (0.286)  Loss:  0.5610 (0.3224)  Acc@1: 75.0000 (89.5417)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 600/2388]  Time: 0.091 (0.286)  Loss:  0.3809 (0.3287)  Acc@1: 81.2500 (89.1743)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 650/2388]  Time: 0.088 (0.286)  Loss:  0.3052 (0.3287)  Acc@1: 93.7500 (89.1225)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 700/2388]  Time: 0.093 (0.286)  Loss:  0.6782 (0.3329)  Acc@1: 81.2500 (88.8820)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 750/2388]  Time: 0.087 (0.286)  Loss:  0.3596 (0.3381)  Acc@1: 93.7500 (88.5985)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 800/2388]  Time: 0.091 (0.286)  Loss:  0.3835 (0.3418)  Acc@1: 87.5000 (88.3817)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 850/2388]  Time: 0.087 (0.286)  Loss:  0.4202 (0.3425)  Acc@1: 81.2500 (88.2712)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 900/2388]  Time: 0.092 (0.287)  Loss:  0.0739 (0.3411)  Acc@1: 100.0000 (88.2492)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 950/2388]  Time: 0.087 (0.286)  Loss:  0.7354 (0.3400)  Acc@1: 68.7500 (88.2821)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1000/2388]  Time: 0.091 (0.287)  Loss:  0.3362 (0.3398)  Acc@1: 87.5000 (88.3367)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1050/2388]  Time: 0.088 (0.286)  Loss:  0.5386 (0.3407)  Acc@1: 87.5000 (88.2612)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1100/2388]  Time: 0.091 (0.286)  Loss:  0.3020 (0.3435)  Acc@1: 93.7500 (88.2266)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1150/2388]  Time: 0.087 (0.286)  Loss:  0.9224 (0.3451)  Acc@1: 93.7500 (88.1950)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1200/2388]  Time: 0.091 (0.287)  Loss:  0.3015 (0.3459)  Acc@1: 87.5000 (88.2182)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1250/2388]  Time: 0.087 (0.287)  Loss:  0.4731 (0.3443)  Acc@1: 81.2500 (88.2644)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1300/2388]  Time: 0.092 (0.287)  Loss:  0.2391 (0.3437)  Acc@1: 93.7500 (88.3023)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1350/2388]  Time: 0.087 (0.287)  Loss:  0.3586 (0.3434)  Acc@1: 87.5000 (88.3003)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1400/2388]  Time: 0.092 (0.287)  Loss:  0.4072 (0.3433)  Acc@1: 81.2500 (88.3030)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1450/2388]  Time: 0.087 (0.287)  Loss:  0.1736 (0.3575)  Acc@1: 100.0000 (87.6378)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1500/2388]  Time: 0.093 (0.287)  Loss:  1.1514 (0.3793)  Acc@1: 31.2500 (86.6506)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1550/2388]  Time: 0.086 (0.287)  Loss:  1.8467 (0.3977)  Acc@1: 31.2500 (85.7874)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1600/2388]  Time: 0.094 (0.287)  Loss:  0.6328 (0.4084)  Acc@1: 68.7500 (85.2787)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1650/2388]  Time: 0.087 (0.287)  Loss:  1.5498 (0.4260)  Acc@1: 43.7500 (84.4110)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1700/2388]  Time: 0.092 (0.287)  Loss:  0.4634 (0.4344)  Acc@1: 87.5000 (83.9065)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1750/2388]  Time: 0.090 (0.287)  Loss:  0.3091 (0.4423)  Acc@1: 81.2500 (83.3881)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1800/2388]  Time: 0.091 (0.287)  Loss:  0.3469 (0.4498)  Acc@1: 87.5000 (82.8463)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1850/2388]  Time: 0.087 (0.287)  Loss:  0.8540 (0.4543)  Acc@1: 50.0000 (82.4521)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1900/2388]  Time: 0.092 (0.287)  Loss:  0.9561 (0.4596)  Acc@1: 50.0000 (82.1541)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1950/2388]  Time: 0.087 (0.287)  Loss:  1.2793 (0.4655)  Acc@1: 37.5000 (81.7754)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2000/2388]  Time: 0.093 (0.287)  Loss:  0.5566 (0.4689)  Acc@1: 81.2500 (81.5030)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2050/2388]  Time: 0.087 (0.287)  Loss:  0.6289 (0.4730)  Acc@1: 50.0000 (81.1708)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2100/2388]  Time: 0.093 (0.287)  Loss:  0.1792 (0.4741)  Acc@1: 100.0000 (81.0090)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2150/2388]  Time: 0.087 (0.287)  Loss:  0.2561 (0.4760)  Acc@1: 93.7500 (80.8054)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2200/2388]  Time: 0.092 (0.287)  Loss:  0.5918 (0.4792)  Acc@1: 68.7500 (80.5770)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2250/2388]  Time: 0.089 (0.287)  Loss:  0.5132 (0.4845)  Acc@1: 81.2500 (80.1838)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2300/2388]  Time: 0.091 (0.287)  Loss:  0.5513 (0.4914)  Acc@1: 68.7500 (79.8294)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2350/2388]  Time: 0.093 (0.287)  Loss:  0.5176 (0.4925)  Acc@1: 81.2500 (79.7400)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2388/2388]  Time: 0.014 (0.287)  Loss:  0.9155 (0.4935)  Acc@1: 50.0000 (79.7095)  Acc@5: 100.0000 (100.0000)\n",
      "Current checkpoints:\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-18.pth.tar', 79.7095001308558)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-17.pth.tar', 79.34310389950275)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-16.pth.tar', 78.52918084271133)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-15.pth.tar', 78.18895577074065)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-14.pth.tar', 77.14210939544621)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-12.pth.tar', 76.89871761319026)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-13.pth.tar', 76.618686207799)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-11.pth.tar', 75.48024077466631)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-10.pth.tar', 75.29442554305156)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-9.pth.tar', 72.60926459042136)\n",
      "\n",
      "Train: 19 [   0/2388 (  0%)]  Loss: 0.3689 (0.369)  Time: 1.835s,    8.72/s  (1.835s,    8.72/s)  LR: 3.088e-04  Data: 1.514 (1.514)\n",
      "Train: 19 [  50/2388 (  2%)]  Loss: 0.4331 (0.384)  Time: 0.343s,   46.61/s  (0.399s,   40.10/s)  LR: 3.088e-04  Data: 0.024 (0.078)\n",
      "Train: 19 [ 100/2388 (  4%)]  Loss: 0.4750 (0.385)  Time: 0.374s,   42.79/s  (0.384s,   41.72/s)  LR: 3.088e-04  Data: 0.053 (0.063)\n",
      "Train: 19 [ 150/2388 (  6%)]  Loss: 0.4262 (0.387)  Time: 0.341s,   46.97/s  (0.381s,   41.98/s)  LR: 3.088e-04  Data: 0.020 (0.061)\n",
      "Train: 19 [ 200/2388 (  8%)]  Loss: 0.3991 (0.388)  Time: 0.399s,   40.12/s  (0.377s,   42.48/s)  LR: 3.088e-04  Data: 0.077 (0.056)\n",
      "Train: 19 [ 250/2388 ( 10%)]  Loss: 0.3498 (0.388)  Time: 0.354s,   45.23/s  (0.375s,   42.67/s)  LR: 3.088e-04  Data: 0.033 (0.054)\n",
      "Train: 19 [ 300/2388 ( 13%)]  Loss: 0.4440 (0.388)  Time: 0.343s,   46.65/s  (0.375s,   42.68/s)  LR: 3.088e-04  Data: 0.022 (0.054)\n",
      "Train: 19 [ 350/2388 ( 15%)]  Loss: 0.4175 (0.388)  Time: 0.412s,   38.87/s  (0.373s,   42.84/s)  LR: 3.088e-04  Data: 0.092 (0.053)\n",
      "Train: 19 [ 400/2388 ( 17%)]  Loss: 0.3981 (0.387)  Time: 0.364s,   43.90/s  (0.373s,   42.94/s)  LR: 3.088e-04  Data: 0.042 (0.052)\n",
      "Train: 19 [ 450/2388 ( 19%)]  Loss: 0.3529 (0.387)  Time: 0.446s,   35.90/s  (0.372s,   43.02/s)  LR: 3.088e-04  Data: 0.126 (0.051)\n",
      "Train: 19 [ 500/2388 ( 21%)]  Loss: 0.3927 (0.385)  Time: 0.439s,   36.41/s  (0.372s,   43.07/s)  LR: 3.088e-04  Data: 0.119 (0.051)\n",
      "Train: 19 [ 550/2388 ( 23%)]  Loss: 0.3228 (0.386)  Time: 0.348s,   45.95/s  (0.372s,   43.05/s)  LR: 3.088e-04  Data: 0.028 (0.051)\n",
      "Train: 19 [ 600/2388 ( 25%)]  Loss: 0.4318 (0.386)  Time: 0.345s,   46.35/s  (0.371s,   43.09/s)  LR: 3.088e-04  Data: 0.026 (0.051)\n",
      "Train: 19 [ 650/2388 ( 27%)]  Loss: 0.3639 (0.386)  Time: 0.444s,   36.02/s  (0.372s,   43.04/s)  LR: 3.088e-04  Data: 0.123 (0.051)\n",
      "Train: 19 [ 700/2388 ( 29%)]  Loss: 0.4128 (0.386)  Time: 0.338s,   47.27/s  (0.371s,   43.09/s)  LR: 3.088e-04  Data: 0.019 (0.051)\n",
      "Train: 19 [ 750/2388 ( 31%)]  Loss: 0.4225 (0.386)  Time: 0.579s,   27.65/s  (0.372s,   43.06/s)  LR: 3.088e-04  Data: 0.258 (0.051)\n",
      "Train: 19 [ 800/2388 ( 34%)]  Loss: 0.3842 (0.386)  Time: 0.339s,   47.18/s  (0.372s,   43.07/s)  LR: 3.088e-04  Data: 0.019 (0.051)\n",
      "Train: 19 [ 850/2388 ( 36%)]  Loss: 0.3578 (0.386)  Time: 0.451s,   35.45/s  (0.372s,   43.05/s)  LR: 3.088e-04  Data: 0.132 (0.051)\n",
      "Train: 19 [ 900/2388 ( 38%)]  Loss: 0.3350 (0.386)  Time: 0.339s,   47.18/s  (0.371s,   43.08/s)  LR: 3.088e-04  Data: 0.017 (0.051)\n",
      "Train: 19 [ 950/2388 ( 40%)]  Loss: 0.3493 (0.386)  Time: 0.430s,   37.23/s  (0.371s,   43.11/s)  LR: 3.088e-04  Data: 0.109 (0.051)\n",
      "Train: 19 [1000/2388 ( 42%)]  Loss: 0.4418 (0.386)  Time: 0.338s,   47.29/s  (0.371s,   43.16/s)  LR: 3.088e-04  Data: 0.019 (0.050)\n",
      "Train: 19 [1050/2388 ( 44%)]  Loss: 0.3442 (0.386)  Time: 0.395s,   40.50/s  (0.370s,   43.21/s)  LR: 3.088e-04  Data: 0.072 (0.050)\n",
      "Train: 19 [1100/2388 ( 46%)]  Loss: 0.3859 (0.386)  Time: 0.338s,   47.40/s  (0.370s,   43.21/s)  LR: 3.088e-04  Data: 0.018 (0.050)\n",
      "Train: 19 [1150/2388 ( 48%)]  Loss: 0.3803 (0.385)  Time: 0.475s,   33.72/s  (0.370s,   43.22/s)  LR: 3.088e-04  Data: 0.155 (0.050)\n",
      "Train: 19 [1200/2388 ( 50%)]  Loss: 0.3883 (0.385)  Time: 0.340s,   47.12/s  (0.370s,   43.22/s)  LR: 3.088e-04  Data: 0.020 (0.050)\n",
      "Train: 19 [1250/2388 ( 52%)]  Loss: 0.3837 (0.385)  Time: 0.407s,   39.29/s  (0.370s,   43.22/s)  LR: 3.088e-04  Data: 0.088 (0.050)\n",
      "Train: 19 [1300/2388 ( 54%)]  Loss: 0.3875 (0.386)  Time: 0.340s,   47.07/s  (0.370s,   43.20/s)  LR: 3.088e-04  Data: 0.020 (0.050)\n",
      "Train: 19 [1350/2388 ( 57%)]  Loss: 0.3245 (0.386)  Time: 0.468s,   34.22/s  (0.371s,   43.18/s)  LR: 3.088e-04  Data: 0.147 (0.050)\n",
      "Train: 19 [1400/2388 ( 59%)]  Loss: 0.3643 (0.386)  Time: 0.337s,   47.41/s  (0.371s,   43.17/s)  LR: 3.088e-04  Data: 0.018 (0.050)\n",
      "Train: 19 [1450/2388 ( 61%)]  Loss: 0.4262 (0.386)  Time: 0.410s,   39.06/s  (0.371s,   43.18/s)  LR: 3.088e-04  Data: 0.090 (0.050)\n",
      "Train: 19 [1500/2388 ( 63%)]  Loss: 0.2852 (0.385)  Time: 0.340s,   47.09/s  (0.371s,   43.18/s)  LR: 3.088e-04  Data: 0.019 (0.050)\n",
      "Train: 19 [1550/2388 ( 65%)]  Loss: 0.3239 (0.385)  Time: 0.530s,   30.20/s  (0.371s,   43.13/s)  LR: 3.088e-04  Data: 0.209 (0.051)\n",
      "Train: 19 [1600/2388 ( 67%)]  Loss: 0.3685 (0.385)  Time: 0.340s,   47.09/s  (0.371s,   43.13/s)  LR: 3.088e-04  Data: 0.019 (0.051)\n",
      "Train: 19 [1650/2388 ( 69%)]  Loss: 0.5064 (0.385)  Time: 0.512s,   31.22/s  (0.371s,   43.14/s)  LR: 3.088e-04  Data: 0.191 (0.050)\n",
      "Train: 19 [1700/2388 ( 71%)]  Loss: 0.4124 (0.385)  Time: 0.343s,   46.66/s  (0.371s,   43.14/s)  LR: 3.088e-04  Data: 0.019 (0.050)\n",
      "Train: 19 [1750/2388 ( 73%)]  Loss: 0.3362 (0.385)  Time: 0.444s,   36.02/s  (0.371s,   43.13/s)  LR: 3.088e-04  Data: 0.123 (0.051)\n",
      "Train: 19 [1800/2388 ( 75%)]  Loss: 0.4229 (0.385)  Time: 0.341s,   46.93/s  (0.371s,   43.14/s)  LR: 3.088e-04  Data: 0.020 (0.050)\n",
      "Train: 19 [1850/2388 ( 78%)]  Loss: 0.3405 (0.385)  Time: 0.473s,   33.83/s  (0.371s,   43.14/s)  LR: 3.088e-04  Data: 0.150 (0.050)\n",
      "Train: 19 [1900/2388 ( 80%)]  Loss: 0.4518 (0.385)  Time: 0.340s,   47.08/s  (0.371s,   43.16/s)  LR: 3.088e-04  Data: 0.020 (0.050)\n",
      "Train: 19 [1950/2388 ( 82%)]  Loss: 0.2553 (0.385)  Time: 0.483s,   33.13/s  (0.371s,   43.17/s)  LR: 3.088e-04  Data: 0.164 (0.050)\n",
      "Train: 19 [2000/2388 ( 84%)]  Loss: 0.4460 (0.385)  Time: 0.340s,   47.03/s  (0.371s,   43.17/s)  LR: 3.088e-04  Data: 0.020 (0.050)\n",
      "Train: 19 [2050/2388 ( 86%)]  Loss: 0.4138 (0.385)  Time: 0.607s,   26.37/s  (0.371s,   43.15/s)  LR: 3.088e-04  Data: 0.287 (0.050)\n",
      "Train: 19 [2100/2388 ( 88%)]  Loss: 0.4878 (0.385)  Time: 0.342s,   46.84/s  (0.371s,   43.17/s)  LR: 3.088e-04  Data: 0.019 (0.050)\n",
      "Train: 19 [2150/2388 ( 90%)]  Loss: 0.3881 (0.385)  Time: 0.433s,   36.92/s  (0.371s,   43.18/s)  LR: 3.088e-04  Data: 0.113 (0.050)\n",
      "Train: 19 [2200/2388 ( 92%)]  Loss: 0.3475 (0.385)  Time: 0.341s,   46.89/s  (0.370s,   43.19/s)  LR: 3.088e-04  Data: 0.020 (0.050)\n",
      "Train: 19 [2250/2388 ( 94%)]  Loss: 0.3864 (0.385)  Time: 0.498s,   32.12/s  (0.370s,   43.20/s)  LR: 3.088e-04  Data: 0.177 (0.050)\n",
      "Train: 19 [2300/2388 ( 96%)]  Loss: 0.3950 (0.385)  Time: 0.333s,   48.08/s  (0.370s,   43.21/s)  LR: 3.088e-04  Data: 0.013 (0.050)\n",
      "Train: 19 [2350/2388 ( 98%)]  Loss: 0.3680 (0.385)  Time: 0.453s,   35.29/s  (0.370s,   43.19/s)  LR: 3.088e-04  Data: 0.133 (0.050)\n",
      "Train: 19 [2387/2388 (100%)]  Loss: 0.3117 (0.385)  Time: 0.318s,   50.31/s  (0.370s,   43.20/s)  LR: 3.088e-04  Data: 0.000 (0.050)\n",
      "Test: [   0/2388]  Time: 1.340 (1.340)  Loss:  0.1902 (0.1902)  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [  50/2388]  Time: 0.097 (0.296)  Loss:  0.2302 (0.2856)  Acc@1: 87.5000 (92.4020)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 100/2388]  Time: 0.093 (0.293)  Loss:  0.2285 (0.3018)  Acc@1: 93.7500 (91.5842)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 150/2388]  Time: 0.205 (0.287)  Loss:  0.1709 (0.3147)  Acc@1: 100.0000 (90.3560)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 200/2388]  Time: 0.087 (0.289)  Loss:  0.3574 (0.3202)  Acc@1: 87.5000 (89.4900)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 250/2388]  Time: 0.490 (0.288)  Loss:  0.4031 (0.3212)  Acc@1: 81.2500 (89.3177)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 300/2388]  Time: 0.086 (0.288)  Loss:  0.1427 (0.3164)  Acc@1: 100.0000 (89.5349)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 350/2388]  Time: 0.748 (0.288)  Loss:  0.2524 (0.3104)  Acc@1: 87.5000 (89.8504)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 400/2388]  Time: 0.087 (0.286)  Loss:  0.1832 (0.3043)  Acc@1: 100.0000 (90.0094)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 450/2388]  Time: 0.886 (0.287)  Loss:  0.1165 (0.3016)  Acc@1: 93.7500 (90.1746)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 500/2388]  Time: 0.086 (0.287)  Loss:  0.5269 (0.2957)  Acc@1: 75.0000 (90.4192)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 550/2388]  Time: 1.012 (0.288)  Loss:  0.6094 (0.2983)  Acc@1: 75.0000 (90.4378)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 600/2388]  Time: 0.086 (0.287)  Loss:  0.3613 (0.3049)  Acc@1: 81.2500 (90.0478)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 650/2388]  Time: 0.883 (0.288)  Loss:  0.2917 (0.3050)  Acc@1: 93.7500 (89.9962)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 700/2388]  Time: 0.086 (0.287)  Loss:  0.5811 (0.3093)  Acc@1: 87.5000 (89.7290)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 750/2388]  Time: 0.847 (0.288)  Loss:  0.3442 (0.3145)  Acc@1: 93.7500 (89.4724)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 800/2388]  Time: 0.086 (0.287)  Loss:  0.3503 (0.3187)  Acc@1: 87.5000 (89.2478)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 850/2388]  Time: 0.894 (0.287)  Loss:  0.3862 (0.3196)  Acc@1: 81.2500 (89.1084)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 900/2388]  Time: 0.109 (0.287)  Loss:  0.0605 (0.3183)  Acc@1: 100.0000 (89.1024)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [ 950/2388]  Time: 0.826 (0.287)  Loss:  0.7100 (0.3174)  Acc@1: 68.7500 (89.1430)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1000/2388]  Time: 0.086 (0.286)  Loss:  0.2927 (0.3173)  Acc@1: 87.5000 (89.1858)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1050/2388]  Time: 0.921 (0.287)  Loss:  0.4985 (0.3180)  Acc@1: 87.5000 (89.1235)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1100/2388]  Time: 0.087 (0.287)  Loss:  0.2756 (0.3205)  Acc@1: 93.7500 (89.0781)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1150/2388]  Time: 0.931 (0.287)  Loss:  0.8501 (0.3221)  Acc@1: 93.7500 (89.0530)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1200/2388]  Time: 0.086 (0.287)  Loss:  0.2986 (0.3227)  Acc@1: 87.5000 (89.0508)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1250/2388]  Time: 0.861 (0.288)  Loss:  0.5171 (0.3210)  Acc@1: 81.2500 (89.1437)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1300/2388]  Time: 0.086 (0.288)  Loss:  0.1959 (0.3199)  Acc@1: 93.7500 (89.1862)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1350/2388]  Time: 0.930 (0.289)  Loss:  0.3374 (0.3192)  Acc@1: 87.5000 (89.1978)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1400/2388]  Time: 0.087 (0.288)  Loss:  0.3533 (0.3186)  Acc@1: 81.2500 (89.2086)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1450/2388]  Time: 0.847 (0.289)  Loss:  0.2390 (0.3335)  Acc@1: 93.7500 (88.5424)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1500/2388]  Time: 0.087 (0.288)  Loss:  1.1885 (0.3570)  Acc@1: 31.2500 (87.5042)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1550/2388]  Time: 0.850 (0.288)  Loss:  1.8633 (0.3768)  Acc@1: 31.2500 (86.5732)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1600/2388]  Time: 0.086 (0.289)  Loss:  0.7344 (0.3889)  Acc@1: 50.0000 (85.9697)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1650/2388]  Time: 0.866 (0.289)  Loss:  1.7617 (0.4091)  Acc@1: 37.5000 (85.0242)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1700/2388]  Time: 0.086 (0.289)  Loss:  0.4712 (0.4187)  Acc@1: 87.5000 (84.4944)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1750/2388]  Time: 0.865 (0.289)  Loss:  0.3162 (0.4285)  Acc@1: 81.2500 (83.9199)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1800/2388]  Time: 0.092 (0.289)  Loss:  0.3669 (0.4384)  Acc@1: 87.5000 (83.3287)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1850/2388]  Time: 1.022 (0.289)  Loss:  0.9224 (0.4448)  Acc@1: 50.0000 (82.8404)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1900/2388]  Time: 0.086 (0.289)  Loss:  1.1162 (0.4521)  Acc@1: 37.5000 (82.4829)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [1950/2388]  Time: 1.012 (0.289)  Loss:  1.3975 (0.4595)  Acc@1: 37.5000 (82.0509)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2000/2388]  Time: 0.086 (0.289)  Loss:  0.5771 (0.4641)  Acc@1: 87.5000 (81.7529)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2050/2388]  Time: 0.852 (0.289)  Loss:  0.7085 (0.4694)  Acc@1: 50.0000 (81.3810)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2100/2388]  Time: 0.086 (0.288)  Loss:  0.2007 (0.4712)  Acc@1: 100.0000 (81.1697)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2150/2388]  Time: 1.062 (0.289)  Loss:  0.2727 (0.4741)  Acc@1: 93.7500 (80.9246)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2200/2388]  Time: 0.087 (0.289)  Loss:  0.6216 (0.4782)  Acc@1: 68.7500 (80.6764)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2250/2388]  Time: 0.923 (0.289)  Loss:  0.5337 (0.4848)  Acc@1: 81.2500 (80.2338)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2300/2388]  Time: 0.086 (0.289)  Loss:  0.6406 (0.4936)  Acc@1: 68.7500 (79.8349)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2350/2388]  Time: 0.915 (0.289)  Loss:  0.5278 (0.4955)  Acc@1: 81.2500 (79.7108)  Acc@5: 100.0000 (100.0000)\n",
      "Test: [2388/2388]  Time: 0.014 (0.289)  Loss:  0.9741 (0.4974)  Acc@1: 50.0000 (79.6441)  Acc@5: 100.0000 (100.0000)\n",
      "Current checkpoints:\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-18.pth.tar', 79.7095001308558)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-19.pth.tar', 79.64407223239989)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-17.pth.tar', 79.34310389950275)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-16.pth.tar', 78.52918084271133)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-15.pth.tar', 78.18895577074065)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-14.pth.tar', 77.14210939544621)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-12.pth.tar', 76.89871761319026)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-13.pth.tar', 76.618686207799)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-11.pth.tar', 75.48024077466631)\n",
      " ('./output/train/dfl-benchmark-training-fix-extract-images/checkpoint-10.pth.tar', 75.29442554305156)\n",
      "\n",
      "*** Best metric: 79.7095001308558 (epoch 18)\n"
     ]
    }
   ],
   "source": [
    "#!g1.4\n",
    "%run ../image_models/train.py work/split_images/ \\\n",
    "    -b 16 \\\n",
    "    --input-size 3 720 1280 \\\n",
    "    --img-size 32 \\\n",
    "    --amp \\\n",
    "    --epochs 20 \\\n",
    "    --pretrained \\\n",
    "    --num-classes 4 \\\n",
    "    --model tf_efficientnet_b0_ap \\\n",
    "    --experiment dfl-benchmark-training-fix-extract-images \\\n",
    "    --bce-loss \\\n",
    "    --cooldown-epochs 0 \\\n",
    "    --drop 0.2 \\\n",
    "    --mixup 0.2 \\\n",
    "    --color-jitter 0.6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f8c058",
   "metadata": {
    "cellId": "2971s71xtw1b6kdg017laj"
   },
   "source": [
    "We take the training checkpoints and average the weights from the last few ones using the script provided by timm to save it as a model so we don't have to train it every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3786a571",
   "metadata": {
    "cellId": "0oawu9b48ap06celxorckuh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Extracting metric from checkpoint 'output/train/dfl-benchmark-training-fix-extract-images/checkpoint-10.pth.tar'\n",
      "=> Extracting metric from checkpoint 'output/train/dfl-benchmark-training-fix-extract-images/checkpoint-11.pth.tar'\n",
      "=> Extracting metric from checkpoint 'output/train/dfl-benchmark-training-fix-extract-images/checkpoint-12.pth.tar'\n",
      "=> Extracting metric from checkpoint 'output/train/dfl-benchmark-training-fix-extract-images/checkpoint-13.pth.tar'\n",
      "=> Extracting metric from checkpoint 'output/train/dfl-benchmark-training-fix-extract-images/checkpoint-14.pth.tar'\n",
      "=> Extracting metric from checkpoint 'output/train/dfl-benchmark-training-fix-extract-images/checkpoint-15.pth.tar'\n",
      "=> Extracting metric from checkpoint 'output/train/dfl-benchmark-training-fix-extract-images/checkpoint-16.pth.tar'\n",
      "=> Extracting metric from checkpoint 'output/train/dfl-benchmark-training-fix-extract-images/checkpoint-17.pth.tar'\n",
      "=> Extracting metric from checkpoint 'output/train/dfl-benchmark-training-fix-extract-images/checkpoint-18.pth.tar'\n",
      "=> Extracting metric from checkpoint 'output/train/dfl-benchmark-training-fix-extract-images/model_best.pth.tar'\n",
      "=> Extracting metric from checkpoint 'output/train/dfl-benchmark-training-fix-extract-images/last.pth.tar'\n",
      "=> Extracting metric from checkpoint 'output/train/dfl-benchmark-training-fix-extract-images/checkpoint-19.pth.tar'\n",
      "Selected checkpoints:\n",
      "76.618686207799 output/train/dfl-benchmark-training-fix-extract-images/checkpoint-13.pth.tar\n",
      "76.89871761319026 output/train/dfl-benchmark-training-fix-extract-images/checkpoint-12.pth.tar\n",
      "77.14210939544621 output/train/dfl-benchmark-training-fix-extract-images/checkpoint-14.pth.tar\n",
      "78.18895577074065 output/train/dfl-benchmark-training-fix-extract-images/checkpoint-15.pth.tar\n",
      "78.52918084271133 output/train/dfl-benchmark-training-fix-extract-images/checkpoint-16.pth.tar\n",
      "79.34310389950275 output/train/dfl-benchmark-training-fix-extract-images/checkpoint-17.pth.tar\n",
      "79.64407223239989 output/train/dfl-benchmark-training-fix-extract-images/checkpoint-19.pth.tar\n",
      "79.64407223239989 output/train/dfl-benchmark-training-fix-extract-images/last.pth.tar\n",
      "79.7095001308558 output/train/dfl-benchmark-training-fix-extract-images/checkpoint-18.pth.tar\n",
      "79.7095001308558 output/train/dfl-benchmark-training-fix-extract-images/model_best.pth.tar\n",
      "=> Saved state_dict to 'model/tf_efficientnet_b0_ap-456-fix.pt, SHA256: 0e89415141c1a028e49813a8d754717055f5b9b867f0754e7294211cdc9159a3'\n"
     ]
    }
   ],
   "source": [
    "#!c1.32\n",
    "%run ../image_models/avg_checkpoints.py --input output/train/dfl-benchmark-training-fix-extract-images \\\n",
    "    --output ../model/tf_efficientnet_b0_ap-456-fix.pt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "notebookId": "aebf9036-0f35-4d5d-b4e3-3f4a26ee3039",
  "notebookPath": "baseline_model_creation.ipynb",
  "vscode": {
   "interpreter": {
    "hash": "2a8dfe095fce2b5e88c64a2c3ee084c8e0e0d70b23e7b95b1cfb538be294c5c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
